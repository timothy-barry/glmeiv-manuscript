\documentclass[12pt]{article}
\usepackage{amsfonts}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage[toc,page]{appendix}
\usepackage{algpseudocode}
\setcounter{tocdepth}{1}
\usepackage{xcolor}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\usepackage{float}
\allowdisplaybreaks
\usepackage[caption = false]{subfig}
\usepackage{/Users/timbarry/optionFiles/mymacros}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}
\noindent
Tim, Gene, Kathryn
\begin{center}
\textbf{CRISPR genome editing, single-cell sequencing, and exponential family measurement error models}
\end{center}

\begin{abstract}
CRISPR genome engineering and single-cell sequencing have transformed biological discovery. Single-cell CRISPR screens unite these two technologies, linking genetic perturbations in individual cells to changes in gene expression and potentially illuminating regulatory networks underlying diseases. In this work we study single-cell CRISPR screens from a statistical perspective. We demonstrate on real data that a standard method for estimation and inference in single-cell CRISPR screens — “thresholded regression” — exhibits attenuation bias and a bias-variance tradeoff as a function of an intrinsic tuning parameter. We recover these phenomena in precise theoretical terms in an idealized Gaussian setting. Next, we introduce GLM-EIV (“generalized linear model with errors-in-variables”), a new method for single-cell CRISPR screen analysis. GLM-EIV generalizes the classical errors-in-variables model to response distributions and sources of measurement error that are exponential family-distributed, overcoming limitations of thresholded regression. We develop a computational infrastructure to deploy GLM-EIV across hundreds or thousands of processors on clouds (e.g., Microsoft Azure) and high-performance clusters. Leveraging this infrastructure, we apply GLM-EIV to analyze two recent, large-scale, single-cell CRISPR screen datasets, yielding new biological insights.
\end{abstract}
\tableofcontents

\section{Introduction}
CRISPR is a genome engineering tool that has enabled scientists to precisely edit human and nonhuman genomes, opening the door to new medical therapies \cite{Rothgangl2021,Musunuru2021} and transforming basic biology research \cite{Przybyla2021}. Recently, scientists have paired CRISPR genome engineering with single-cell sequencing \cite{Dixit2016,Datlinger2017}. The resulting assays, known as a ``single-cell CRISPR screens,'' link genetic perturbations in individual cells to changes in gene expression, illuminating regulatory networks underlying human diseases and other traits \cite{Morris2021a}.

Despite their promise, single-cell CRISPR screens present substantial statistical challenges. A major difficulty is that CRISPR perturbations \blue{are assigned stochastically to cells and cannot be observed directly}. As a consequence, one cannot know with certainty which cells were perturbed. Instead, one must leverage an indirect, noisy proxy of perturbation presence or absence -- namely, transcribed guide RNA counts -- to ``guess'' which cells were perturbed. Using these imputed perturbation assignments, one can attempt to estimate the effect of the perturbation on gene expression. The standard approach, which we call ``thresholded regression'' or the ``thresholding method,'' is to assign perturbation identities to cells by simply thresholding the guide RNA counts.

We study estimation and inference in single-cell CRISPR screens from a statistical perspective, formulating the data generating mechanism using a new class of errors-in-variables (or measurement error) models. We assume that the response variable $y$ is a GLM of an underlying predictor variable $x^*$. We do not observe $x^*$ directly; rather, we observe a noisy version $x$ of $x^*$ that itself is a GLM of $x^*$. The goal of the analysis is to estimate the effect of $x^*$ on $y$ using the observed data $(x , y)$ only. In the context of the biological application, $x^*$, $y$, and $x$ are CRISPR perturbations, gene expressions, and guide RNA counts, respectively.

Our work makes two main contributions. First, we study the thresholding method from empirical and theoretical perspectives. Notably, we demonstrate on real data that the thresholding method exhibits attenuation bias and a bias-variance tradeoff as a function of the selected threshold, and we recover these phenomena in precise mathematical terms in an idealized Gaussian model. \blue{Second, we introduce a new method, GLM-EIV (generalized linear model with errors-in-variables), for single-cell CRISPR screen analysis. GLM-EIV generalizes the classical errors-in-variables model to response distributions and sources of measurement error that are exponential family-distributed. GLM-EIV implicitly estimates the probability that each cell was perturbed, obviating the need to explicitly impute perturbation assignments via thresholding or another heuristic.} Theoretical analyses and simulation studies indicate that GLM-EIV outperforms the thresholding method in large regions of the parameter space.

We implement several statistical accelerations (that likely are of independent utility) to bring the cost of GLM-EIV down to within an order of magnitude of the thresholding method. Finally, we develop a computational infrastructure to deploy GLM-EIV at-scale across hundreds or thousands of processors on clouds (e.g., Microsoft Azure) and high-performance clusters. Leveraging this infrastructure, we apply GLM-EIV to analyze two recent, large-scale, high multiplicity-of-infection single-cell CRISPR screen datasets, yielding new biological and statistical insights.

\section{Background and analysis challenges}

\subsection{Related work}

Motivated by the challenges of single-cell data, several authors recently have extended statistical models that (implicitly or explicitly) assume Gaussianity and homoscedasticity to a broader class of exponential family distributions. For example, Lin, Lei, and Roeder \cite{Lin2021} developed eSVD, an extension of SVD to exponential family and curved Gaussian responses. Unlike SVD, eSVD models the relationship between the mean and variance of a gene's expression level, a phenomenon induced by the countedness of single-cell data \cite{Lause2021}.
Similarly, Townes et al.\ \cite{Townes2019} proposed GLM-PCA, a generalization of PCA that directly models Poisson- or negative binomially-distributed gene expression counts. We see our work as a continuation of this broad effort to ``port'' common statistical methods and models to single-cell count data. Our focus, however, is on regression rather than dimension reduction: we extend the classical errors-in-variables model to response distributions and sources of measurement error that are exponential family-distributed.

The closest parallels to our work in the statistical methodology literature are Gr\"{u}n \& Leisch \cite{Grun2008} and Ibrahim \cite{Ibrahim1990}. Gr\"{u}n \& Leisch derived a method for estimation and inference in a $k$-component mixture of GLMs. While we prefer to view GLM-EIV as a generalized errors-in-variables method,  the GLM-EIV model is equivalent to a two-component mixture of \textit{products} of GLM densities. Ibrahim proposed a procedure for fitting GLMs in the presence of missing-at-random covariates. Our method, by contrast, involves fitting two conditionally independent GLMs in the presence of a totally latent covariate. Thus, while Ibrahim and Gr\"{u}n \& Leisch are helpful references, our estimation and inference tasks are more complex than theirs.

The genomics literature has produced several applied methods for single-cell CRISPR screen analysis. In a prior work we developed SCEPTRE \cite{Barry2020}, a custom implementation of the conditional randomization test \cite{Candes2018, Liu2021} tailored to single-cell CRISPR screen data. SCEPTRE tests whether a given perturbation is associated with the change in expression of a given gene, adjusting for sources of confounding and ensuring robustness to expression model misspecification. Other applied methods for single-cell CRISPR screen analysis include MIMOSCA \cite{Dixit2016} and scMAGeCK \cite{Yang2019}. These methods, like SCEPTRE, focus on hypothesis testing (rather than estimation), but unlike SCEPTRE, they ignore the countedness of the data and are unable to handle confounders. In this work we tackle a set of analysis challenges that are complimentary to the challenges addressed by SCEPTRE. Most importantly, we seek to \textit{estimate} (with confidence) the effect size of a perturbation on gene expression change, \blue{a statistical objective unattainable within the nonparameteric hypothesis testing framework of SCEPTRE.}

\subsection{Assay overview}

\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{../../figures/analysis_challenges/plot.pdf}
	\caption{\textbf{Experimental design and analysis challenges}: \textbf{a,} Experimental design. For a given perturbation (e.g., the perturbation indicated in blue), we partition the cells into two groups: perturbed and unperturbed. Next, for a given gene, we conduct a differential expression analysis across the two groups, yielding an estimate of the impact of the given perturbation on the given gene. \textbf{b,} DAG representing all variables in the system. The perturbation (latent) impacts both gene expression and gRNA expression; technical factors act as nuisance variables, also impacting gene and gRNA expression. The target of estimation is the effect of the perturbation on gene expression. \textbf{c,} Schematic illustrating the ``background read'' phenomenon. Due to errors in the sequencing process, unperturbed cells exhibit a nonzero gRNA count distribution (bottom). The target of estimation is the change in mean gene expression in response to the perturbation (top). \textbf{d}, Example data on four cells for a given perturbation-gene pair. Note that (i) the perturbation is unobserved, and (ii) the gene and gRNA data are discrete counts.}
	\label{analysis_challenges}
\end{figure}

There are several broad classes of single-cell CRISPR screen assays, each suited to answer a different set of biological questions \cite{Gasperini2019,Datlinger2021,Mimitou2019}. In this work we focus on so-called high-multiplicity of infection (MOI) single-cell CRISPR screens. We expect the ideas that we develop for this assay to apply (with some effort) to other classes of single-cell CRISPR screens as well. In this section we motivate high MOI single-cell screens, overview the experimental protocol, and present relevant analysis challenges.

The human genome consists of genes, enhancers (segments of DNA that regulate the expression of one or more genes), and other genomic elements (that are not of importance to the current discussion). Genome-wide association studies (GWAS) have revealed that the majority ($>90\%$) of variants associated with diseases lie outside genes and (very likely) inside enhancers \cite{Gallagher2018}. These noncoding variants are thought to contribute to disease by modulating the expression one or more disease-relevant genes. We do not know the gene (or genes) through which most noncoding variants exert their effect, limiting the interpretability of GWAS results. A central open challenge in genetics, therefore, is to link enhancers that harbor GWAS variants to the genes that they target at genome-wide scale \cite{Gasperini2020}.

The most promising biotechnology for solving this challenge are high MOI single-cell CRISPR screens. High MOI single-cell CRISPR screens combine CRISPR interference (CRISPRi) -- a version of CRISPR that represses a targeted region of the genome -- with single-cell sequencing. The experimental protocol is as follows. First, the scientist develops a library of several hundred to several thousand CRISPRi perturbations, each designed to target a candidate enhancer for repression. The scientist then cultures tens or hundreds of thousands of cells and delivers the CRISPRi perturbations to these cells. The perturbations assort into the cells randomly, with each cell receiving on average 10-40 distinct perturbations. Conversely, a given perturbation enters about 0.1-2\% of cells. 

After waiting several days for CRISPRi to take effect, the scientist profiles each cell's transcriptome (i.e., its gene expressions) and the set of perturbations that it received. Finally, the scientist conducts perturbation-to-gene association analyses. Figure \ref{analysis_challenges}a depicts this process schematically, with colored bars (blue, red, and purple) representing distinct perturbations. For a given perturbation (e.g., the perturbation represented in blue), the scientist partitions the cells into two groups: those that received the perturbation (top) and those that did not (bottom). Next, for a given gene, the scientist runs a differential expression analysis across the two groups of cells, producing an estimate for the magnitude of the gene expression change in response to the perturbation. If the estimated change in expression is large, the scientist can conclude that the enhancer \textit{targeted} by the perturbation exerts a strong regulatory effect on the gene. This procedure is repeated for a large set of preselected perturbation-gene pairs. \blue{The enhancer-by-enhancer approach is valid because the perturbations assort into cells approximately independently of one another.}

\subsection{Analysis challenges}
\blue{High MOI single-cell CRISPR screens present several statistical challenges, four of which we highlight here.} Throughout, we consider a single perturbation-gene pair. First, the ``treatment'' variable -- i.e., the presence or absence of a perturbation -- cannot be directly observed. Instead, perturbed cells transcribe molecules called  \textit{guide RNAs} (or \textit{gRNAs}) that serve as indirect proxies of perturbation presence. We must leverage these gRNAs to impute (explicitly or implicitly) perturbation assignments onto the cells (Figure \ref{analysis_challenges}b). \blue{Second, ``technical factors'' -- sources of variation that are experimental rather than biological in origin -- impact both gene expression and gRNA expression (Figure \ref{analysis_challenges}b). Technical factors act as confounders in the measurement process and therefore must be accounted for in differential expression models.} Third, the gene and gRNA data are sparse, discrete counts. Therefore, classical statistical approaches that assume Gaussianity or homoscedasticity are inapplicable. Finally, and most subtly, sequenced gRNAs sometimes are mapped to cells that have not received a perturbation. This phenomenon, which we call the ``background read'' phenomenon, results from errors in the sequencing and alignment processes \cite{Replogle2020}. The marginal distribution of the gRNA counts is best conceptualized as a mixture model (Figure \ref{analysis_challenges}c; Gaussian distributions used for illustration purposes only). \blue{Unperturbed and perturbed cells both exhibit nonzero gRNA count distributions, but this distribution overall is greater for perturbed cells.} Figure \ref{analysis_challenges}d shows example data on four (of possibly tens or hundreds of thousands of) cells. \blue{The analysis objective is to leverage the gene expressions and gRNA counts to estimate the effect of the (latent) perturbation on gene expression, accounting for the technical factors.}

In this work we analyze two large-scale, high MOI, single-cell CRISPR screen datasets published by Gasperini et al.\ and Xie et al. Gasperini (resp., Xie) targeted approximately 6,000 (resp., 500) candidate enhancers in a population of approximately 200,000 (resp., 100,000) cells. Gasperini additionally designed 381 positive control, gene-targeting perturbations and 50 non-targeting, negative control perturbations to assess method sensitivity and specificity.

\section{Thresholding method}
\blue{In this section we study thresholded regression from empirical and theoretical perspectives, uncovering several limitations of the method. Gasperini and Xie both imputed perturbation identities onto the cells via thresholding, but they carried out the subsequent differential expression analysis in different ways: Gasperini used negative binomial regression, whereas Xie used nonparametric independence testing. These two strategies pose similar challenges, but we investigate Gasperini's variant of the thresholding method, as it relates most closely to GLM-EIV.}

Let $n \in \N$ be the number of cellxs assayed in the experiment. Consider a single perturbation and a single gene. For cell $i \in \{1, \dots, n\}$, let $m_i \in \N$ be the number of gene transcripts sequenced; let $g_i \in \N$ be the number of gRNA transcripts sequenced; let $l^m_i \in \N$ be the number of gene transcripts sequenced across \textit{all} genes (the library size); and finally, let $z_i \in \R^{d-1}$ be the cell-specific technical factors (e.g., sequencing batch, percent mitochondrial reads, etc.) The letters ``m,'' ``g'', and ``l'' stand for ``mRNA,'' ``gRNA,'' and ``library,'' respectively. The thresholding method is defined as follows:
\begin{itemize}
\item[1.] For a given threshold $c \in \N$, let the imputed perturbation assignment $\hat{p}_i \in \{0, 1\}$ be $$\begin{cases} \hat{p}_i = 0 \textrm{ if } g_i < c, \\ \hat{p}_i = 1 \textrm{ if } g_i \geq c. \end{cases}$$
\item[2.] Assume that $m_i$ is related to $\hat{p}_i, l^m_i,$ and $z_i$ through the following GLM:
$$m_i | (\hat{p}_i, z_i, l^m_i) \sim \textrm{NB}_{\theta^m}(\mu_i),$$
\begin{equation}\label{thresh_glm}
\log(\mu_i) = \beta^m_0 + \beta^m_1 \hat{p}_i + \gamma^T_m z_i + \log\left( l_i^m\right),
\end{equation}
where (i) $NB_\theta(\mu_i)$ is a negative binomial distribution with mean $\mu_i$ and known size parameter $\theta^m$; (ii) $\beta^m_0 \in \R, \beta^m_1 \in \R,$ and $\gamma_m \in  \R^{d-1}$ are unknown parameters; and (iii) $\log(l_i^m)$ is an offset term. Fit a GLM to obtain estimates of the parameters.

\item[3.] Compute a $p$-value and confidence interval for the target of inference $\beta^m_1$.
\end{itemize}
\blue{We include the library size $l^m_i$ as an offset term in (\ref{thresh_glm}) so that $\beta^m_0 + \beta^m_1 \hat{p}_i + \gamma^T_m z_i$ can be interpreted as a relative expression: exponentiating both sides of (\ref{thresh_glm}), we obtain
$$\mu_i = \exp \left( \beta^m_0 + \beta^m_1 \hat{p}_i + \gamma^T_m z_i \right) l_i^m.$$ We see that $\exp \left( \beta^m_0 + \beta^m_1 \hat{p}_i + \gamma^T_m z_i \right)$ is the \textit{fraction} of all transcripts sequenced in the cell produced by the gene under consideration.}

The biological interpretation for the target of inference $\beta^m_1$ as follows:  $\beta^m_1$ is the log-transformed fold change in gene expression in response to the perturbation, \blue{controlling for} the technical factors. Fold change (obtained by exponentiating $\beta^m_1$) is the ratio of the mean gene expression in cells that were perturbed to the mean gene expression in cells that were not perturbed. $\exp(\beta^m_1) = 1$ indicates no change in mean expression, while $\exp(\beta^m_1) > 1$ and $\exp(\beta^m_1) < 1$ indicate an increase and decrease in expression in response to the perturbation, respectively (accounting for technical factors).

\subsection{Empirical challenges of thresholding method}\label{sec:thresholding_empirical}

We examined the behavior of the thresholding method on real data and uncovered attenuation bias and bias-variance tradeoff effects. We applied the thresholding method to analyze the set of 381 positive control perturbation-gene pairs in the Gasperini dataset. The positive control pairs consisted of perturbations that targeted gene transcription start sites (TSSs) for inhibition. Repressing the TSS of a given gene decreases its expression; therefore, the positive control pairs \textit{a priori} are expected to exhibit a strong, negative log fold change in expression.

To investigate the sensitivity of the thresholding method to the selected threshold, we deployed the thresholding method on the positive control data using three different thresholds: 1, 5, and 20. We found that the chosen threshold substantially impacted the results (Figure \ref{thresholding_empirical}a-b). Estimates for log fold change produced by threshold = 1 were smaller in magnitude than those produced by threshold = 5. (Equivalently, estimates for \textit{raw} fold change were closer to the baseline of $1$ for threshold = 1; Figure \ref{thresholding_empirical}a.) Estimates produced by threshold = 5 and threshold = 20 were more concordant, but threshold = 20 yielded slightly larger effect sizes (Figure \ref{thresholding_empirical}b).

We reasoned that thresholded regression systematically underestimated effect sizes on the positive control pairs, especially for small thresholds (an example of \textit{attenuation bias}). For a given perturbation, the vast majority ($>98\%$) of cells are unperturbed. This imbalance leads to an asymmetry: misclassifying \textit{unperturbed} cells as \textit{perturbed} is intuitively ``worse'' than misclassifying \textit{perturbed} cells as \textit{unperturbed}. Misclassified unperturbed cells contaminate the set of truly perturbed cells, leading to attenuation bias; by contrast, misclassified perturbed cells are swamped in number and ``neutralized'' by the truly unperturbed cells. Setting the threshold to a large number reduces the unperturbed-to-perturbed misclassification rate, decreasing bias.

We hypothesized, however, that the reduction in bias conferred by selecting a large threshold comes at the cost of increasing the variance of the estimator. To investigate, we compared $p$-values and confidence intervals produced by threshold = 5 and threshold = 20 for the target of inference $\beta^m_1$. We found that threshold = 5 yielded smaller (i.e., more significant) $p$-values and narrower confidence intervals than did threshold = 20 (Figure \ref{thresholding_empirical}c-d). We concluded that the threshold controls a bias-variance tradeoff: as the threshold increases, bias of the estimator decreases and variance increases.

Finally, to determine whether there is an ``obvious'' location at which to draw the threshold, we examined the empirical gRNA count distributions and checked for bimodality. Figures \ref{thresholding_empirical}e and \ref{thresholding_empirical}f display the empirical distribution of a randomly-selected gRNA from the Gasperini and Xie datasets, respectively (counts of $0$ omitted). The distributions peak at $1$ and then taper off gradually; there does not exist a sharp boundary that cleanly separates the perturbed from the unperturbed cells. Overall, we concluded that the thresholding method faces several challenges: (i) the threshold is a tuning parameter that significantly impacts the results; (ii) the threshold mediates an intrinsic bias-variance tradeoff; and (iii) the gRNA count distributions do not imply a clear threshold selection strategy.

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{../../figures/thresholding_empirical/plot}
	\caption{\textbf{Empirical challenges of thresholded regression.} \textbf{a-b,} Fold change estimates produced by threshold = 1 versus threshold = 5 (a) and threshold = 20 versus threshold = 5 (b). The selected threshold substantially impacts the results. \textbf{c-d,} $p$-values (c) and CI widths (d) produced by threshold = 20 versus threshold = 5. The latter threshold yields more confident estimates. \textbf{e-f}, Empirical distribution of randomly-selected gRNA from Gasperini (e) and Xie (f) data (0 counts not shown). The gRNA data do not appear to imply an obvious threshold selection strategy.}
	\label{thresholding_empirical}
\end{figure}
\newpage

\subsection{Theoretical challenges of thresholding method}\label{sec:thresholding_theory}

Next, we study the thresholding method from a theoretical perspective, recovering in precise mathematical terms the attenuation bias and bias-variance tradeoff effects uncovered on real data, as well as several other interesting phenomena.  We work in an idealized Gaussian setting. Suppose that we observe gRNA and gene expression data $\{(g_1, m_1), \dots, (g_n, m_n)\}$ on $n \in \N$ cells from the following model:

\begin{equation}\label{theoretical_model}
\begin{cases}
m_i = \beta^m_0 + \beta^m_1 p_i + \ep_i \\
g_i = \beta^g_0 + \beta^g_1 p_i + \tau_i \\
p_i \sim \textrm{Bern}(\pi) \\
\ep_i, \tau_i \sim N(0,1) \\
p_i \indep \tau_i \indep \ep_i.
\end{cases}
\end{equation}
For a given threshold $c \in \R$, the imputed perturbation assignment $\hat{p}_i$ is given by $\hat{p}_i = \mathbb{I}(g_i \geq c).$ The thresholding estimator $\hat{\beta}^m_1$ for $\beta^m_1$ is $$\hat{\beta}^m_1 = \frac{\sum_{i=1}^n (\hat{p}_i - \overline{\hat{p}}) (m_i - \overline{m})}{\sum_{i=1}^n (\hat{p}_i - \overline{\hat{p}})^2 }.$$

\begin{proposition}\label{prop:convergence}
 The almost sure limit (as $n \to \infty$) of $\hat{\beta}^m_1$ is
\begin{equation}\label{thresh_est_intercepts}
\hat{\beta}^m_1 \xrightarrow{a.s.} \beta^m_1 \left(\frac{ \pi( \omega - \E[ \hat{p}_i ])}{ \E[\hat{p}_i] (1 - \E[\hat{p}_i])}\right),
\end{equation}
where
$$\begin{cases}
\E[\hat{p}_i] = \zeta(1-\pi) + \omega\pi , \\
\omega = \Phi\left(\beta_1^g + \beta_0^g -c \right) ,\\ \zeta = \Phi\left( \beta^g_0 - c \right). \\
\end{cases}$$
\end{proposition}
Let $\gamma: \R^4 \to \R$ be defined by
$$ \gamma(\beta^g_1, \pi, c, \beta^g_0) = \frac{\pi (\omega - \E[\hat{p}_i])}{\E[\hat{p}_i] (1 - \E[\hat{p}_i]) }.$$ We call $\gamma$ the ``attenuation function.'' Observe that \begin{itemize}
\item[i.] $\gamma$ does not depend on $\beta^m_1$ or $\beta^m_0$, and
\item[ii.] $\hat{\beta}^m_1 \xrightarrow{a.s.} [\gamma(\beta_0^g, \beta_1^g, c, \pi)] \beta^m_1.$
\end{itemize}
Let $b: \R^4 \to \R$ be the asymptotic relative bias of $\hat{\beta}^m_1$:
\begin{multline*}
b(\beta^g_1, \pi, c, \beta^g_0) = \left(\frac{1}{\beta^m_1}\right) \lim_{n\to\infty} \left(\beta^m_1 - \E[\hat{\beta}^m_1]\right) = \left( \frac{1}{\beta^m_1}\right)\left(\beta^m_1 - \E \left(\lim_{a.s.} \hat{\beta}^m_1 \right)\right) \\ = \frac{1}{\beta^m_1} \left(\beta^m_1 - \gamma(\beta^g_1, \pi, c, \beta^g_0)\beta^m_1\right) = 1 - \gamma(\beta^g_1, \pi, c, \beta^g_0),
\end{multline*}
where $\lim_{a.s.}$ denotes a.s.\ convergence. The asymptotic relative bias vanishes when the attenuation function equals $1$.

\begin{center}
\textbf{Bias as a function of threshold (Panel a)}
\end{center}

To investigate the basic question of ``What is a good threshold selection strategy?'', we study the relationship between the asymptotic relative bias $b$ of $\hat{\beta}^m_1$ and the selected threshold $c$. For simplicity, we begin by setting the perturbation probability $\pi$ to $1/2$. Let $c_\textrm{bayes} \in \R$ be the Bayes-optimal decision boundary for classifying cells as perturbed or unperturbed, i.e. $$c_\textrm{bayes} = \argmin_{c \in \R} \P(\hat{p}_i \neq p_i).$$ Simple algebra shows that $c_\textrm{bayes} = \beta_0^g + (1/2) \beta^g_1.$ Below, we give several results for the asymptotic relative bias $b$ of $\hat{\beta}^m_1$. We refer throughout to Figure \ref{thresholding_theoretical}a, which displays plots of asymptotic relative bias versus threshold for different values of $\beta^g_1$. We sometimes refer to ``asymptotic relative bias'' using the shortened term ``bias'' for succinctness. \textcolor{red}{SHOULD WE MOVE PROP 4-6 	TO THE APPENDIX? THESE PROPS ARE LESS IMPORTANT THAN THE OTHERS AND CONTRIBUTE SOMEWHAT LESS TO THE NARRATIVE.}

\begin{itemize}
\item 
\begin{proposition}\label{prop:att_bias} Fix $\pi = 1/2$. For all $(\beta^g_1, c, \beta^g_0) \in \R^3$, the asymptotic relative bias is positive, i.e. 
$$b(\beta^g_1, 1/2, c, \beta^g_0) > 0.$$
\end{proposition}
The thresholding method incurs strict attenuation bias (i.e., it \textit{under}estimates the true effect size) for all choices of the threshold and over all possible values of the model parameters (Figure \ref{thresholding_theoretical}a). Attenuation bias is a common attribute of estimators that ignore measurement in errors-in-variables models \cite{Stefanski2000a}.
\item
\begin{proposition}\label{prop:monotonic} Fix $\pi = 1/2$. The asymptotic relative bias $b$ decreases monotonically in $\beta_1^g$, i.e.
	$$\frac{\partial b}{\partial(\beta^g_1)}\left(\beta^g_1, 1/2, c, \beta^g_0\right) \leq 0.$$
\end{proposition}
This result formalizes the intuition that the problem becomes easier as the gRNA mixture distribution becomes increasingly well-separated. To visualize Proposition (\ref{prop:monotonic}), one can fix a threshold (e.g., $c = 0$) and scan for bias across the panels.	
\item \begin{proposition}\label{prop:bayes_opt}
For $\pi = 1/2$ and given $(\beta^g_1, \beta^g_0) \in \R^2$, the Bayes-optimal decision boundary $c_\textrm{bayes}$ is a critical value of the bias function $b$, i.e.
$$ \frac{\partial b}{\partial c}\left(\beta^g_1, 1/2, c_\textrm{bayes}, \beta^g_0\right) = 0.$$
\end{proposition}
The Bayes-optimal decision boundary is an optimum (or possibly a saddle point) of the asymptotic relative bias function (Figure \ref{thresholding_theoretical}a, vertical blue lines). Interestingly, $c_\textrm{bayes}$ is in some cases a maximizer of the bias (Figure \ref{thresholding_theoretical}a, left) and in other cases a minimizer of the bias (Figure \ref{thresholding_theoretical}a, right).
\item
\begin{proposition}\label{prop:c_limit_half}
Assume without loss of generality that $\beta^g_1 > 0$, and fix $\pi = 1/2$. As the threshold $c$ tends to infinity, the asymptotic relative bias $b$ tends to $1/2$, i.e.
$$\lim_{c \to \infty} b(\beta^g_1, 1/2, c, \beta^g_0) = 1/2.$$
\end{proposition}
In other words, we always can set the threshold to a large number and attain a relative bias of $1/2$ (Figure \ref{thresholding_theoretical}a, all panels). This result establishes an upper bound on the bias of thresholded regression (under optimal threshold selection strategy).

 \item The following proposition compares the two threshold selection strategies introduced above (i.e., large number versus Bayes-optimal decision boundary) head-to-head.
 \begin{proposition}\label{prop:comparison} Assume without loss of generality that $\beta^g_1 > 0$. For $\beta^g_1 \in [0, 2\Phi^{-1}(3/4))$, we have that $$b(\beta^g_1, 1/2, c_\textrm{bayes}, \beta^g_0) > b(\beta^g_1, 1/2, \infty, \beta^g_0).$$ For $\beta^g_1 = 2\Phi^{-1}(3/4)$, we have that $$ b(\beta^g_1, 1/2, c_\textrm{bayes}, \beta^g_0) = b(\beta^g_1, 1/2, \infty, \beta^g_0).$$ Finally, for $\beta^g_1 \in (2\Phi^{-1}(3/4), \infty)$, we have that
 $$b(\beta^g_1, 1/2, c_\textrm{bayes}, \beta^g_0) < b(\beta^g_1, 1/2, \infty, \beta^g_0).$$
 \end{proposition}
Setting the threshold to a large number yields a smaller bias when $\beta^g_1$ is small (i.e., $\beta^g_1 < 2\Phi^{-1}(3/4) \approx 1.35$; Figure \ref{thresholding_theoretical}a, left); setting the threshold to the Bayes-optimal decision boundary yields a smaller bias when $\beta^g_1$ is large (i.e., $\beta^g_1 > 2\Phi^{-1}(3/4)$; Figure \ref{thresholding_theoretical}a, right); and the two approaches coincide when $\beta^g_1$ is intermediate (i.e., $\beta^g_1 = 2\Phi^{-1}(3/4)$; Figure \ref{thresholding_theoretical}a, middle).
\end{itemize}

These results are subtle, but we can summarize them as follows. First, selecting a threshold that minimizes the bias is challenging, as there is no rule of thumb that we can apply universally (e.g., ``always choose the Bayes-optimal decision boundary'' or ``always choose a large number'') due to the complexity of the bias function. Second, even if we \textit{have} selected a good threshold, we incur nonzero attenuation bias.

\begin{center}
\textbf{Generalizing to $\pi \in [0,1/2]$ (Panel b)}
\end{center}
We generalize the expression for bias when the threshold is large to arbitrary $\pi \in [0,1/2]$:
\begin{proposition}\label{prop:c_limit}
Assume without loss of generality that $\beta^g_1 > 0$. As the threshold $c$ tends to infinity, the asymptotic relative bias $b$ tends to $\pi,$ i.e.
$$ \lim_{ c \to \infty } b(\beta^g_1, \pi, c, \beta^g_0) = \pi.$$
\end{proposition}
In other words, if the perturbation probability is $\pi$, and if we set the threshold to a large number, then the asymptotic relative bias is $\pi$ (Figure \ref{thresholding_theoretical}b). We can understand this result intuitively by considering an extreme example: when $\pi$ is very small (e.g., $\pi = 0.01$), most cells are unperturbed. Therefore, as discussed in Section \ref{sec:thresholding_empirical}, selecting a large threshold minimizes the unperturbed-to-perturbed misclassification rate, reducing bias. % Therefore, in selecting a large threshold, we correctly classify nearly all unperturbed cells as unperturbed; on the other hand, the \textit{perturbed} cells that we misclassify as \textit{unperturbed} are swamped in number by the truly unperturbed cells, resulting in a small bias.

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{../../figures/thresholding_theoretical/plot}
	\caption{\textbf{Theoretical challenges of thresholded regression.} \textbf{a,} Asymptotic relative bias versus threshold for different values of $\beta^g_1$. The bias function is highly nonconvex and strictly nonzero. Vertical blue lines, Bayes-optimal decision boundaries. Across all panels, $\beta^g_0 = 0$ and $\pi = 1/2$. \textbf{b,} Asymptotic relative bias versus $\pi$ when the threshold is set to a large number. The two quantities coincide exactly. \textbf{c,} Bias-variance decomposition for thresholding method in no-intercept model. Bias decreases and variance increases as the threshold tends to infinity. $\beta^g_1 = 1, \beta^m_1 = 1,$ and $\pi = 0.1$.}
	\label{thresholding_theoretical}
\end{figure}

\begin{center}
	\textbf{Bias-variance tradeoff (Panel c)}
\end{center}

Finally, to shed light on the costs of selecting a large threshold, we derive an exact bias-variance decomposition for the thresholding estimator. We consider a slightly simpler, no-intercept version of (\ref{theoretical_model}) for this purpose:
\begin{equation}\label{theoretical_model_no_int}
\begin{cases}
m_i = \beta_m p_i + \ep_i \\
g_i = \beta_g p_i + \tau_i \\
p_i \sim \textrm{Bern}(\pi) \\
\ep_i, \tau_i \sim N(0,1) \\
p_i \indep \tau_i \indep \ep_i.
\end{cases}
\end{equation}
The thresholding estimator $\hat{\beta}_m$ in the no-intercept case is
\begin{equation}\label{thresh_estimator_no_int}
 \hat{\beta}_m = \frac{ \sum_{i=1}^n \hat{p}_i m_i }{\sum_{i=1}^n \hat{p}_i^2}. 
\end{equation}
\begin{proposition}\label{prop:bv_decomp}
The limiting distribution of $\hat{\beta}_m$ is
$$\sqrt{n}(\hat{\beta}_m - l) \xrightarrow{d} N\left(0, \frac{ \beta_m\omega\pi(\beta_m - 2l) + \E[\hat{p}_i](1 + l^2) }{\left(\E[\hat{p}_i]\right)^2} \right),$$ where
$$\begin{cases}
l = \beta_m\omega \pi/[\zeta(1-\pi) + \omega \pi], \\
\E[\hat{p}_i] = \pi \omega + (1-\pi) \zeta, \\
\omega = \Phi(\beta_g - c), \\
\zeta = \Phi(- c).\\
\end{cases}
$$
\end{proposition}
This result yields an exact bias-variance decomposition for $\hat{\beta}_m$ for large $n$ (Figure \ref{thresholding_theoretical}c). As the threshold tends to infinity, the bias decreases and the variance increases, consistent with the intuition that a large threshold reduces the misclassification rate at the cost of decreasing the ``effective sample size.'' The best strategy for maximizing estimation accuracy (as quantified by mean squared error) is to select a threshold that induces moderate bias. A downside of this approach, however, is that constructing valid confidence intervals becomes more challenging.

\subsection{Thresholding method summary}

Empirical and theoretical analyses reveal that the thresholding method poses several challenges: the threshold is a tuning parameter that substantially impacts the results; strict attenuation bias obtains uniformly over the parameter space and for all choices of the threshold; and there does not exist and obvious threshold selection strategy due to (i) the unimodality of the empirical gRNA count distributions and (ii) the existence of a bias-variance tradeoff mediated by the threshold. These difficulties motivate our core research question: \textit{Does modeling the gRNA count distribution directly, thereby circumventing the need to threshold altogether, lead to simpler, more accurate estimation and inference in single-cell CRISPR screen analysis?} To answer this question, we generalize the classical errors-in-variables model to response distributions and sources of measurement error that are exponential family-distributed.

\section{Generalized linear model with errors in variables}

In this section we introduce generalized linear model with errors-in-variables (GLM-EIV), derive estimation and inference procedures for the model, and propose several statistical accelerations to reduce the cost of fitting the model.

\subsection{Model}
\subsubsection*{Negative binomial model}
Building on the work of several previous authors \cite{Townes2019,Svensson2020,Hafemeister2019}, Sarkar and Stephens \cite{Sarkar2021} proposed a simple strategy for modeling for single-cell gene expression data, which, in the framework of the negative binomial GLM, is equivalent to using the log-transformed library size as an offset term (as in (\ref{thresh_glm})). We generalize Sarkar and Stephens' approach to model \textit{both} gene and gRNA modalities. To this end, let the latent variable $p_i \in \{0,1\}$ indicate whether cell $i \in \{1, \dots, n\}$ was perturbed. We model the gene expression counts according to
\begin{equation}\label{glmeiv_model_1}
m_i |(p_i, z_i, l^m_i) \sim \textrm{NB}(\mu_i^m),
\end{equation}
\begin{equation}\label{glmeiv_model_2}
\log(\mu^m_i) = \beta^m_0 + \beta^m_1 p_i + \gamma_m^T z_i + \log(l^m_i),
\end{equation}
where $\theta^m > 0$ is a known negative binomial size parameter, and $\beta^m_0 \in \R, \beta^m_1 \in \R,$ and $\gamma_m \in \R^{d - 2}$ are unknown constants. The model (\ref{glmeiv_model_1}) is identical to the thresholding model (\ref{thresh_glm}), but the imputed perturbation indicator $\hat{p}_i$ is replaced by the latent perturbation indicator $p_i$. Next, let $l^g_i \in \N$ be the number of gRNA transcripts sequenced across \textit{all} gRNAs in cell $i$ (i.e., the gRNA library size). The model for the gRNA counts is 
\begin{equation}\label{glmeiv_model_3}
g_i | (p_i, z_i, l^g_i) \sim \textrm{NB}_{\theta^g}\left(\mu_i^g\right),
\end{equation}
\begin{equation}\label{glmeiv_model_4}
\log(\mu_i^g) = \beta^g_0 + \beta^g_1p_i + \gamma^T_g z_i + \log(l^g_i),
\end{equation}
where, similar to above, $\theta^g > 0$ is a known negative binomial size parameter, and $\beta^g_0 \in \R, \beta^g_1 \in \R, \gamma_g \in \R^{d - 2}$ are unknown constants. We use a negative binomial GLM to model the gRNA counts because gRNA molecules are transcribed in the cell in the same way that gene transcripts are \cite{Datlinger2017,Hill2018}. Finally, we model the marginal perturbation probability as
\begin{equation}\label{glmeiv_model_5}
p_i \sim \textrm{Bern}(\pi),
\end{equation} where $\pi \in (0,1/2]$. 
 Together, (\ref{glmeiv_model_1}, \ref{glmeiv_model_2}, \ref{glmeiv_model_3}, \ref{glmeiv_model_4}, \ref{glmeiv_model_5}) define the standard GLM-EIV model. The terms $(\beta^m_0 + \beta^m_1p_i + \gamma^T_m z_i)$ and $(\beta^g_0 + \beta^g_1 p_i + \gamma^T_g z_i)$ can be interpreted as relative gene and gRNA expressions, similar to the analogous term in the thresholding model. Likewise, the target of inference $\beta^m_1$ is the log fold change in gene expression in response to the perturbation, accounting for technical factors.
 
\subsubsection*{Full GLM-EIV model}
To provide greater modeling flexibility, we generalize the GLM-EIV model to arbitrary exponential family response distributions and link functions. To increase notational compactness, let $\tilde{x_i} = [1, p_i, z_i]^T \in \R^d$ be the vector of covariates (including an intercept term) for the $i$th cell. (We use the tilde as a reminder that the vector is partially unobserved.) Let $\beta_m = [\beta^m_0, \beta^m_1, \gamma_m]^T \in \R^d$ and $\beta_g = [\beta^g_0, \beta^g_1, \gamma_g]^T \in \R^d$ be the unknown coefficient vectors corresponding to the gene and gRNA expression models, respectively. Finally, let $o^m_i$ and $o^g_i$ be the (possibly zero) offset terms for the gene and gRNA models; in practice, we typically set $o^m_i$ and $o^g_i$ to $\log(l^m_i)$ and $\log(l^g_i)$, respectively.

We use a GLM approach to model the gene and gRNA expressions. Considering first the gene expression model, let the $i$th linear component $l^m_i$ of the model be $$l^m_i = \langle \tilde{x}_i, \beta_m \rangle + o^m_i.$$ Let the mean $\mu^m_i$ of the $i$th observation be
$$r_m(\mu^m_i) = l^m_i,$$ where $r_m:\R \to \R$ is a strictly increasing, differentiable link function. Let $\psi_m: \R \to \R$ be the differentiable, cumulant-generating function of the selected exponential family distribution. We can express the canonical parameter $\eta^m_i$ in terms of $\psi_m$ and $r_m$ by
$$\eta^m_i = \left([\psi'_m]^{-1} \circ r^{-1}_m\right)(l_i^m) := h_m(l_i^m).$$ Finally, let $c_m: \R \to \R$ be the carrying density of the selected exponential family distribution. The density $f_m$ of $m_i$ conditional on the the canonical parameter $\eta_i$ is
$$f_m(m_i; \eta^m_i) = \exp\left\{ m_i \eta^m_i - \psi_m(\eta^m_i) + c_m(m_i) \right\}.$$ The function $c_m$ does not appear in the log likelihood of $m_i$; therefore, the only functions relevant to inference are $\psi_m$ and $r_m$.

Let the terms $l^g_i, o^g_i, \mu^g_i, \eta^g_i, \psi_g, r_g, h_g$ and $c_g$ be defined in an analogous way for the gRNA model:
$$
\begin{cases}
l^g_i = \langle \tilde{x}_i, \beta_g \rangle + o^g_i,\\
r_g(\mu^g_i) = l^g_i,\\
\eta^g_i = \left([\psi'_g]^{-1} \circ r^{-1}_g\right)(l_i^g) := h_g(l_i^g).
\end{cases}
$$
The density $f_g$ of $g_i$ given the canonical parameter is
$$f_g(m_i; \eta^g_i) = \exp\left\{g_i \eta^g_i - \psi_g(\eta^g_i) + c_g(g_i)\right\}.$$
Finally, the unobserved variable $p_i$ is assumed to follow a Bernoulli distribution with mean $\pi \in (0, 1/2]$. Its marginal density $f_p$ is given by
$$f_p(p_i) = \pi^{p_i}(1-\pi)^{1 - p_i}.$$
The unknown parameters in the model are
$\theta = [\beta_m, \beta_g, \pi]^{T}  \in \R^{2d + 1}.$

\subsubsection*{Notation}
We briefly introduce notation that we will use throughout. For $j \in \{0,1\}$, let $\tilde{x}_i(j) := [1, j, z_i]^T$ denote the value of $\tilde{x}_i$ that results from setting $p_i$ to $j$. Next, let  $l^m_i(j)$, $\eta^m_i(j),$ and $\mu^m_i(j)$ be the values of $l^m_i$, $\eta^m_i$, and $\mu^m_i$, respectively, that result from setting $p_i$ to $j$, i.e.,
$$
\begin{cases}
l^m_i(j) := \langle \tilde{x}_i(j), \beta_m \rangle + o^m_i \\ \eta^m_i(j) := h_m(l^m_i(j)) \\
\mu_i^m(j) = r_m^{-1}(l^m_i(j)).
\end{cases}
$$
Let the corresponding gRNA quantities $l^g_i(j)$, $\eta_i^g(j)$, and $\mu^g_i(j)$ be defined analogously. Define the design matrix $\tilde{X} \in \R^{n \times d}$ by
$$ \tilde{X} := \begin{bmatrix}
\tilde{x}_1^T \\ \tilde{x}_2^T \\ \vdots \\ \tilde{x}_n^T
\end{bmatrix} = 
\begin{bmatrix}
1 & p_1 & z_1 \\
1 & p_2 & z_2 \\ 
\vdots & \vdots & \vdots \\
1 & p_n & z_n
\end{bmatrix}.$$ For $j \in \{0,1\}$, let $\tilde{X}(j) \in \R^{n \times d}$ be the matrix that results from setting $p_i$ to $j$ for all $i \in \{1, \dots, n\}$ in $\tilde{X}$.

Next, let $[m,m]^T \in \R^{2n}$ denote the vector that results from concatenating $m := [m_1, \dots, m_n]^T \in \R^n$ to itself, i.e.
$$ [m,m]^T := [\underbrace{m_1, m_2 \dots, m_{n-1}, m_n}_\textrm{first copy of $m$}, \underbrace{m_1, m_2, \dots, m_{n-1}, m_n}_\textrm{second copy of $m$}].$$ Define $[g,g]^T$, $[o^g,o^g]^T$, and $[o^m,o^m]^T$ similarly. Finally, let $\begin{bmatrix} \tilde{X}(0) \\ \tilde{X}(1) \end{bmatrix}$ denote the $\R^{2n \times d}$ matrix that results from vertically concatenating $\tilde{X}(0)$ and $\tilde{X}(1)$.

\subsubsection*{Log likelihood and model properties}
We derive the log-likelihood of the GLM-EIV model. We conduct estimation and inference \textit{conditional} on the library sizes and technical factors $l^m_i, l^g_i,$ and $z_i$; therefore, we treat these quantities as fixed constants. We assume that the gene expression $m_i$ and gRNA expression $g_i$ are \textit{conditionally independent} given the perturbation $p_i$. The joint density $f$ of $(m_i, g_i, p_i)$ given $\theta$ is
\begin{equation}\label{full_density}
f(m_i,g_i,p_i; \theta) = f_m(m_i | p_i) f_g(g_i | p_i) f_p(p_i) \\ = \pi^{p_i}(1-\pi)^{1-p_i} f_m(m_i; \eta^m_i) f_g(g_i; \eta^g).
\end{equation}
The log-likelihood is
\begin{multline}\label{full_log_lik}
\mathcal{L}(\theta; m, g, p) = \sum_{i=1}^n \log\left( \pi^{p_i}(1-\pi)^{1-p_i} \right) \\ + \sum_{i=1}^n \log\left( f_m(m_i; \eta^m_i)\right) + \sum_{i=1}^n \log\left( f_g(g_i; \eta_i^g) \right). \end{multline}
Integrating over the unobserved variable $p_i$, we can write the marginal density $f$ of $(m_i, g_i)$ as
\begin{equation}\label{marginal_density}
f(m_i, g_i; \theta) = (1-\pi) f(m_i; \eta^m_i(0)) f(g_i; \eta^g_i(0)) + \pi f(m_i; \eta^m_i(1)) f(g_i; \eta^g_i(1)).
\end{equation}
Finally, the marginal log-likelihood is
\begin{multline}\label{marginal_log_lik}
\mathcal{L}(\theta; m, g) \\ = \sum_{i=1}^n \log\left[ (1-\pi) f(m_i; \eta^m_i(0)) f(g_i; \eta^g_i(0)) + \pi f(m_i; \eta^m_i(1)) f(g_i; \eta^g_i(1)) \right]
\end{multline}

We see from (\ref{marginal_density}) that the GLM-EIV model is equivalent to a two-component mixture of \textit{products} of GLM densities. Additionally, the GLM-EIV model is a generalization of the classical errors-in-variables model (when the predictor is binary). Suppose that we observe data $(x_1, y_1), \dots, (x_n, y_n)$ from the following model:
\begin{equation}\label{classical_eiv}
\begin{cases}
y_i = \beta_0 + \beta_1 x^*_i + \ep_i \\
x_i = x^*_i + \tau_i,
\end{cases}
\end{equation}
where $x^*_i \sim \textrm{Bern}(\pi), \ep_i \sim N(0,1), \tau_i \sim N(0,1),$ and $\ep_i$,$\tau_i$, and $x^*_i$ are independent. The model (\ref{classical_eiv}) is a special case of the GLM-EIV model. More generally, GLM-EIV allows the use of regression functions (that optionally include covariates and use nonlinear links) to model $y_i$ and $x_i$; $y_i$ and $x_i$ need not be Gaussian.

\subsection{Estimation and inference}

We derive an EM algorithm (Algorithm \ref{algo:em_full}) to estimate the parameters of the GLM-EIV model. The E step entails computing the membership probability (i.e., the probability of perturbation) of each cell. The membership probability $T_i(1)$ of cell $i \in \{1, \dots, n\}$ given the current parameter estimates $(\beta_m^\textrm{(t)}, \beta_g^\textrm{(t)}, \pi^\textrm{(t)})$ and observed data $(m_i, g_i)$ is
$$T_i(1) = \P(p_i = 1| M_i = m_i, G_i = g_i, \beta^\textrm{(t)}_m, \beta^\textrm{(t)}_g, \pi^\textrm{(t)}).$$ We can calculate this quantity by applying (i) Bayes rule, (ii) the conditional independence property of $M_i$ and $G_i$, (iii) the density of $M_i$ and $G_i$, and (iv) a log-sum-exp-type trick to ensure numerical stability. Next, we produce updated estimates $\pi^{(t +1)}$, $\beta_g^{(t+1)}$, and $\beta_m^{(t+1)}$ of the parameters by maximizing the M step objective function. It turns out that maximizing the objective function is equivalent to setting $\pi^{\textrm{(t+1)}}$ to the mean of the current membership probabilities and setting $\beta_g^{(t+1)}$ and $\beta_m^{(t+1)}$ to the fitted coefficients of a GLM weighted by the current membership probabilities (Algorithm \ref{algo:em_full}). We iterate through the E and M steps until the marginal log likelihood (\ref{marginal_log_lik}) converges (see appendix for full details). Our EM algorithm is reminiscent of (but distinct from) that of Ibrahim \cite{Ibrahim1990}, who also applied weighted GLM solvers to carry out the M step.

\begin{algorithm}
	\caption{EM algorithm for GLM-EIV model.}\label{algo:em_full}
	\begin{algorithmic}
		\Require Pilot estimates $\beta^\textrm{curr}_m, \beta^\textrm{curr}_g,$ and $\pi^\textrm{curr}$; data $[m_1, \dots, m_n]$, $[g_1, \dots, g_n]$, $[o^m_1, \dots, o^m_n]$, $[o^g_1, \dots, o^g_n]$, and $[z_1, \dots, z_n]$.
		\While{Not converged}
			\For{$i \in \{1, \dots, n\}$} \Comment{E step}
			\State $T_i(1) \gets \P\left(p_i = 1 |M_i = m_i, G_i = g_i, \beta_m^\textrm{curr}, \beta_g^\textrm{curr}, \pi^\textrm{curr} \right)$
			\State $T_i(0) \gets 1 - T_i(1)$
		 \EndFor
		 \State $\pi^{\textrm{curr}} \gets (1/n) \sum_{i=1}^n T_i(1)$ \Comment{M step}
		 \State $w \gets [T_1(0), T_2(0), \dots, T_n(0), T_1(1), T_2(1), \dots, T_n(1)]^T$
		 \State Fit GLM with responses $[m,m]^T$, offsets $[o^m, o^m]^T$, weights $w$, and design matrix $\begin{bmatrix} \tilde{X}(0) \\ \tilde{X}(1) \end{bmatrix}$; set $\beta_m^\textrm{curr}$ to estimated coefficient vector.
		\State Fit GLM with responses  $[g,g]^T$, offsets $[o^g, o^g]^T$, weights $w$, and design matrix $\begin{bmatrix} \tilde{X}(0) \\ \tilde{X}(1) \end{bmatrix}$; set $\beta_g^\textrm{curr}$ to estimated coefficient vector.
		\State Compute marginal likelihood given $\beta_m^\textrm{curr}$, $\beta_g^\textrm{curr}$, and $\pi^\textrm{curr}$.
		\EndWhile
		\State $\hat{\beta}_m \gets \beta_m^\textrm{curr}$; $\hat{\beta}_g \gets \beta_g^\textrm{curr}$; $\hat{\pi} \gets \pi^\textrm{curr}$.
		\State \textbf{return} $(\hat{\beta}_m, \hat{\beta}_g, \hat{\pi})$
	\end{algorithmic}
\end{algorithm}

After fitting the model, we perform inference on the estimated parameters. The easiest approach, given the complexity of the log likelihood, would be to run a parametric bootstrap on the fitted model. This strategy, however, is prohibitively slow, as the data are large and we use an EM algorithm (that, in practice, requires multiple starts) to fit the model. Therefore, we derive an analytic formula for the asymptotic observed information matrix using Louis's Theorem \cite{Louis2012} (see appendix). Leveraging this analytic formula, we can calculate standard errors (and $p$-values and confidence intervals) quickly, enabling us to perform inference in practice on real, large-scale data.

\subsection{Statistical accelerations}

% We expect some of these tricks to be of independent utility, especially for designing single-cell methods.

% \subsection{Computational implementation}
\section{Simulation studies}

\section{Real data analysis}

\section{Discussion}

\begin{appendices}
\section{Theoretical details for thresholding estimator}
This section contains proofs of the propositions presented Section \ref{sec:thresholding_theory}, ``Theoretical analysis of thresholding estimator.'' The subsections are organized as follows. Section (\ref{sec:notation}) introduces some notation. Section (\ref{sec:convergence}) establishes almost sure convergence of the thresholding estimator in the model (\ref{theoretical_model}), proving Proposition \ref{prop:convergence}. Section (\ref{sec:simplication}) simplifies the expression for the attenuation function $\gamma$, and section (\ref{sec:derivatives})  computes derivatives of $\gamma$ to be used throughout the proofs. Section (\ref{sec:c_limit}) establishes the limit in $c$ of $\gamma$, proving Proposition \ref{prop:c_limit} and as a corollary Proposition \ref{prop:c_limit_half}. Section (\ref{sec:bayes_opt}) establishes that the Bayes-optimal decision boundary is a critical value of $\gamma$, proving Proposition \ref{prop:bayes_opt}, and section (\ref{sec:comparison}) compares the competing threshold selection strategies head-to-head, proving Proposition \ref{prop:comparison}. Section (\ref{sec:monotone}) demonstrates that $\gamma$ is monotone in $\beta^g_1$, proving Proposition \ref{prop:monotonic}, and Section (\ref{sec:att_bias}) establishes attenuation bias of the thresholding estimator, proving Proposition \ref{prop:att_bias}. Finally, Section (\ref{sec:bv_decomp}) derives the bias-variance decomposition of the thresholding estimator in the model (\ref{theoretical_model_no_int}), proving Proposition \ref{prop:bv_decomp}.

% Labels of propositions and theorems
% propositions:
% prop:convergence
% prop:att_bias
% prop:bayes_opt
% prop:c_limit_half
% prop:comparison
% prop:monotonic
% prop:c_limit
% prop:bv_decomp

% sections
% sec:notation
% sec:convergence
% sec:simplication
% sec:derivatives
% sec:c_limit
% sec:bayes_opt
% sec:comparison
% sec:beta_lim
% sec:monotone
% sec:att_bias
% sec:bv_decomp

\subsection{Notation}\label{sec:notation}
 All notation introduced in this subsection (i.e., \ref{sec:notation}) pertains to the Gaussian model with intercepts (\ref{theoretical_model}). Recall that the attenuation function $\gamma: \R^4 \to \R$ is defined by
$$ \gamma(\beta^g_1, c, \pi, \beta^g_0) = \frac{\pi(\omega - \E[\hat{p}_i])}{ \E[\hat{p}_i](1 -\E[\hat{p}_i])},$$ where $$\begin{cases} \E[\hat{p}_i] = \zeta(1-\pi) + \omega\pi, \\
\omega = \Phi\left(\beta_1^g + \beta_0^g - c \right) ,\\ \zeta = \Phi\left( \beta^g_0 - c \right).
\end{cases}$$ Additionally, recall that the asymptotic relative bias function $b: \R^4 \to \R$ is
$$ b(\beta^g_1, c, \pi, \beta^g_0) = 1 - \gamma(\beta^g_1, c, \pi, \beta^g_0).$$ Next, we define the functions $g$ and $h: \R^4 \to \R$ by
\begin{equation}\label{def_g}
g(\beta^g_1, c, \pi, \beta^g_0) = (1-\pi)\left( \Phi(\beta_0^g + \beta_1^g - c)\right) - (1-\pi)\left(\Phi(\beta_0^g - c)\right)\end{equation}
 and
\begin{multline}\label{def_h}
h(\beta^g_1, c, \pi, \beta^g_0) = \left[(1-\pi)\left( \Phi(\beta_0^g - c)\right) + \pi\left(\Phi(\beta^g_0 + \beta^g_1 - c) \right) \right] \cdot \\ \left[(1-\pi)\left( \Phi(c - \beta^g_0) \right) + \pi\left(\Phi(c - \beta_0^g - \beta_1^g) \right) \right].
\end{multline}
We use $f:\R \to \R$ to denote the $N(0,1)$ density, and we denote the right-tail probability probability of $f$ by $\bar{\Phi}$, i.e., 
$$\bar{\Phi}(x) = \int_{x}^{\infty} f = \Phi(-x).$$

The parameter $\beta^g_0$ is a given, fixed constant throughout the proofs. Therefore, to minimize notation, we typically use $\gamma(\beta^g_1, c, \pi)$ (resp., $b(\beta^g_1, c, \pi),$ $g(\beta^g_1, c, \pi),$ $h(\beta^g_1, c, \pi)$) to refer to the function $\gamma$ (resp., $b, g, h$) evaluated at $(\beta^g_1, c, \pi, \beta^g_0)$. Finally, for a given function $r: \R^{p} \to \R$, point $x \in \R^p$, and index $i \in \{1, \dots, p\}$, we use the symbol $D_i r(x)$ to refer to the derivative of the $i$th component of $r$ evaluated at $x$ (\textit{sensu} \cite{fitzpatrick2009}). For example, $D_1 \gamma(\beta^g_1, c, 1/2)$ is the derivative of the first component of $\gamma$ (the component corresponding to $\beta^g_1$) evaluated at $(\beta^g_1, c, 1/2)$. Likewise,  $D_2g(\beta^g_1, c, \pi)$ is the derivative of the second component of $g$ (the component corresponding to $c$) evaluated at $(\beta^g_1, c, \pi).$

\subsection{Almost sure limit of $\hat{\beta}^m_1$}\label{sec:convergence}

We derive the limit in probability of $\hat{\beta}^m_1$ for the Gaussian model with intercepts (\ref{theoretical_model}). Dividing by $n$ in (\ref{thresh_est_intercepts}), we can express $\hat{\beta}^m_1$ as
$$ \hat{\beta}^m_1 = \frac{ \frac{1}{n} \sum_{i=1}^n ( \hat{p}_i - \overline{\hat{p}_i})(m_i - \overline{m})}{ \frac{1}{n} \sum_{i=1}^n (\hat{p}_i - \overline{\hat{p}})}.$$ By weak LLN,
$$ \hat{\beta}^m_1 \xrightarrow{P} \frac{\textrm{Cov}(\hat{p}_i, m_i)}{\V\left(\hat{p}_i\right)}.$$ To compute this quantity, we first compute several simpler quantities:
\begin{itemize}
\item[1.] Expectation of $m_i$: $\E[m_i] = \beta^m_0 + \beta^m_1\pi.$
\item[2.] Expectation of $\hat{p}_i$: \begin{multline*}
\E[\hat{p}_i] = \P\left[\hat{p}_i = 1\right] = \P\left[\beta^g_0 + \beta^g_1 p_i + \tau_i \geq c \right] = \\ \textrm{(By LOTP) } \P\left[ \beta^g_0 + \tau_i \geq c \right]\P\left[p_i = 0\right] + \P\left[ \beta^g_0 + \beta^g_1 + \tau_i \geq c \right] \P[p_i = 1] \\ = \P\left[ \tau_i \geq c - \beta^g_0\right](1- \pi) + \P\left[ \tau_i \geq c - \beta^g_1 - \beta^g_0 \right](\pi) \\ =  \left(\bar{\Phi}(c - \beta^g_0) \right) (1 - \pi) + \left( \bar{\Phi}(c - \beta^g_1 - \beta^g_0) \right)(\pi) = \\  \Phi(\beta^g_0 - c) (1-\pi) + \Phi(\beta^g_1 + \beta^g_0 - c) \pi = \zeta(1-\pi) + \omega \pi.
\end{multline*}
\item[3.] Expectation of $\hat{p}_i p_i$: 
$$\E\left[ \hat{p}_i p_i \right] = \E\left[\hat{p}_i | p_i = 1 \right] \P\left[ p_i =1 \right] = \P\left[ \beta^g_0 + \beta^g_1 + \tau_i \geq c \right] \pi = \omega \pi.$$
\item[4.] Expectation of $\hat{p}_i m_i$:
\begin{multline*}
\E\left[\hat{p}_i m_i\right] = \E[\hat{p}_i (\beta^m_0 + \beta^m_1 p_i + \ep_i)] = \beta^m_0 \E\left[\hat{p}_i\right] + \beta^m_1 \E\left[\hat{p}_i p_i\right] + \E[\hat{p}_i \ep_i] \\ = \beta^m_0 \E[\hat{p}_i] + \beta^m_1 \omega \pi + \E[\hat{p}_i] \E[\ep_i] = \beta^m_0 \E[\hat{p}_i] + \beta^m_1 \omega \pi.
\end{multline*}
\item[5.] Variance of $\hat{p}_i$: Because $\hat{p}_i$ is binary, we have that $\V[\hat{p}_i] = \E[\hat{p}_i]\left(1 - \E[\hat{p}_i]\right) .$
\item[6.] Covariance of $\hat{p}_i, m_i$:
\begin{multline*}
\textrm{Cov}\left(\hat{p}_i, m_i\right) = \E\left[\hat{p}_i m_i\right] - \E[\hat{p}_i] \E[m_i] = \beta^m_0 \E[\hat{p}_i] + \beta^m_1 \omega \pi - \E[\hat{p}_i]( \beta^m_0 + \beta^m_1 \pi)\\ = \beta^m_1 \omega \pi - \E[\hat{p}_i] \beta_1^m \pi = \beta^m_1 \pi \left( \omega - \E[\hat{p}_i]\right).
\end{multline*}
Combining these expressions, we have that
$$ \hat{\beta}^m_1 \xrightarrow{P} \frac{\beta^m_1 \pi (\omega - \E[\hat{p}_i])}{\E[\hat{p}_i](1 - \E[\hat{p}_i])} = \beta^m_1 \gamma(\beta^g_1, c, \pi).$$
\end{itemize}

\subsection{Re-expressing $\gamma$ in a simpler form}\label{sec:simplication}
We rewrite the attenuation fraction $\gamma$ in a way that makes it more amenable to theoretical analysis. We leverage the fact that $f$ integrates to unity and is even. We have that
\begin{multline}\label{thm:gamma_expression_1} \E\left[\hat{p}_i\right] = (1 - \pi) \bar{\Phi}(c - \beta_0^g) + \pi \bar{\Phi}(c - \beta^g_0 - \beta^g_1) \\ = (1 - \pi) \Phi(\beta_0^g - c) + \pi\Phi(\beta^g_0 + \beta^g_1 - c), \end{multline}
 and so \begin{multline}\label{thm:gamma_expression_2} 1 - \E\left[\hat{p}_i\right] = (1 - \pi) + \pi - \E[\hat{p}_i]  = (1-\pi) \left(1 - \bar{\Phi}(c - \beta_0^g)\right)  + \pi \left(1 - \bar{\Phi}(c - \beta^g_0 - \beta^g_1) \right) \\ = (1 - \pi)\Phi(c - \beta^g_0) + \pi \Phi(c - \beta_0^g - \beta_1^g).
\end{multline}
Next,
\begin{equation}\label{thm:gamma_expression_3}
\omega = \Phi(\beta^g_1 + \beta^g_0 - c),\end{equation} and so
\begin{multline}\label{thm:gamma_expression_4}
\omega - \E[\hat{p}_i] = \Phi(\beta^g_1 + \beta^g_0 - c) - (1-\pi)\Phi(\beta^g_0 - c) - \pi \Phi(\beta^g_0 + \beta^g_1 - c)  \\ (1-\pi)\Phi(\beta^g_1 + \beta^g_0 - c)  - (1-\pi)\Phi(\beta^g_0 - c).
\end{multline}

Combining (\ref{thm:gamma_expression_1}, \ref{thm:gamma_expression_2}, \ref{thm:gamma_expression_3}, \ref{thm:gamma_expression_4}), we find that
\begin{multline}\label{gamma_alternative}
\gamma(\beta^g_1, c, \pi) = \frac{\pi(\omega - \E[\hat{p}_i])}{\E[\hat{p}_i](1 - \E[\hat{p}_i])} \\ = \frac{\pi \left[(1 - \pi) \Phi(\beta_0^g + \beta_1^g - c) - (1 - \pi) \Phi(\beta_0^g - c)\right]}{\left[(1-\pi)\Phi(\beta_0^g - c) + \pi \Phi(\beta^g_0 + \beta^g_1 - c) \right] \left[(1 - \pi) \Phi(c - \beta^g_0) + \pi\Phi(c - \beta_0^g - \beta_1^g) \right]}.
\end{multline}
As a corollary, when $\pi = 1/2$,
\begin{multline}\label{gamma_alternative_pi_half}
\gamma(\beta^g_1, c, 1/2)  \\ = \frac{\Phi(\beta_0^g + \beta_1^g - c) - \Phi(\beta_0^g - c) }{\left[\Phi(\beta_0^g - c) +\Phi(\beta^g_0 + \beta^g_1 - c)\right] \left[\Phi(c - \beta^g_0) + \Phi(c - \beta_0^g - \beta_1^g) \right]}.
\end{multline}
Recalling the definitions of $g$ (\ref{def_g}) and $h$ (\ref{def_h}), we can write $\gamma$ as
$$ \gamma(\beta^g_1, c, \pi) = \frac{\pi g(\beta^g_1, c, \pi)}{h(\beta^g_1, c,\pi)}.$$
The special case (\ref{gamma_alternative_pi_half}) is identical to
\begin{equation}\label{gamma_alt2_pi_half}
\gamma(\beta^g_1, c, 1/2) = \frac{(4)(1/2)g(\beta^g_1, c, 1/2)}{4 h(\beta^g_1, c, 1/2)} = \frac{2 g(\beta^g_1, c, 1/2)}{4h(\beta^g_1, c, 1/2)},
\end{equation}
i.e., the numerator and denominator of  (\ref{gamma_alt2_pi_half}) coincide with those of (\ref{gamma_alternative_pi_half}). We sometimes will use the notation $2\cdot g$ and $4\cdot h$ to refer to the numerator and denominator of (\ref{gamma_alternative_pi_half}), respectively.

\subsection{Derivatives of  $g$ and $h$ in $c$}\label{sec:derivatives}
We compute the derivatives of $g$ and $h$ in $c$, which we will need to prove subsequent results. First, by FTC and the evenness of $f$, we have that
\begin{multline}\label{dg_dc}
D_2 g(\beta^g_1, c, \pi) = -(1-\pi)f( \beta^g_0 + \beta^g_1 - c ) + (1-\pi) f(\beta^g_0 - c) \\ = (1-\pi) f(c - \beta^g_0) - (1-\pi)f(c - \beta^g_0 - \beta^g_1).
\end{multline}
Second, we have that
\begin{multline}\label{dh_dc}
D_2 h(\beta^g_1, c, \pi) \\ = -[(1-\pi)f(\beta^g_0 - c) + \pi f( \beta^g_0 + \beta^g_1 - c )]\left[(1-\pi)\Phi(c - \beta^g_0) + \pi \Phi(c - \beta_0^g - \beta_1^g)  \right] \\ + [(1-\pi) f(c - \beta^g_0) +  \pi f(c - \beta^g_0 - \beta^g_1)] \left[(1-\pi) \Phi(\beta_0^g - c) + \pi \Phi(\beta^g_0 + \beta^g_1 - c) \right] \\ = \left[ (1-\pi) f(c - \beta^g_0) +  \pi f(c - \beta^g_0 - \beta^g_1) \right] \cdot \\ \bigg[ (1-\pi) \Phi(\beta_0^g - c) + \pi\Phi(\beta^g_0 + \beta^g_1 - c) \\ - (1-\pi) \Phi(c - \beta^g_0) - \pi \Phi(c - \beta_0^g - \beta_1^g) \bigg].
\end{multline}

\subsection{Limit of $\gamma$ in $c$}\label{sec:c_limit}

Assume (without loss of generality) that $\beta^g_1 > 0$. We compute $\lim_{c \to \infty} \gamma(\beta^g_1, c, \pi)$. Observe that $$\lim_{c \to \infty} g(\beta^g_1, c, \pi) = \lim_{c \to \infty} h(\beta^g_1, c, \pi)  = 0.$$ Therefore, we can apply L'H\^{o}pital's rule. We have by (\ref{dg_dc}) and (\ref{dh_dc}) that \begin{multline}\label{c_limit_product}
\lim_{c \to \infty} \gamma(\beta^g_1, c, \pi) = \lim_{c \to \infty} \frac{\pi D_2 g(\beta^g_1, c, \pi)}{D_2h(\beta^g_1, c, \pi)} \\ = \lim_{c \to \infty} \bigg\{ \frac{(1-\pi) f(c - \beta^g_0) + \pi f(c - \beta^g_0 - \beta^g_1)}{\pi (1-\pi) f(c - \beta^g_0) - \pi (1-\pi)f(c - \beta^g_0 - \beta^g_1)} \\ \cdot \bigg[ (1-\pi) \Phi(\beta_0^g - c) + \pi \Phi(\beta^g_0 + \beta^g_1 - c) \\ - (1-\pi) \Phi(c - \beta^g_0) - \pi \Phi(c - \beta_0^g - \beta_1^g) \bigg] \bigg\}^{-1}.
 \end{multline}
 We evaluate the two terms in the product (\ref{c_limit_product}) separately. Dividing by $f(c - \beta^g_0 - \beta^g_1) > 0$, we see that
 \begin{equation}\label{c_limit_product_2}
 \frac{(1-\pi) f(c - \beta^g_0) + \pi f(c - \beta^g_0 - \beta^g_1)}{\pi (1-\pi) f(c - \beta^g_0) - \pi (1-\pi)f(c - \beta^g_0 - \beta^g_1)} = \frac{\frac{(1-\pi) f(c - \beta^g_0)}{ f(c - \beta^g_0 - \beta^g_1)} + \pi}{\frac{ \pi(1-\pi) f(c - \beta^g_0)}{ f(c - \beta^g_0 - \beta^g_1)} - \pi(1-\pi)}.
 \end{equation}
 To evaluate the limit of (\ref{c_limit_product_2}), we first evaluate the limit of
 \begin{multline}\label{c_limit_product_3}
 \frac{f(c - \beta^g_0)}{f(c - \beta^g_0 - \beta^g_1)} = \frac{\exp{[-(1/2)(c - \beta_0^g)^2]}}{\exp{[-(1/2)( c - \beta^g_0 - \beta^g_1)^2]}} \\ = \frac{\exp[ -(1/2)(c^2 - 2 c \beta^g_0 + (\beta^g_0)^2)]}{\exp\left[-(1/2)( c^2 - 2c \beta^g_0 - 2 c \beta^g_1 + (\beta^g_0)^2 + 2( \beta^g_0 \beta^g_1) + (\beta^g_1)^2)\right]} \\ = \exp\big[-c^2/2 + c \beta^g_0 - (\beta^g_0)^2/2 \\ + c^2/2 - c \beta^g_0 - c \beta^g_1 + (\beta^g_0)^2/2 + \beta^g_0 \beta^g_1 + (\beta^g_1)^2/2 \big] \\ = \exp[ -c \beta^g_1 + \beta^g_0 \beta^g_1 + (\beta^g_1)^2/2] = \exp[ \beta^g_0 \beta^g_1 + (\beta^g_1)^2/2]\exp[ -c \beta^g_1]. 
\end{multline}
Taking the limit in (\ref{c_limit_product_3}), we obtain
$$
\lim_{c \to \infty} \frac{f(c - \beta^g_0)}{f(c - \beta^g_0 - \beta^g_1)} = \exp[ \beta^g_0 \beta^g_1 + (\beta^g_1)^2/2] \lim_{c \to \infty} \exp[ -c \beta^g_1] = 0
$$ for $\beta^g_1 > 0$. We now can evaluate the limit of (\ref{c_limit_product_2}):
$$ \lim_{c \to \infty} \frac{(1-\pi) f(c - \beta^g_0) + \pi f(c - \beta^g_0 - \beta^g_1)}{\pi (1-\pi) f(c - \beta^g_0) - \pi (1-\pi)f(c - \beta^g_0 - \beta^g_1)} = \frac{-\pi}{\pi(1-\pi)} = -\frac{1}{1 -\pi}.$$ Next, we compute the limit of the other term in the product (\ref{c_limit_product}):
\begin{multline}\label{c_limit_product_4}
\lim_{c \to \infty} \bigg[ (1-\pi)\Phi(\beta_0^g - c) + \pi \Phi(\beta^g_0 + \beta^g_1 - c) \\ - (1-\pi)\Phi(c - \beta^g_0) - \pi \Phi(c - \beta_0^g - \beta_1^g) \bigg] = -(1-\pi) - \pi = -1.
\end{multline}
Combining (\ref{c_limit_product_2}) and (\ref{c_limit_product_4}), the limit (\ref{c_limit_product}) evaluates to
$$ \lim_{c \to \infty} \gamma(\beta^g_1, c, \pi) = \left(  \frac{ 1 }{ 1 - \pi }\right)^{-1} = 1 - \pi.$$ It follows that the limit in $c$ of the asymptotic relative bias $b$ is
$$\lim_{c \to \infty} b(\beta^g_1, c, \pi) = 1 - \lim_{c \to \infty} \gamma(\beta^g_1, c, \pi) = \pi.$$
A corollary is that
$$\lim_{c \to \infty} b(\beta^g_1, c, 1/2) = 1/2.$$

\subsection{Bayes-optimal decision boundary as a critical value of $\gamma$}\label{sec:bayes_opt}
Let $c_\textrm{bayes} = \beta^g_0 + (1/2)\beta^g_1.$ We show that $c = c_\textrm{bayes}$ is a critical value of $\gamma$ for $\pi = 1/2$ and given $\beta^g_1$, i.e, $$D_2 \gamma (\beta^g_1, c_\textrm{bayes}, 1/2) = 0.$$ Differentiating (\ref{gamma_alt2_pi_half}), the quotient rule implies that
\begin{equation}\label{quotient_rule}
D_2\gamma(\beta^g_1, c, 1/2) \\ = \frac{D_2[2g(\beta^g_1, c, 1/2)] 4h(\beta^g_1, c, 1/2) - 2g(\beta^g_1, c, 1/2) D_2[4h(\beta^g_1, c, 1/2)]}{[4h(\beta^g_1, c, \pi)]^2}.
\end{equation}
We have by (\ref{dg_dc}) that
\begin{equation}\label{dg_dc_bayes}
D_2[2g(\beta^g_1, c_\textrm{bayes}, 1/2)] = f( \beta^g_1/2) - f( -\beta^g_1/2) = f(\beta^g_1/2) - f(\beta^g_1/2) = 0.
\end{equation}
Similarly, we have by (\ref{dh_dc}) that
\begin{multline}\label{dh_dc_bayes}
D_2[4 h(\beta^g_1, c_\textrm{bayes}, \pi)] = [f( \beta^g_1/2) + f( -\beta^g_1/2)] \cdot \\ \left[  \Phi(-\beta^g_1/2) + \Phi(\beta^g_1/2) -  \Phi(\beta^g_1/2) - \Phi(-\beta^g_1/2) \right] = 0.
\end{multline}
Plugging in (\ref{dh_dc_bayes}) and (\ref{dg_dc_bayes}) to (\ref{quotient_rule}), we find that 
$$D_2[\gamma(\beta^g_1, c_\textrm{bayes}, 1/2)] = 0.$$ Finally, because
$$b(\beta^g_1, c, 1/2) = 1 - \gamma(\beta^g_1, c, 1/2),$$ it follows that
$$D_2[b(\beta^g_1, c_\textrm{bayes}, 1/2)] = -D_2[\gamma(\beta^g_1, c_\textrm{bayes}, 1/2)] = 0.$$

\subsection{Comparing Bayes-optimal decision boundary and large threshold}\label{sec:comparison}

We compare the bias produced by setting the threshold to a large number to the bias produced by setting the threshold to the Bayes-optimal decision boundary. Let $r: \R^{\geq 0} \to \R$ be the value of attenuation function evaluated at the Bayes-optimal decision boundary $c_\textrm{bayes} = \beta^g_0 + (1/2) \beta^g_1$, i.e.
\begin{multline*}
r(\beta^g_1) = \gamma(\beta^g_1, \beta^g_0 + (1/2)\beta^g_1, 1/2) = \frac{\Phi(\beta^g_1/2) - \Phi(-\beta^g_1/2)}{\left[\Phi(-\beta^g_1/2) + \Phi( \beta^g_1/2) \right] \left[\Phi(\beta^g_1/2) + \Phi( -\beta^g_1/2)\right]} \\ = \frac{\int_{-\beta^g_1/2}^{\beta^g_1/2} f}{\left[ 1 - \Phi(\beta^g_1/2) + \Phi(\beta^g_1/2) \right]\left[ \Phi(\beta^g_1/2) + 1 - \Phi(\beta^g_1/2) \right]} = 2 \int_{0}^{\beta^g_1/2} f = 2 \Phi(\beta^g_1/2) - 1.
\end{multline*}
We set $r$ to $1/2$ and solve for $\beta^g_1$:
\begin{multline*}
r(\beta^g_1) = 1/2 \iff 2\Phi(\beta^g_1/2) -1 = 1/2 \\ \iff \Phi(\beta^g_1/2) = 3/4 \iff \beta^g_1 = 2 \Phi^{-1}(3/4) \approx 1.35.
\end{multline*}
Because $r$ is a strictly increasing function, it follows that $r(\beta^g_1) < 1/2$ for $\beta^g_1 < 2\Phi^{-1}(3/4)$ and $r(\beta^g_1) > 1/2$ for $\beta^g_1 > 2\Phi^{-1}(3/4).$ Next, because $$b(\beta^g_1, c_\textrm{bayes}, 1/2) = 1 - \gamma(\beta^g_1, c_\textrm{bayes}, 1/2) = 1 - r(\beta^g_1),$$ we have that $b(\beta^g_1, c_\textrm{bayes}, 1/2) > 1/2$ for $\beta^g_1 < 2 \Phi^{-1}(3/4)$ and $b(\beta^g_1, c_\textrm{bayes}, 1/2) < 1/2$ for $\beta^g_1 > 2 \Phi^{-1}(3/4)$. Recall that the bias induced by sending the threshold to infinity (as stated in Proposition \ref{prop:c_limit_half} and proven in Section \ref{sec:c_limit}) is $1/2$, i.e. $$b(\beta^g_1, \infty, 1/2) = 1/2.$$ We conclude that $b(\beta^g_1, c_\textrm{bayes},1/2) > b(\beta^g_1, \infty, 1/2)$ on $\beta^g_1 \in [0, 2\Phi^{-1}(3/4))$; $b(\beta^g_1, c_\textrm{bayes},1/2) = b(\beta^g_1, \infty, 1/2)$ for $\beta^g_1 = 2\Phi^{-1}(3/4)$; and $b(\beta^g_1, c_\textrm{bayes},1/2) < b(\beta^g_1, \infty, 1/2)$ on $\beta^g_1 \in (2\Phi^{-1}(3/4), \infty)$.

\subsection{Monotonicity in $\beta^g_1$}\label{sec:monotone}
We show that $\gamma$ is monotonically increasing in $\beta^g_1$ for $\pi = 1/2$ and given threshold $c$. We begin by stating and proving two lemmas. The first lemma establishes an inequality that will serve as the basis for the proof.

\begin{lemma}
The following inequality holds: 
\begin{multline}\label{basic_ineq_cp}
\left[\Phi(\beta^g_0 - c) + \Phi(\beta^g_0 + \beta^g_1 - c) \right] \\ \cdot \left[\Phi(\beta_0^g + \beta_1^g - c) - \Phi(\beta_0^g - c) + \Phi(c - \beta^g_0) + \Phi(c - \beta_0^g - \beta_1^g) \right] \\ \geq \left[\Phi(\beta_0^g + \beta_1^g - c) - \Phi(\beta_0^g - c)\right]\left[\Phi(c - \beta^g_0) + \Phi(c - \beta_0^g - \beta_1^g)\right].
\end{multline}
\end{lemma}
\textbf{Proof}: We take cases on the sign on $\beta^g_1$.

\noindent
\underline{Case 1}: $\beta^1_g < 0$. Then $ \beta^g_1 + (\beta^g - c) < (\beta^g_0 - c),$ implying $\Phi(\beta^g_0 + \beta^g_1 - c) < \Phi(\beta^g_0 - c),$ or $[\Phi(\beta^g_0 + \beta^g_1 - c) - \Phi(\beta^g_0 - c)] < 0.$ Moreover, $[\Phi(c - \beta^g_0) + \Phi(c - \beta_0^g - \beta_1^g)]$ is positive. Therefore, the right-hand side of (\ref{basic_ineq_cp}) is negative.

Turning our attention of the left-hand side of (\ref{basic_ineq_cp}), we see that
\begin{equation}\label{basic_ineq_cp_2}
\Phi(\beta^g_0 + \beta^g_1 - c) + \Phi( c - \beta^g_0 - \beta^g_1) = 1 -\Phi(\beta^g_0 + \beta^g_1 - c) + \Phi( c - \beta^g_0 - \beta^g_1) = 1.
\end{equation}
Additionally, $\Phi(\beta^g_0 - c) < 1$ and $ \Phi(c - \beta^g_0) > 0$. Combining these facts with (\ref{basic_ineq_cp_2}), we find that
$$ \left[\Phi(\beta_0^g + \beta_1^g - c) - \Phi(\beta_0^g - c) + \Phi(c - \beta^g_0) + \Phi(c - \beta_0^g - \beta_1^g) \right] > 0. $$ Finally, because $\left[\Phi(\beta^g_0 - c) + \Phi(\beta^g_0 + \beta^g_1 - c) \right] > 0,$ the entire left-hand side of (\ref{basic_ineq_cp}) is positive. The inequality holds for $\beta^g_1 < 0$.

\noindent
\underline{Case 2}: $\beta^1_g \geq 0.$  We will show that the first term on the LHS of (\ref{basic_ineq_cp}) is greater than the first term on the RHS of (\ref{basic_ineq_cp}), and likewise that the second term on the LHS is greater than the second term on the RHS, implying the truth of the inequality. Focusing on the first term, the positivity of $\Phi(\beta^g_0 -c)$ implies that
$$ \Phi(\beta^g_0 - c) \geq - \Phi(\beta^g_0 - c),$$ and so
$$ \Phi(\beta^g_0 - c) + \Phi(\beta^g_0 + \beta^g_1 - c) \geq \Phi(\beta^g_0 - \beta^g_1 - c) - \Phi(\beta^g_0 - c).$$
Next, focusing on the second term, $\beta^g_1 \geq 0$ implies that 
\begin{equation}\label{basic_ineq_cp_3}
\beta^g_1 + \beta^g_0 - c \geq \beta^g_0 - c \implies \Phi(\beta^g_1 + \beta^g_0 - c) - \Phi(\beta^g_0 - c) \geq 0.
\end{equation}
Adding $\Phi(c - \beta^g_0) + \Phi(c - \beta^g_0 - \beta^g_1)$ to both sides of (\ref{basic_ineq_cp_3}) yields
\begin{multline*}
\Phi(\beta^g_1 + \beta^g_0 - c) - \Phi(\beta^g_0 - c) + \Phi(c - \beta^g_0) + \Phi(c - \beta^g_0 - \beta^g_1) \\ \geq \Phi(c - \beta^g_0) + \Phi(c - \beta^g_0 - \beta^g_1). \textrm{ }
\end{multline*}
The inequality holds for $\beta^g_1 \geq 0$. Combining the cases, the inequality holds for all $\beta^g_1 \in \R$. $\square$

The second lemma establishes the derivatives of the functions $2\cdot g$ and $4 \cdot h$ in $\beta^g_1$.
\begin{lemma}
The derivatives in $\beta^g_1$ of $2\cdot g$ and $4\cdot h$ are
\begin{equation}\label{dg_dbeta}
\textcolor{violet}{D_1[2g(\beta^g_1, c, 1/2)] = f(\beta^g_0 + \beta^g_1 - c)}
\end{equation}
and
\begin{multline}\label{dh_dbeta}
\textcolor{teal}{D_1[4h(\beta^g_1, c, 1/2)] = f(\beta^g_0 + \beta^g_1 - c) \left[\Phi(c - \beta^g_0) + \Phi(c - \beta_0^g - \beta_1^g) \right]} \\ \textcolor{teal}{- f(\beta^g_0 + \beta^g_1 - c) \left[\Phi(\beta_0^g - c) + \Phi(\beta^g_0 + \beta^g_1 - c) \right]}.\end{multline}
\end{lemma}
\textbf{Proof}: Apply FTC and product rule. $\square$

We are ready to prove the monotonicity of $\gamma$ in $\beta^g_1$. Subtracting $$\left[\Phi(\beta_0^g - c) + \Phi(\beta^g_0 + \beta^g_1 - c) \right]\left[\Phi(\beta_0^g + \beta_1^g - c) - \Phi(\beta_0^g - c)\right]$$ from both sides of (\ref{basic_ineq_cp}) and multiplying by $f(\beta^g_0 + \beta^g_1 - c) > 0$ yields
\begin{multline}\label{basic_ineq_cp_4}
\textcolor{violet}{f(\beta^g_0 + \beta^g_1 - c)} \textcolor{red}{ \left[\Phi(\beta^g_0 - c) + \Phi\left(\beta^g_0 + \beta^g_1 - c \right) \right] \left[ \Phi(c - \beta^g_0) + \Phi(c - \beta^g_0 - \beta^g_1) \right]}  \\ \geq \textcolor{teal}{f(\beta^g_0 + \beta^g_1 - c) \left[\Phi(c - \beta^g_0) + \Phi(c - \beta_0^g - \beta_1^g)\right]}\textcolor{blue}{\left[\Phi(\beta_0^g + \beta_1^g - c) - \Phi(\beta_0^g - c)\right]} \\ -\textcolor{teal}{f(\beta^g_0 + \beta^g_1 - c)   \left[\Phi(\beta_0^g - c) + \Phi(\beta^g_0 + \beta^g_1 - c) \right]} \textcolor{blue}{\left[\Phi(\beta_0^g + \beta_1^g - c) - \Phi(\beta_0^g - c)\right]}.
\end{multline}
Next, recall that
\begin{equation}\label{def_2g}
\textcolor{blue}{2g(\beta^g_1,c,1/2) = \Phi(\beta^g_0 + \beta^g_1 - c) - \Phi(\beta^g_0 - c)}.
\end{equation}
and
\begin{equation}\label{def_4h}
\textcolor{red}{4h(\beta^g_1, c, 1/2) = \left[ \Phi(\beta^g_0 - c) + \Phi(\beta^g_0 + \beta^g_1 - c) \right] \left[\Phi(c - \beta^g_0) + \Phi( c - \beta^g_0 - \beta^g_1) \right]}.
\end{equation}
Substituting (\ref{dg_dbeta}, \ref{dh_dbeta}, \ref{def_2g}, \ref{def_4h}) into (\ref{basic_ineq_cp_4}) produces
\begin{equation*}
\textcolor{violet}{D_1[2g(\beta^g_1, c, 1/2)]}\textcolor{red}{4h(\beta^g_1, c, 1/2)} \geq \textcolor{blue}{2g(\beta^g_1, c, 1/2)}\textcolor{teal}{D_1[4h(\beta^g_1, c, 1/2)]},
\end{equation*}
or 
\begin{equation}\label{basic_ineq_cp_5}
\textcolor{violet}{D_1[2g(\beta^g_1, c, 1/2)]}\textcolor{red}{4h(\beta^g_1, c, 1/2)} - \textcolor{blue}{2g(\beta^g_1, c, 1/2)}\textcolor{teal}{D_1[4h(\beta^g_1, c, 1/2)]} \geq 0.
\end{equation}
The quotient rule implies that
\begin{multline}\label{d_gamma_d_beta}
D_1 \gamma(\beta^g_1, c, 1/2) \\ = \frac{ \textcolor{violet}{D_1[2g(\beta^g_1, c, 1/2)]}\textcolor{red}{4h(\beta^g_1, c, 1/2)} - \textcolor{blue}{2g(\beta^g_1, c, 1/2)}\textcolor{teal}{D_1[4h(\beta^g_1, c, 1/2)]} }{[4h(\beta^g_1, c, 1/2)]^2}.
\end{multline}
We conclude by (\ref{basic_ineq_cp_5}) and (\ref{d_gamma_d_beta}) that $\gamma$ is monotonically increasing in $\beta^g_1$. Finally, $b(\beta^g_1, c, \pi) = 1 - \gamma(\beta^g_1, c, \pi)$ is monotonically decreasing in $\beta^g_1$.

\subsection{Strict attenuation bias}\label{sec:att_bias}

We begin by computing the limit of $\gamma$ in $\beta^g_1$ given $\pi = 1/2$.  First,
\begin{multline*}
\lim_{\beta^g_1 \to \infty} \gamma(\beta^g_1, c, 1/2) = \frac{1 - \Phi(\beta^g_0 - c)}{\left[1 + \Phi(\beta^g_0 - c) \right] \left[\Phi(c - \beta^g_0) \right]} \\ = \frac{\Phi(c - \beta^g_0)}{ \left[1 + \Phi(\beta^g_0 - c) \right] \left[\Phi(c - \beta^g_0) \right]} = \frac{1}{1 + \Phi(\beta^g_0 - c)} < 1.
\end{multline*}
Similarly,
\begin{multline*}
\lim_{\beta^g_1 \to -\infty} \gamma(\beta^g_1, c, 1/2) = \frac{ - \Phi(\beta^g_0 - c)}{\left[\Phi(\beta^g_0 - c)\right] \left[\Phi(c - \beta^g_0) + 1 \right]} = \frac{-1}{1 + \Phi(c - \beta^g_0)} > -1.
\end{multline*}
The function $\gamma(\beta^g_1, c, 1/2, \beta^g_0)$ is monotonically increasing in $\beta^g_1$ (as stated in Proposition \ref{prop:monotonic} and proven in section \ref{sec:monotone}). It follows that 
$$-1 < -\frac{1}{1 + \Phi(c - \beta^g_0)} \leq \gamma(\beta^g_1, c, 1/2, \beta^g_0) \leq \frac{1}{1 - \Phi(\beta^g_0 - c)} < 1$$ for all $\beta^g_1 \in \R$. But $\beta^g_0$ and $c$ were chosen arbitrarily, and so
$$-1 < \gamma(\beta^g_1, c, 1/2, \beta^g_0) < 1$$ for all $(\beta^g_1, c, \beta^g_0) \in \R^3$. Finally, because $b(\beta^g_1, c, 1/2, \beta^g_0) = 1 - \gamma(\beta^g_1, c, 1/2, \beta^g_0)$, it follows that
$$ 0 < b(\beta^g_1, c, 1/2, \beta^g_0) < 2$$ for all $(\beta^g_1, c, \beta^g_0) \in \R^3$

\subsection{Bias-variance decomposition in no-intercept model}\label{sec:bv_decomp}
 
We prove the bias-variance decomposition for the no-intercept model (\ref{theoretical_model_no_int}). Define $l$ (for ``limit'') by
$$l = \beta_m \left(\frac{\omega \pi}{\zeta(1-\pi) + \omega \pi}\right),$$ where
$$
\begin{cases}
\omega = \bar{\Phi}(c - \beta_g) = \Phi(\beta_g - c) \\
\zeta = \bar{\Phi}(c) = \Phi(-c).
\end{cases}
$$
We have that
\begin{equation*}
\hat{\beta}_m - l = \frac{\sum_{i=1}^n \hat{p}_i m_i}{ \sum_{i=1}^n \hat{p}^2_i} - l = \frac{\sum_{i=1}^n \hat{p}_i m_i}{ \sum_{i=1}^n \hat{p}^2_i} - \frac{l \sum_{i=1}^n \hat{p}_i^2 }{ \sum_{i=1}^n \hat{p}_i^2} \\ = \frac{\sum_{i=1}^n \hat{p}_i(m_i - l \hat{p}_i)}{ \sum_{i=1}^n \hat{p}_i^2}.
\end{equation*}
Therefore,
\begin{equation}\label{bc_decomp_1}
\sqrt{n}(\hat{\beta}_m - l) = \frac{(1/\sqrt{n})\sum_{i=1}^n \hat{p}_i(m_i - l \hat{p}_i)}{(1/n)\sum_{i=1}^n \hat{p}_i^2}.
\end{equation}
Next, we compute the expectation and variance of $\hat{p}_i(m_i - l\hat{p}_i)$. To do so, we first compute several simpler quantities:
\begin{enumerate}
\item Expectation of $\hat{p}_i$: 
\begin{multline*}
\E[\hat{p}_i] = \P(p_i\beta_g + \tau_i \geq c) =  \P(\beta_g + \tau_i \geq c)\pi + \P(\tau_i \geq c)(1-\pi) \\ = \pi \omega + (1-\pi)\zeta.
\end{multline*}
\item Expectation of $\hat{p}_i p_i$: $$\E\left[\hat{p}_i p_i\right] = \E\left[\hat{p}_i | p_i = 1 \right]\P\left[p_i = 1\right] = \omega \pi.$$
\item Expectation of $\hat{p}_i m_i$:
\begin{multline*}
\E[\hat{p}_i m_i] = \E\left[\hat{p}_i(\beta_m p_i + \ep_i)\right] = \E\left[\beta_m \hat{p}_i p_i + \hat{p}_i \ep_i \right] \\ = \beta_m \E\left[ \hat{p}_i p_i \right] + \E[\hat{p}_i]\E[\ep_i] = \beta_m \omega \pi + 0 = \beta_m \omega \pi.
\end{multline*}
\item Expectation of $\hat{p}_i m_i^2$: \begin{multline*}
\E\left[\hat{p}_i m_i^2\right] = \E \left[ \hat{p}_i( \beta_m p_i + \ep_i )^2 \right] = \E\left[ \hat{p}_i \left( \beta_m^2 p_i^2 + 2 \beta_m p_i \ep_i + \ep_i^2 \right)  \right] \\ = \E\left[ \hat{p}_i p_i \beta^2_m + 2 \beta_m p_i \hat{p}_i \ep_i + \hat{p}_i \ep_i^2 \right] = \beta^2_m \E[ \hat{p}_i p_i] + 2 \beta_m \E[p_i\hat{p}_i] \E[\ep_i] + \E[\hat{p}_i] \E[ \ep^2_i ] \\ = \beta^2_m \E[ \hat{p}_i p_i] + \E[\hat{p}_i] = \beta^2_m \omega \pi + \E[ \hat{p}_i]. 
\end{multline*}
\end{enumerate}

Now, we can compute the expectation and variance of $\hat{p}_i(m_i - l\hat{p}_i)$. First,
\begin{multline}\label{bv_decomp_2}
\E\left[\hat{p}_i(m_i - l\hat{p}_i) \right] = \E[\hat{p}_i m_i] - l \E[\hat{p}_i] \\ = \beta_m \omega \pi - \left(\frac{\beta_m \omega \pi}{\zeta (1-\pi) + \omega \pi}\right)[\zeta (1-\pi) + \omega \pi] = 0.
\end{multline}
Additionally,
\begin{multline}\label{bv_decomp_3}
\V\left[\hat{p}_i(m_i - l\hat{p}_i)\right] = \E\left[\hat{p}_i^2(m_i - l\hat{p}_i)^2\right] - (\E\left[ \hat{p}_i(m_i - l\hat{p}_i)\right])^2 \\ = \E\left[ \hat{p}_i m_i^2\right] - 2l \E[m_i\hat{p}_i] +l^2 \E[\hat{p}_i]= \beta^2_m \omega \pi + \E[ \hat{p}_i] -2l \beta_m \omega \pi + l^2 \E[\hat{p}_i] \\ = \beta_m\omega\pi(\beta_m - 2l) + \E[\hat{p}_i](1 + l^2).
\end{multline}
Therefore, by CLT, (\ref{bv_decomp_2}), and (\ref{bv_decomp_3}),
\begin{equation}\label{bv_decomp_4}
(1/\sqrt{n})\sum_{i=1}^n \hat{p}_i(m_i - l \hat{p}_i) \xrightarrow{d} N\left(0, \beta_m\omega\pi(\beta_m - 2l) + \E[\hat{p}_i](1 + l^2) \right).
\end{equation}
Next, by weak LLN,
\begin{equation}\label{bv_decomp_5}
(1/n) \sum_{i=1}^n \hat{p}_i^2 = (1/n) \sum_{i=1}^n \hat{p}_i \xrightarrow{P} \E[\hat{p}_i].
\end{equation}
Finally, by (\ref{bc_decomp_1}), (\ref{bv_decomp_4}), (\ref{bv_decomp_5}), and Slutsky's Theorem,
$$ \sqrt{n}(\hat{\beta}_m - l) \xrightarrow{d} N\left(0, \frac{ \beta_m\omega\pi(\beta_m - 2l) + \E[\hat{p}_i](1 + l^2) }{\left(\E[\hat{p}_i]\right)^2} \right).$$ Thus, for large $n \in \N$, we have that 
$$ \begin{cases}
\E [\hat{\beta}_m] \approx l, \\
\V[\hat{\beta}_m] \approx \left[\beta_m\omega\pi(\beta_m - 2l) + \E[\hat{p}_i](1 + l^2)\right]/[n\E^2[\hat{p}_i]],
\end{cases}$$
completing the bias-variance decomposition.

\section{Estimation and inference in the GLM-EIV model}
\subsection{Estimation}
We estimate the parameters of the GLM-EIV model using an EM algorithm.

\subsubsection*{E step}
The E step entails computing the membership probability of each cell. Let $\theta^{(t)} = (\beta_m^{(t)}, \beta_g^{(t)}, \pi^{(t)})$ be the parameter estimate at the $t$-th iteration of the algorithm. For $k \in \{0,1\}$, let $[\eta^m_i(k)]^{(t)}$ be the $i$th canonical parameter at the $t$-th iteration of the algorithm of the gene expression distribution that results from setting $p_i$ to $k$, i.e.
$$
[\eta^m_i(k)]^{(t)} := h_m\left( \langle \tilde{x}_i(k) , \beta_m^{(t)} \rangle + o^m_i \right).
$$ Similarly, let $\left[\eta^g_i(k)\right]^{(t)}$ be defined by
$$\left[\eta^g_i(k)\right]^{(t)} :=  h_m\left( \langle \tilde{x}_i(k) , \beta_g^{(t)} \rangle + o^g_i \right).$$
Next, for $k \in \{0,1\},$ define $\alpha^{(t)}_i(k)$ by
\begin{multline*}
\alpha^{(t)}_i(k) := \P\left( M_i = m_i, G_i = g_i | P_i = k, \theta^{(t)} \right) \\ = \P\left( M_i = m_i | P_i = k, \theta^{(t)} \right) \P\left(G_i = g_i | P_i = k, \theta^{(t)} \right) \textrm{ (because $G_i \indep M_i | P_i$)} \\ = f_m\left(m_i; \left[ \eta^m_i(k) \right]^{(t)}\right) f_g\left(g_i; \left[ \eta^g_i(k) \right]^{(t)} \right).
\end{multline*}
Finally, let 
$$
\begin{cases}
\pi^{(t)}(1) := \pi^{(t)} = \P\left(P_i = 1 | \theta^{(t)} \right) \\
\pi^{(t)}(0) := 1 - \pi^{(t)} = \P\left(P_i = 0 | \theta^{(t)} \right).
\end{cases}
$$
The $i$th membership probability $T^{(t)}_i(1)$ is
\begin{multline*}
T^{(t)}_i(1) = \P(P_i = 1 | M_i = m_i, G_i = g_i, \theta^{(t)}) \\ = \frac{\pi^{(t)}(1) \alpha^{(t)}_i(1)}{ \sum_{k=0}^1 \pi^{(t)}(k) \alpha^{(t)}_i(k)} \textrm{ (by Bayes rule)} = \frac{1}{\frac{ \pi^{(t)}(0) \alpha_i(0)}{\pi^{(t)}(1) \alpha_i(1)} + 1} \\ = \frac{1}{ \exp\left(\log\left(\frac{\pi^{(t)}(0) \alpha_i(0)}{\pi^{(t)}(1) \alpha_i(1)}\right)\right) + 1} = \frac{ 1 }{ \exp\left(q^{(t)}_i\right) + 1},
\end{multline*}
where we set $$q_i^{(t)} := \log\left(\frac{\pi^{(t)}(0) \alpha_i(0)}{\pi^{(t)}(1) \alpha_i(1)}\right).$$
Next, we have that
\begin{multline*}
q^{(t)}_i = \log\left[ \pi^{(t)}(0) \right] + \log\left[ f_m\left(m_i; \left[ \eta^m_i(0) \right]^{(t)}\right) \right] + \log\left[ f_g\left(g_i; \left[ \eta^g_i(0) \right]^{(t)}\right) \right] \\ - \log\left[ \pi^{(t)}(1) \right] - \log\left[ f_m\left(m_i; \left[ \eta^m_i(1) \right]^{(t)}\right) \right] - \log\left[ f_g\left(g_i; \left[ \eta^g_i(1) \right]^{(t)}\right) \right],
\end{multline*}
which we can compute. We therefore conclude that
$$ T_i^{(t)} = \frac{1}{\exp\left(q^{(t)}_i\right) + 1}.$$
\subsection*{M step}
Recall that the log-likelihood (\ref{full_log_lik}) of the GLM-EIV model  is
\begin{multline*}
\mathcal{L}(\theta; m, g, p) = \sum_{i=1}^n \left[ p_i \log(\pi) + (1-p_i) \log(1-\pi) \right] \\ + \sum_{i=1}^n \log\left( f_m(m_i; \eta^m_i)\right) + \sum_{i=1}^n \log\left( f_g(g_i; \eta_i^g) \right).
\end{multline*}
Define $Q(\theta | \theta^{(t)}) = \E_{\left(P |M = m, G = g, \theta^{(t)}\right)}\left[ \mathcal{L}(\theta; m, g, p) \right].$ We have that
\begin{multline}\label{Q_funct}
Q(\theta |\theta^{(t)}) = \sum_{i=1}^n \left[T^{(t)}_i(1)\log(\pi) + T_i^{(t)}(0) \log(1 - \pi)\right] \\ + \sum_{k=0}^1 \sum_{i=1}^n T^{(t)}_i(k) \log \left[ f_m(m_i; \eta_i^m(k)) \right] + \sum_{k=0}^1 \sum_{i=1}^n T^{(t)}_i(k) \log \left[ f_g( g_i; \eta^{g,b}_i(k)) \right].
\end{multline}
The three terms of (\ref{Q_funct}) are functions of different parameters: the first is a function of $\pi,$ the second is a function of $\beta_m,$ and the third is a function of $\beta_g$. Therefore, to find the maximizer $\theta^{(t+1)}$ of (\ref{Q_funct}), we maximize the three terms separately. Differentiating the first term with respect to $\pi$, we find that
	\begin{multline*}
\frac{ \partial }{\partial \pi } \sum_{i=1}^n \left[ T^{(t)}_i(1)\log(\pi) + T_i^{(t)}(0) \log(1 - \pi)\right]  \\ = \frac{\sum_{i=1}^n T_i^{(t)}(1)}{\pi} - \frac{ \sum_{i=1}^n T_i^{(t)}(0) }{ 1 - \pi}.
\end{multline*} Setting the derivative equal to $0$ and solving for $\pi$,
\begin{multline*}
\frac{\sum_{i=1}^n T_i^{(t)}(1)}{\pi} - \frac{ \sum_{i=1}^n T_i^{(t)}(0) }{ 1 - \pi} = 0 \iff \sum_{i=1}^n T_i^{(t)}(1) - \pi \sum_{i=1}^n T^{(t)}_i(1) = \pi \sum_{i=1}^n T_i(0) \\ \iff \sum_{i=1}^n T^{(t)}_i(1) - \pi\sum_{i=1}^n T_i^{(t)}(1) = \pi n - \pi\sum_{i=1}^n T_i(1) \iff \pi = \frac{ \sum_{i=1}^n T_i^{(t)} (1) }{n}.\end{multline*}
Thus, the maximizer $\pi^{(t+1)}$ of (\ref{Q_funct}) in $\pi$ is $\pi^{(t+1)} = (1/n)\sum_{i=1}^n T^{(t)}_i(1)$. Next, define $w^{(t)} = [T^{(t)}_1(0), \dots, T^{(t)}_n(0), T^{(t)}_1(1), \dots, T^{(t)}_n(1)]^T \in \R^{2n}$. We can view the second term of (\ref{Q_funct}) as the log-likelihood of a GLM -- call it $\textrm{GLM}^{(t)}_m$ -- that has exponential family density $f_m$, link function $r_m$, responses $[m,m]^T$, offsets $[o^m, o^m]^T$, weights $w^{(t)}$, and design matrix $\begin{bmatrix} \tilde{X}(0) \\ \tilde{X}(1) \end{bmatrix}.$ Therefore, the maximizer $\beta^{(t+1)}_m$ of the second term of (\ref{Q_funct}) is the maximizer of $\textrm{GLM}^{(t)}_m$, which we can compute using the iteratively reweighted least squares procedure, as implemented in R's GLM function. Similarly, the maximizer $\beta^{(t+1)}_g$ of the third term of (\ref{Q_funct}) is the maximizer of the GLM with exponential family density $f_g$, link function $r_g$, responses $[g,g]^T$, offsets $[o^g, o^g]^T$, weights $w^{(t)}$, and design matrix $\begin{bmatrix} \tilde{X}(0) \\ \tilde{X}(1) \end{bmatrix}.$

\subsection{Inference}
We derive the asymptotic observed information matrix of the GLM-EIV log likelihood, enabling us to perform inference on the parameters. First, we define some notation. For $i \in \{1, \dots, n\}$, $j \in \{0, 1\}$, and $\theta = (\pi, \beta_m, \beta_g),$ let $T^\theta_i(j)$ be defined by
$$T^\theta_i(j) = \P_\theta\left(P_i = j | M_i = m_i, G_i = g_i\right).$$ Also, define the $\R^n$ vectors $T^\theta(0)$ and $T(1)$ by
$$T^\theta(0) := [T^\theta_1(0), T^\theta_2(0), \dots, T^\theta_n(0)]$$ and 
$$T^\theta(1) := [T^\theta_1(1), T^\theta_2(1), \dots, T^\theta_n(1)].$$ Moreover, let $\pi^\theta$ 

Next, define the diagonal $n \times n$ matrices $\Delta^m$, $[\Delta']^m$, $V^m$, and $H^m$ by
$$
\begin{cases}
	\Delta^m = \textrm{diag} \{ h_m'(l_1^m), \dots, h_m'( l_n^m ) \} \\
	[\Delta']^m = \textrm{diag} \{ h_m''(l_1^m), \dots, h_m''( l_n^m) \} \\
	V^m = \textrm{diag} \{ \psi_m( \eta^m_1), \dots, \psi_m( \eta^m_n) \} \\
	H^m = \textrm{diag} \{ m_1 - \mu_1^m, \dots, m_n - \mu_n^m\}.
\end{cases} 
$$ Define the $n \times n$ matrices $\Delta^g, [\Delta']^g, V^g,$ and $H^g$ analogously. These matrices are \textit{unobserved}, as they depend on $\{p_1, \dots, p_n\}$.

For $j \in \{0,1\}$, let the diagonal $n \times n$ matrices $\Delta^m(j), [\Delta']^m(j), V^m(j),$ and $H^m(j)$ be given by
$$\begin{cases}
\Delta^m(j) = \textrm{diag} \{ h_m'(l_1^m(j)), \dots, h_m'( l_n^m(j) ) \} \\
[\Delta']^m(j) = \textrm{diag} \{ h_m''(l_1^m(j)), \dots, h_m''( l_n^m(j)) \} \\
V^m(j) = \textrm{diag} \{ \psi_m( \eta^m_1(j)), \dots, \psi_m( \eta^m_n(j)) \} \\
H^m(j) = \textrm{diag} \{ m_1 - \mu_1^m(j), \dots, m_n - \mu_n^m(j)\} .
\end{cases}
$$
Define the matrices $\Delta^g(j)$, $[\Delta']^{g}(j)$, $V^g(j),$ and $H^g(j)$ analogously. Finally, define the vectors $s^m(j)$ and $w^m(j)$ in $\R^n$ by 
$$ \begin{cases}
s^m(j) = [m_1 - \mu_1^m(j), \dots, m_n - \mu_n^m(j) ]^T \\ w^m(j) = [ T_1(0)T_1(1)\Delta^m_1(j) H^m_1(j), \dots, T_n(0)T_n(1)\Delta_n^m(j) H_n^m(j)]^T,
\end{cases} $$
and let the vectors $s^g(j)$ and $w^g(j)$ be defined analogously. The quantities $\Delta^m(j), [\Delta']^m(j), V^m(j),$ $H^m(j),$ $s^m(j),$ $w^m(j),$ $\Delta^g(j), [\Delta']^g(j), V^g(j),$ $H^g(j),$ $s^g(j),$ and $w^g(j)$ are all \textit{observed}. 

The observed information matrix $J(\theta; m, g)$ evaluated at $\theta = (\pi, \beta_m, \beta_g)$ is the negative Hessian of the marginal log likelihood (\ref{marginal_log_lik}) evaluated at $\theta$, i.e.
$$J(\theta; m, g) = - \nabla^2\mathcal{L}(\theta; m, g) .$$ This quantity, unfortunately, is hard to compute, as the log likelihood (\ref{marginal_log_lik}) is a complicated mixture. Louis \cite{Louis1982} showed that $J(\theta; m, g)$ is equivalent to the following quantity:
\begin{multline}\label{zero_inf_info_mat}
J(\theta; m, g) = -\E \left[\nabla^2 \mathcal{L}(\theta; m, g, p) | G = g, M = m \right] \\ + \E\left[\nabla \mathcal{L}(\theta; m, g, p) | G = g, M = m \right] \E\left[\nabla \mathcal{L}(\theta; m, g, p) | G = g, M = m \right]^T \\ - \E\left[ \nabla\mathcal{L}(\theta; m, g, p) \nabla \mathcal{L}(\theta; m, g, p)^T | G = g, M = m \right].
\end{multline}
The observed information matrix $J(\theta; m, g)$ has dimension $(2d+1) \times (2d + 1).$ Recall that the log-likelihood (\ref{full_log_lik}) is the sum of three terms. The first term depends only on $\pi$, the second on $\beta_m$, and the third on $\beta_g$. Therefore, the observed information matrix can be viewed as block matrix consisting of nine submatrices (Figure \ref{infomatrixbackground}; only six submatrices labelled). Submatrix I depends on $\pi$, submatrix II on $\beta_m$, submatrix III on $\beta_g$, submatrix IV on $\beta_m$ and $\beta_g$, submatrix V on $\pi$ and $\beta_m$, and submatrix VI on $\pi$ and $\beta_g$. We only need to compute these six submatrices to compute the entire matrix, as the matrix is symmetric. The following sections derive formulas for submatrices I-VI. All expectations are understood to be \textit{conditional} on $m$ and $g$. The notation $\nabla_v$  and $\nabla^2_v$  represent the gradient and Hessian, respectively, with respect to the vector $v$.

\begin{figure}
	\centering
	\includegraphics[width=0.45\linewidth]{../../figures/info_matrix/info_matrix_background}
	\caption{Block structure of the observed information matrix $J(\theta; m, g) = -\nabla^2 \mathcal{L}(\theta; m, g)$. The matrix is symmetric, and so we only need to compute submatrices I-VI to compute the entire matrix.}
	\label{infomatrixbackground}
\end{figure}

\subsubsection*{Submatrix I}
Denote submatrix I by $J_{\pi}(\theta; m, g).$ The formula for $J_{\pi}(\theta; m, g)$ is 
\begin{multline}\label{sub_mat_pi}
J_{\pi}(\theta; m, g) = -\E\left[\nabla^2_\pi \mathcal{L}(\theta; m, g, p) \right] \\ + \left(\E\left[ \nabla_\pi \mathcal{L}(\theta; m, g, p) \right] \right)^2 - \E\left[(\nabla_\pi \mathcal{L}(\theta; m, g, p))^2 \right].
\end{multline}

We begin by calculating the first and second derivatives of the log-likelihood $\mathcal{L}$ with respect to $\pi$. The first derivative is
\begin{multline}\label{d_L_d_pi}
\nabla_\pi \mathcal{L}(\theta; m, g, p) = \frac{\partial }{\partial \pi } \left( \sum_{i=1}^n p_i \log(\pi) + \sum_{i=1}^n (1 - p_i) \log(1 - \pi) \right) \\ = \frac{ \sum_{i=1}^n p_i }{\pi} - \frac{ \sum_{i=1}^n (1 - p_i) }{ 1 - \pi } = \frac{\sum_{i=1}^n p_i}{\pi} - \frac{n - \sum_{i=1}^n p_i}{1 - \pi} \\ = \left( \frac{1}{\pi} + \frac{1}{1 - \pi} \right) \sum_{i=1}^n p_i - \frac{n}{1-\pi}.
\end{multline}
The second derivative is
\begin{multline*}
\nabla^2_\pi \mathcal{L}(\theta; m, g, p)  = \frac{\partial^2}{\partial^2\pi} \left( \frac{ \sum_{i=1}^n p_i }{ \pi } - \frac{ n - \sum_{i=1}^n p_i }{1 - \pi}  \right) = \frac{\left( \sum_{i=1}^n p_i \right) - n}{(1 - \pi)^2} - \frac{\sum_{i=1}^n p_i }{ \pi^2 }.
\end{multline*}
We compute the expectation of the first term of (\ref{sub_mat_pi}):
\begin{multline}\label{submat_pi_1}
\E \left[ -\nabla^2_{\pi} \mathcal{L}(\theta; m, g, p)\right] = - \E\left[\frac{ ( \sum_{i=1}^n p_i) - n}{(1 - \pi)^2} - \frac{\sum_{i=1}^n p_i}{\pi^2} \right] \\ = - \E\left\{\left[\frac{1}{(1-\pi)^2} - \frac{1}{\pi^2} \right] \sum_{i=1}^n p_i - \frac{n}{ (1 - \pi)^2 } \right\} \\ = - \left\{\left[ \frac{1}{(1-\pi)^2} - \frac{1}{\pi^2} \right] \sum_{i=1}^n T^\theta_i(1) - \frac{n}{ (1 - \pi)^2}  \right\} \\ = \left[ \frac{1}{\pi^2} - \frac{1}{(1 - \pi)^2} \right] \sum_{i=1}^n T^\theta_i(1) + \frac{n}{(1-\pi)^2}.
\end{multline}
Next, we compute the difference of the second two pieces of (\ref{sub_mat_pi}). To this end, define $$a := \frac{1}{(1-\pi)} + \frac{1}{\pi}$$ and $$b := \frac{n}{(1-\pi)}.$$ We have that
\begin{multline*}
\E \left[\nabla_\pi \mathcal{L}(\theta; m, g, p)^2 \right] \\ = \E \left[ \left( a \sum_{i=1}^n p_i - b\right)^2 \right]  =  \E \left[ a^2 \left( \sum_{i=1}^n p_i \right)^2 - 2ab \sum_{i=1}^n p_i + b^2 \right] \\ = a^2 \sum_{i=1}^n \sum_{j=1}^n \E[p_i p_j] -2ab \sum_{i=1}^n \E [p_i] + b^2.
\end{multline*}
Next,
\begin{multline*}
\left( \E \left[\nabla_\pi \mathcal{L}(\theta; m, g, x) \right] \right)^2 = \left( a \sum_{i=1}^n \E [p_i] - b \right)^2 \\ = a^2 \sum_{i=1}^n \sum_{j=1}^n \E[p_i]  E[p_j] - 2ab \sum_{i=1}^n \E[p_i] + b^2.
\end{multline*}
Therefore,
\begin{multline}\label{submat_pi_2}
(\E [\nabla_\pi \mathcal{L}(\theta; m, g, p)])^2 - \E \left[\nabla_\pi \mathcal{L}(\theta; m, g, p)^2 \right] \\ = a^2 \sum_{i=1}^n \sum_{j=1}^n \E[p_i] \E[p_j] - a^2 \sum_{i=1}^n \sum_{j=1}^n \E[p_i p_j] = a^2 \left( \sum_{i=1}^n \E[p_i]^2 - \E[p_i^2]\right) \\ = a^2 \left( \sum_{i=1}^n [T^\theta_i(1)]^2 - T^\theta_i(1) \right) = \left( \frac{1}{(1 - \pi)} + \frac{1}{\pi} \right)^2 \left(\sum_{i=1}^n [T^\theta_i(1)]^2 - T^\theta_i(1) \right).
\end{multline}
Stringing (\ref{sub_mat_pi}), (\ref{submat_pi_1}) and (\ref{submat_pi_2}) together, we obtain
\begin{multline}\label{sub_mat_1_formula}
J_\pi(\theta; m, g) = 
\left[ \frac{1}{\pi^2} - \frac{1}{(1 - \pi )^2} \right] \sum_{i=1}^n T^\theta_i(1) + \frac{n}{(1-\pi )^2} \\ + \left( \frac{1}{(1 - \pi )} + \frac{1}{\pi} \right)^2 \left( \sum_{i=1}^n [T^\theta_i(1)]^2 - T^\theta_i(1) \right).
\end{multline}

\subsubsection*{Submatrix II}
Denote submatrix II by $J_{\beta^m}(\theta; m, g).$ The formula for $J_{\beta^m}(\theta; m, g)$ is
\begin{multline}\label{sub_mat_2}
J_{\beta^m}(\theta; m, g) = -\E \left[\nabla_{\beta^m}^2 \mathcal{L}(\theta; m, g, p) \right] \\ + \E\left[\nabla_{\beta^m} \mathcal{L}(\theta; m, g, p) \right] \E\left[ \nabla_{\beta^m} \mathcal{L}(\theta; m, g, p) \right]^T \\ - \E\left[ \nabla_{\beta^m} \mathcal{L}(\theta; m, g, p) \nabla_{\beta^m} \mathcal{L}(\theta; m, g, p)^T  \right].
\end{multline}
Standard GLM results imply that
$$ -\nabla_{\beta^m}^2 \mathcal{L}(\theta; m, g, p) = \tilde{X}^T ( \Delta^m V^m \Delta^m - [\Delta']^m H^m ) \tilde{X}$$ and $$\nabla_{\beta^m}\mathcal{L}(\theta; m, g, p) = \tilde{X}^T \Delta^m s^m.$$
We compute the first term of (\ref{sub_mat_2}). The $(k,l)$th entry of this matrix is
\begin{multline*}
\left( \E\left[-\nabla_{\beta^m}^2 \mathcal{L}(\theta; m, g, p)\right]\right)[k,l] = \E \left\{\tilde{X}[,k]^T (\Delta^m V^m \Delta^m - [\Delta']^mH^m) \tilde{X}[,l] \right\} \\ = \sum_{i=1}^n \E \left\{ \tilde{x}_{i,k} (\Delta^m_{i} V^m_{i} \Delta^m_{i} - [\Delta']^m_{i} H^m_{i}) \tilde{x}_{i,l} \right\} \\ = \sum_{i=1}^n \tilde{x}_{i,k}(0) T_i^{\theta}(0) [{\Delta}^m_i(0)  {V}^m_i(0) {\Delta}^m_i(0) - [\Delta']^m_i(0) {H}^m_i(0)] \tilde{x}_{i,l}(0) \\ + \sum_{i=1}^n \tilde{x}_{i,k}(1) T_i^{\theta}(1) [ {\Delta}^m_i(1)  {V}^m_i(1) {\Delta}^m_i(1) - [{\Delta}']^m_i(1) {H}^m_i(1)] \tilde{x}_{i,l}(1) \\ = \sum_{s = 0}^1 \tilde{X}(s)[,k]^T {T}^{\theta}(s) \left[ {\Delta}^m(s) {V}^m(s) {\Delta}^m(s) - [{\Delta}']^m(s) {H}^m(s) \right] \tilde{X}(s)[,l].
\end{multline*}
We therefore have that
\begin{multline}\label{sub_mat_2_1}
\E\left[-\nabla_{\beta^m}^2 \mathcal{L}(\theta; m, g, p)\right] \\ = \sum_{s=0}^1 \tilde{X}(s)^T T^{\theta}(s) \left[ {\Delta}^m(s) {V}^m(s) {\Delta}^m(s) - [{\Delta}']^m(s) {H}^m(s) \right] \tilde{X}(s).
\end{multline}
Next, we compute the difference of the last two terms of (\ref{sub_mat_2}). The $(k,l)$th entry is
\begin{multline*}
\bigg[ \E \left[ \nabla_{\beta^m} \mathcal{L}(\theta; m, g, p) \right] \E \left[ \nabla_{\beta^m} \mathcal{L}(\theta; m, g, p) \right]^T \\ - \E \left[\nabla_{\beta^m} \mathcal{L}(\theta; m, g, p) \nabla_{\beta^m} \mathcal{L}(\theta; m, g, p)^T \right] \bigg] [k,l] \\ 
= \left[ \E \left[\tilde{X}^T \Delta^m s^m \right] \E \left[\tilde{X}^T \Delta^m s^m \right]^T\right][k,l] - \E \left[\tilde{X}^T \Delta^m s^m (s^m)^T \Delta^m \tilde{X} \right][k,l] \\
= \E\left[ \tilde{X}[,k]^T \Delta^m s^m \right] \E \left[ \tilde{X}[,l]^T \Delta^m s^m \right] - \E \left[ \tilde{X}[,k]^T \Delta^m s^m (s^m)^T \Delta^m \tilde{X}[,l ] \right] \\ 
=\E\left(\sum_{i=1}^n \tilde{x}_{ik} \Delta^m_i s^m_{i} \right) \E \left( \sum_{j=1}^n \tilde{x}_{jl} \Delta^m_j s^m_j \right) - \E \left( \sum_{i=1}^n \sum_{j=1}^n \tilde{x}_{ik} \Delta^m_i s^m_i s^m_j \Delta^m_j \tilde{x}_{jl} \right) \\
= \sum_{i=1}^n \sum_{j=1}^n \E[ \tilde{x}_{ik} \Delta^m_is^m_i] \E [\tilde{x}_{jl} \Delta^m_j s^m_j]  -  \sum_{i=1}^n \sum_{j=1}^n \E [ \tilde{x}_{ik} \Delta^m_i s^m_i s^m_j \Delta^m_j \tilde{x}_{jl}] \\
= \sum_{i=1}^n \sum_{j=1}^n \E[ \tilde{x}_{ik} \Delta^m_i s^m_i] \E \left[\tilde{x}_{jl} \Delta^m_j s^m_j \right]  - \sum_{i \neq j} \E [ \tilde{x}_{ik} \Delta^m_i s^m_i] \E[s^m_j \Delta^m_j \tilde{x}_{jl}] \\ - \sum_{i=1}^n \E[\tilde{x}_{ik} \Delta^m_i s^m_i s^m_i \Delta^m_i \tilde{x}_{il}] \\ 
= \sum_{i=1}^n \E[\tilde{x}_{ik} \Delta^m_i s^m_i ] \E [ \tilde{x}_{il} \Delta^m_i s^m_i] - \sum_{i=1}^n \E[\tilde{x}_{ik} (\Delta_i^m)^2 (H_i^m)^2 \tilde{x}_{il}] \\ = \sum_{i=1}^n \left[\tilde{x}_{ik}(0) {\Delta}^m_i(0) T_i^{\theta}(0) {H}^m_i(0) + \tilde{x}_{ik}(1) {\Delta}^m_i(1) T_i^{\theta}(1) {H}^m_i(1) \right] \\ \cdot \left[ \tilde{x}_{il}(0) {\Delta}^m_i(0) T_i^{\theta}(0) {H}^m_i(0) + \tilde{x}_{il}(1) {\Delta}^m_i(1) T_i^{\theta}(1) {H}^m_i(1) \right] \\ - \sum_{i=1}^n \left[ \tilde{x}_{ik}(0) T_i^{\theta}(0) (\Delta_i^m(0))^2 ({H}_i^m(0))^2 \tilde{x}_{il}(0)  + \tilde{x}_{ik}(1) T_i^{\theta}(1) ({\Delta}^m_i(1))^2 ({H}_i^m(1))^m \tilde{x}_{il}(1) \right] \\ = \sum_{s=0}^1 \sum_{t=0}^1 \left[ \sum_{i=1}^n \tilde{x}_{ik}(s) T^{\theta}_i(s) {\Delta}^m_i(s) {H}^m_i(t) T_i^{\theta}(t){\Delta}^m_i(t) {H}^m_i(t) \tilde{x}_{il}(t) \right] \\ - \sum_{s=0}^1 \left[\sum_{i=1}^n \tilde{x}_{ik}(s) T_i^\theta(s) ({\Delta}^m_i(s))^2 ({H}_i^m(s))^2 \tilde{x}_{il}(s) \right] \\ = \sum_{s=0}^1 \sum_{t=0}^1 \tilde{X}(s)[,k]^T {T}^\theta(s) {\Delta}^m(s) {H}^m(s) {T}^\theta(t) {\Delta}^m(t) {H}^m(t) \tilde{X}(k)[,l] \\ - \sum_{s=0}^1{X}(s)[,k]^T {T}^\theta(s) ({\Delta}^m(s))^2 ({H}^m(s))^2 \tilde{X}(s)[,l].
\end{multline*}
The sum of the last two terms on the right-hand side of (\ref{sub_mat_2}) is therefore
\begin{multline}\label{sub_mat_2_2}
 \E \left[ \nabla_{\beta^m} \mathcal{L}(\theta; m, g, p) \right] \E \left[ \nabla_{\beta^m} \mathcal{L}(\theta; m, g, p) \right]^T \\ - \E \left[\nabla_{\beta^m} \mathcal{L}(\theta; m, g, p) \nabla_{\beta^m} \mathcal{L}(\theta; m, g, p)^T \right] \\ =
\sum_{s=0}^1 \sum_{t=0}^1 \tilde{X}(s)^T {T}^\theta(s) {\Delta}^m(s) {H}^m(s) T^{\theta}(t) {\Delta}^m(t) {H}^m(t) \tilde{X}(t) \\ - \sum_{s=0}^1 \tilde{X}(s)^T {T}^\theta(s) ({\Delta}^m(s))^2 ({H}^m(s))^2 \tilde{X}(s). \end{multline}
Combining (\ref{sub_mat_2}), (\ref{sub_mat_2_1}), (\ref{sub_mat_2_2}), we find that
\begin{multline}\label{sub_mat_2_formula}
J_{\beta^m}(\theta; m, g) = \sum_{s=0}^1 \tilde{X}(s)^T T^{\theta}(s) \left[ {\Delta}^m(s) {V}^m(s) {\Delta}^m(s) - [{\Delta}']^m(s) {H}^m(s) \right] \tilde{X}(s) \\ + \sum_{s=0}^1 \sum_{t=0}^1 \tilde{X}(s)^T {T}^\theta(s) {\Delta}^m(s) {H}^m(s) {T}^\theta(t) {\Delta}^m(t) {H}^m(t) \tilde{X}(t) \\ - \sum_{s=0}^1 \tilde{X}(s)^T T^{\theta}(s) ({\Delta}^m(s))^2 ({H}^m(s))^2 \tilde{X}(s).
\end{multline}

\subsubsection*{Submatrix III}
Denote submatrix III by $J_{\beta^g}(\theta; m, g).$ The formula for sub-matrix III is similar to that of sub-matrix II (\ref{sub_mat_2_formula}). Substituting $g$ for $m$ in this equation yields
\begin{multline}\label{sub_mat_3_formula}
J_{\beta^g}(\theta; m, g) = \sum_{s=0}^1 \tilde{X}(s)^T T^{\theta}(s) \left[{\Delta}^g(s) {V}^g(s) {\Delta}^g(s) - [{\Delta}']^g(s) {H}^g(s) \right] \tilde{X}(s) \\ + \sum_{s=0}^1 \sum_{t=0}^1 \tilde{X}(s)^T {T}^\theta(s) {\Delta}^g(s) {H}^g(s) {T}^\theta(t) {\Delta}^g(t) {H}^g(t) \tilde{X}(t) \\ - \sum_{s=0}^1 \tilde{X}(s)^T T^{\theta}(s) ({\Delta}^g(s))^2 ({H}^g(s))^2 \tilde{X}(s).
\end{multline}

\subsubsection*{Submatrix IV}
Denote sub-matrix IV by $J_{(\beta^g, \beta^m)}(\theta; m, g)$. The formula for $J_{(\beta^g, \beta^m)}(\theta; m, g)$ is 
\begin{multline}\label{sub_mat_4}
J_{(\beta^g, \beta^m)}(\theta; m,g) = \E \left[-\nabla_{\beta^g} \nabla_{\beta^m} \mathcal{L}(\theta; m, g, p) \right] \\ + \E\left[ \nabla_{\beta^{g}}\mathcal{L}(\theta ; m,g,p) \right] \E \left[\nabla_{\beta^m}\mathcal{L} (\theta ; m,g,p)  \right]^T \\ - \E \left[ \nabla_{\beta^{g}}\mathcal{L} (\theta; m,g,p) \nabla_{\beta^m}\mathcal{L}(\theta; m,g,p)^T  \right].
\end{multline}
First, we have that
\begin{equation}\label{sub_mat_4_1}
\E\left[-\nabla_{\beta^g} \nabla_{\beta^m} \mathcal{L}(\theta; m, g, p) \right] = 0,
\end{equation}
as differentiating $\mathcal{L}$ with with respect to $\beta^g$ yields a vector that is a function of $\beta^g$, and differentiating this vector with respect to $\beta^m$ yields $0$. Next, recall from GLM theory that $$\nabla_{\beta^g} \mathcal{L}(\theta; m, g, p) =   \tilde{X}^T\Delta^g s^g$$ and $$\nabla_{\beta^m} \mathcal{L}(\theta; m, g, p) = \tilde{X}^T \Delta^m s^m.$$ The $(k,l)$th entry of the last two terms of (\ref{sub_mat_4}) is
\begin{multline}\label{sub_mat_4_2}
\bigg[ \E \left[\nabla_{\beta^g} \mathcal{L}(\theta; m, g, p) \right] \E \left[\nabla_{\beta^m} \mathcal{L}(\theta; m, g, p) \right]^T \\ - \E \left[ \nabla_{\beta^g} \mathcal{L}(\theta; m, g, p) \nabla_{\beta^m} \mathcal{L}(\theta; m, g, p)^T \right] \bigg][k,l] \\ 
= \left[ \E \left[ \tilde{X}^T \Delta^g s^g \right]\E \left[ \tilde{X}^T \Delta^m s^m \right]^T\right][k,l] - \E \left[ \tilde{X}^T \Delta^g s^g (s^m)^T \Delta^m \tilde{X} \right][k,l] \\ 
= \E\left[\tilde{X}[,k]^T \Delta^g s^g \right] \E \left[\tilde{X}[,l]^T \Delta^m s^m \right] - \E \left[\tilde{X}[,k]^T \Delta^g s^g (s^m)^T \Delta^m \tilde{X}[,l ] \right] \\
=\E\left( \sum_{i=1}^n \tilde{x}_{ik} \Delta^g_i s^g_i \right) \E \left( \sum_{j=1}^n \tilde{x}_{jl} \Delta^m_j s^m_j \right) - \E \left( \sum_{i=1}^n \sum_{j=1}^n \tilde{x}_{ik} \Delta^g_i s^g_i s^m_j \Delta^m_j \tilde{x}_{jl} \right) \\ 
= \sum_{i=1}^n \sum_{j=1}^n \E[\tilde{x}_{ik} \Delta^g_is^g_i] \E[ \tilde{x}_{jl} \Delta^m_j s^m_j ] - \sum_{i=1}^n \sum_{j=1}^n \E[ \tilde{x}_{ik} \Delta^g_i s^g_i s^m_j \Delta^m_j \tilde{x}_{jl}]  \\
= \sum_{i=1}^n \sum_{j=1}^n \E[ \tilde{x}_{ik} \Delta^g_i s^g_i] \E \left[\tilde{x}_{jl} \Delta^m_j s^m_j \right]  - \sum_{i \neq j} \E[\tilde{x}_{ik} \Delta^g_i s^g_i] \E[\tilde{x}_{jl}\Delta^m_j  s^m_j ] \\ - \sum_{i=1}^n \E[\tilde{x}_{ik} \Delta^g_i s^g_i s^m_i \Delta^m_i \tilde{x}_{il}] \\
= \sum_{i=1}^n \E[\tilde{x}_{ik} \Delta^g_i H^g_i] \E[\tilde{x}_{il} \Delta_i^m H^m_i] - \sum_{i=1}^n \E[\tilde{x}_{ik} H_i^g \Delta_i^g \Delta_i^m H_i^m \tilde{x}_{il}] \\ 
= \sum_{i=1}^n \left[\tilde{x}_{ik}(0) {\Delta}^g_i(0) T^\theta_i(0) {H}^g_i(0) + \tilde{x}_{ik}(1) {\Delta}^g_i(1) T^\theta_i(1) {H}^g_i(1)\right] \\ 
\cdot \left[\tilde{x}_{il}(0) {\Delta}^m_i(0) T^\theta_i(0) {H}^m_i(0) + \tilde{x}_{il}(1) {\Delta}^m_i(1) T^\theta_i(1) {H}^m_i(1)\right] 
\\ - \sum_{i=1}^n [\tilde{x}_{ik}(0) T^\theta_i(0) {\Delta}^g_i(0) {H}^g_i(0) {\Delta}^m_i(0) {H}^m_i(0) \tilde{x}_{il}(0) \\ + \tilde{x}_{ik}(1) T^\theta_i(1) {\Delta}^g_i(1) {H}^g_i(1) {\Delta}^m_i(1) {H}^m_i(1) \tilde{x}_{il}(1) ] 
\\ = \sum_{s=0}^1 \sum_{t=0}^1 \left[\sum_{i=1}^n \tilde{x}_{ik}(s) T^\theta_i(s) {\Delta}^g_i(s) {H}^g_i(s) T^\theta_i(t){\Delta}^m_i(t) {H}^m_i(t) \tilde{x}_{il}(t) \right]
\\ - \sum_{s=0}^1 \left[\sum_{i=1}^n \tilde{x}_{ik}(s) T^\theta_i(s) {\Delta}^g_i(s) {H}^g_i(s) {\Delta}^m_i(s) {H}^m_i(s) \tilde{x}_{il}(s)\right] 
\\ = \sum_{s=0}^1 \sum_{t=0}^1 \left[ \tilde{X}(s)[,k]^T T^\theta(s) {\Delta}^g(s) {H}^g(s) T^\theta(t){\Delta}^m(t) {H}^m(t) \tilde{X}(t)[,l] \right]\\ - \sum_{s=0}^1 \left[ \tilde{X}[,k]^T T^\theta(s) {\Delta}^g(s) {H}^g(s) {\Delta}^m(s) {H}^m(s) \tilde{X}[,l](s)\right].
\end{multline}
Combining (\ref{sub_mat_4}), (\ref{sub_mat_4_1}), and (\ref{sub_mat_4_2}) produces
\begin{multline}\label{sub_mat_4_formula}
J_{(\beta^g, \beta^m)}(\theta; m, g) = \sum_{s=0}^1 \sum_{t=0}^1 \tilde{X}(s)^T  T^\theta(s) {\Delta}^g(s) {H}^g(s) T^\theta(t){\Delta}^m(t) {H}^m(t) \tilde{X}(t) \\ - \sum_{s=0}^1 \tilde{X}(s)^T T^\theta(s) {\Delta}^g(s) {H}^g(s) {\Delta}^m(s) {H}^m(s) \tilde{X}(s).
\end{multline}
\\ \noindent
\subsubsection*{Submatrix V}
Denote submatrix V by $J_{(\beta^m,\pi)}(\theta; m, g).$ The formula for $J_{(\beta^m,\pi)}(\theta; m, g)$ is
\begin{multline}\label{sub_mat_5}
J_{(\beta^m,\pi)}(\theta; m, g) = \E \left[ - \nabla_{\beta^m} \nabla_{ \pi } \mathcal{L}(\theta; m, g, p) \right] \\ + \E\left[ \nabla_{\beta^m}\mathcal{L}(\theta ; m,g,p) \right] \E \left[ \nabla_{\pi}\mathcal{L}(\theta ; m,g,p) \right]^T  - \E \left[ \nabla_{\beta^m}\mathcal{L}(\theta; m,g,p) \nabla_{\pi}\mathcal{L}(\theta; m,g,p)^T \right].
\end{multline}
We have that
\begin{equation}\label{sub_mat_5_1}
\E \left[ - \nabla_{\beta^m} \nabla_{ \pi } \mathcal{L}(\theta; m, g, p) \right] = 0,
\end{equation}
as $\beta^m$ and $\pi$ separate in the log likelihood. Next, set $a := 1/\pi + 1/(1 - \pi)$ and $b := n/(1 - \pi).$ Recall from GLM theory that
$$\nabla_{\beta^m} \mathcal{L}(\theta; m, g, p) = \tilde{X}^T \Delta^m s^m$$ and from (\ref{d_L_d_pi}) that
$$ a \sum_{i=1}^n p_i - b.$$
The $k$th entry of the last two terms of (\ref{sub_mat_5}) is
\begin{multline}\label{sub_mat_5_2}
\E \left[\nabla_\pi \mathcal{L}(\theta; m, g, p) \right] \E\left[\nabla_{\beta^m} \mathcal{L}(\theta; m, g, p)[k] \right] - \E \left[\nabla_{\pi}\mathcal{L}(\theta; m,g,p) \nabla_{\beta^m}\mathcal{L}(\theta; m,g,p)[k] \right] \\= \left(\E \left[ a \sum_{i=1}^n p_i - b \right] \right) \left(\E\left[ \tilde{X}[,k]^T \Delta^m s^m \right] \right) - \E \left[ \left( a \sum_{i=1}^n p_i - b \right) \tilde{X}[,k]^T \Delta^m s^m \right] \\ = \left( a \sum_{i=1}^n \E[p_i] - b \right) \left( \sum_{j=1}^n \E [ \tilde{x}_{jk}\Delta^m_js^m_j] \right) - \E \left[ \left( a \sum_{i=1}^n p_i - b \right) \left( \sum_{j=1}^n \tilde{x}_{jk} \Delta^m_j s^m_j \right) \right] \\ = a \sum_{i=1}^n \sum_{j=1}^n \E [p_i] \E[ \tilde{x}_{jk} \Delta^m_j s^m_j] - b \sum_{j=1}^n \E[\tilde{x}_{jk} \Delta^m_j s^m_j] \\ - \left[ a \sum_{i=1}^n \sum_{j=1}^n \E [ p_i \tilde{x}_{jk} \Delta^m_j s^m_j] - b \sum_{j=1}^n \E[\tilde{x}_{jk} \Delta^m_j s^m_j] \right] \\ =  a \sum_{i=1}^n \sum_{j=1}^n \E[p_i] \E[\tilde{x}_{jk} \Delta^m_j s^m_j] - a\sum_{i \neq j} \E[p_i] \E[\tilde{x}_{jk} \Delta^m_j s^m_j] - a\sum_{i=1}^n \E[ p_i \tilde{x}_{ik} \Delta^m_i s^m_i ] \\ = a \sum_{i=1}^n \E[p_i] \E[ \tilde{x}_{ik} \Delta^m_i s^m_i] - a \sum_{i=1}^n \E[p_i \tilde{x}_{ik} \Delta^m_i s^m_i] \\ = a \sum_{i=1}^n T^\theta_i(1) [T^\theta_i(0) \Delta^m_i(0) s^m_i(0) \tilde{x}_{ik}(0) + T^\theta_i(1) \Delta^m_i(1) s^m_i(1) \tilde{x}_{ik}(1)] \\ - a \sum_{i=1}^n T^\theta_i(1)\Delta^m_i(1)s^m_i(1)\tilde{x}_{ik}(1) \\ = a \sum_{i=1}^n T^\theta_i(0)T^\theta_i(1) \Delta_i^m(0)H^m_i(0)\tilde{x}_{ik}(0) \\ + a \sum_{i=1}^n \left( [T^\theta_i(1)]^2 \Delta^m_i(1)H^m_i(1) - T^\theta_i(1)\Delta^m_i(1)H^m_i(1) \right) \tilde{x}_{ik}(1)  \\ =a \left[ \sum_{i=1}^n T^\theta_i(0) T^\theta_i(1) \Delta^m_i(0) H^m_i(0) \tilde{x}_{ik}(0) + \sum_{i=1}^n T^\theta_i(1)\Delta^m_i(1)H^m_i(1)[T^\theta_i(1) - 1] \tilde{x}_{ik}(1) \right] \\ = a \left[ \sum_{i=1}^n T^\theta_i(0) T^\theta_i(1) \Delta^m_i(0) H^m_i(0) \tilde{x}_{ik}(0) - \sum_{i=1}^n T^\theta_i(0) T^\theta_i(1) \Delta^m_i(1) H^m_i(1) \tilde{x}_{ik}(1) \right] \\ = a\left(\tilde{X}(0)[,k]^T w^m(0) - \tilde{X}(1)[,k]^T w^m(1)  \right).
\end{multline}
Combining (\ref{sub_mat_5}), (\ref{sub_mat_5_1}), and (\ref{sub_mat_5_2}), we conclude that
\begin{equation}\label{sub_mat_5_formula} J_{(\beta^m, \pi)}(\theta; m, g, p) = \left( \frac{1}{\pi} + \frac{1}{1 - \pi} \right) \left( \tilde{X}(0)^T w^m(0) - \tilde{X}(1)^T w^m(1)\right). \end{equation}

\subsubsection*{Submatrix VI}
Denote submatrix VI by $J_{(\beta^g,\pi)}(\theta; m, g).$ Calculations similar to those for submatrix V show that
\begin{equation}\label{sub_mat_6_formula} J_{(\beta^g, \pi)}(\theta; m, g, p) = \left(\frac{1}{\pi} + \frac{1}{1 - \pi} \right) \left( \tilde{X}(0)^T w^g(0) - \tilde{X}(1)^T w^g(1)\right). \end{equation}

\subsubsection*{Combining submatrices}
To summarize, the formulas for submatrices I-VI are as follows:
\begin{itemize}
\item[I]\begin{multline*}
J_\pi(\theta; m, g) = 
\left[ \frac{1}{\pi^2} - \frac{1}{(1 - \pi )^2} \right] \sum_{i=1}^n T^\theta_i(1) + \frac{n}{(1-\pi )^2} \\ + \left( \frac{1}{(1 - \pi )} + \frac{1}{\pi} \right)^2 \left( \sum_{i=1}^n [T^\theta_i(1)]^2 - T^\theta_i(1) \right).
\end{multline*}
\item[II] \begin{multline*}
J_{\beta^m}(\theta; m, g) = \sum_{s=0}^1 \tilde{X}(s)^T T^{\theta}(s) \left[ {\Delta}^m(s) {V}^m(s) {\Delta}^m(s) - [{\Delta}']^m(s) {H}^m(s) \right] \tilde{X}(s) \\ + \sum_{s=0}^1 \sum_{t=0}^1 \tilde{X}(s)^T {T}^\theta(s) {\Delta}^m(s) {H}^m(s) {T}^\theta(t) {\Delta}^m(t) {H}^m(t) \tilde{X}(t) \\ - \sum_{s=0}^1 \tilde{X}(s)^T T^{\theta}(s) ({\Delta}^m(s))^2 ({H}^m(s))^2 \tilde{X}(s).
\end{multline*}
\item[III] \begin{multline*}
J_{\beta^g}(\theta; m, g) = \sum_{s=0}^1 \tilde{X}(s)^T T^{\theta}(s) \left[{\Delta}^g(s) {V}^g(s) {\Delta}^g(s) - [{\Delta}']^g(s) {H}^g(s) \right] \tilde{X}(s) \\ + \sum_{s=0}^1 \sum_{t=0}^1 \tilde{X}(s)^T {T}^\theta(s) {\Delta}^g(s) {H}^g(s) {T}^\theta(t) {\Delta}^g(t) {H}^g(t) \tilde{X}(t) \\ - \sum_{s=0}^1 \tilde{X}(s)^T T^{\theta}(s) ({\Delta}^g(s))^2 ({H}^g(s))^2 \tilde{X}(s).
\end{multline*}
\item[IV] \begin{multline*}
J_{(\beta^g, \beta^m)}(\theta; m, g) = \sum_{s=0}^1 \sum_{t=0}^1 \tilde{X}(s)^T  T^\theta(s) {\Delta}^g(s) {H}^g(s) T^\theta(t){\Delta}^m(t) {H}^m(t) \tilde{X}(t) \\ - \sum_{s=0}^1 \tilde{X}(s)^T T^\theta(s) {\Delta}^g(s) {H}^g(s) {\Delta}^m(s) {H}^m(s) \tilde{X}(s).
\end{multline*}
\item[V] $$ J_{(\beta^m, \pi)}(\theta; m, g, p) = \left( \frac{1}{\pi} + \frac{1}{1 - \pi} \right) \left( \tilde{X}(0)^T w^m(0) - \tilde{X}(1)^T w^m(1)\right).  $$
\item[VI] $$ J_{(\beta^g, \pi)}(\theta; m, g, p) = \left(\frac{1}{\pi} + \frac{1}{1 - \pi} \right) \left( \tilde{X}(0)^T w^g(0) - \tilde{X}(1)^T w^g(1)\right).$$
\end{itemize}
We stitch these pieces together and transpose submatrices IV, V, and VI to produce the whole information matrix $J(\theta; m, g)$. Evaluating this matrix at the EM estimate $\theta^\textrm{EM}$ and inverting yields the asymptotic covariance matrix, which we can use to compute standard errors.

\subsection{Implementation}

\section{Statistical accelerations}

\section{Additional simulation results}

\end{appendices}

\bibliographystyle{unsrt}
\newpage
\bibliography{/Users/timbarry/optionFiles/glmeiv.bib}

\end{document}
