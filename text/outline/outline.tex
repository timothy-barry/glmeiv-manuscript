\documentclass[12pt]{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{xcolor}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\usepackage{float}
\usepackage[caption = false]{subfig}
\usepackage{/Users/timbarry/optionFiles/mymacros}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}
\noindent
Tim, Gene, Kathryn
\begin{center}
\textbf{CRISPR genome editing, single-cell sequencing, and exponential family measurement error models}
\end{center}

\begin{abstract}
CRISPR genome editing single-cell sequencing have transformed biological discovery. Single-cell CRISPR screens unite these two technologies, linking genetic perturbations in individual cells to changes in gene expression and illuminating regulatory networks underlying diseases. In this work we study single-cell CRISPR screens from a statistical perspective. First, we demonstrate on real data that a standard method for estimation and inference in single-cell CRISPR screens — “thresholded regression” — exhibits attenuation bias and a bias-variance tradeoff as a function of an intrinsic tuning parameter. We recover these phenomena in precise theoretical terms in an idealized Gaussian setting. Next, we introduce GLM-EIV (“generalized linear model with errors-in-variables”), a new method for single-cell CRISPR screen analysis. GLM-EIV generalizes the classical errors-in-variables model to response distributions and sources of measurement error that are exponential family-distributed, overcoming limitations of thresholded regression. We develop a computational infrastructure to deploy GLM-EIV across hundreds or thousands of processors on clouds (e.g., Microsoft Azure) and high-performance clusters. Leveraging this infrastructure, we apply GLM-EIV to analyze two recent, large-scale, single-cell CRISPR screen datasets, yielding new biological insights.
\end{abstract}

\section{Introduction}
CRISPR is a genome engineering tool that has enabled scientists to precisely edit human and nonhuman genomes, opening the door to new medical therapies \cite{Rothgangl2021,Musunuru2021} and transforming basic biology research \cite{Przybyla2021}. Recently, scientists have paired CRISPR genome engineering with single-cell sequencing \cite{Dixit2016,Datlinger2017}. The resulting assays, known as a ``single-cell CRISPR screens,'' link genetic perturbations in individual cells to changes in gene expression, illuminating regulatory networks underlying human diseases and other traits \cite{Morris2021a}.

Despite their promise, single-cell CRISPR screens present substantial statistical challenges. A major difficulty is that CRISPR perturbations are unobservable and assigned stochastically to cells. As a consequence, one cannot know with certainty which cells were perturbed. Instead, one must leverage an indirect, noisy proxy of perturbation presence or absence -- namely, transcribed barcode counts -- to ``guess'' which cells were perturbed. Using these imputed perturbation assignments, one can attempt to estimate the effect of the perturbation on gene expression. The standard approach, which we call ``thresholded regression'' or the ``thresholding method,'' is to assign perturbation identities to cells by simply thresholding the barcode counts.

We study estimation and inference in single-cell CRISPR screens from a statistical perspective, formulating the data generating mechanism using a new class of errors-in-variables (or measurement error) models. We assume that the response variable $y$ is a GLM of an underlying predictor variable $x^*$. We do not observe $x^*$ directly; rather, we observe a noisy version $x$ of $x^*$ that itself is a GLM of $x^*$. The goal of the analysis is to estimate the effect of $x^*$ on $y$ using the observed data $(x , y)$ only. In the context of the biological application, $x^*$, $y$, and $x$ are CRISPR perturbations, gene expressions, and barcode counts, respectively.

Our work makes two main contributions. First, we conduct an in-depth study of the thresholding method from empirical and theoretical perspectives. Notably, we demonstrate on real data that the thresholding method exhibits attenuation bias and a bias-variance tradeoff as a function of the selected threshold, and we recover these phenomena in precise mathematical terms in an idealized Gaussian model. Second, we introduce a new method for estimation and inference in single-cell CRISPR screens that accounts for the measurement error inherent in the experiment. The method, called \textit{GLM-EIV} (generalized linear model with errors-in-variables), implicitly estimates the probability that each cell was perturbed, obviating the need to explicitly impute perturbation assignments via thresholding or another heuristic. Theoretical analyses and simulation studies indicate that GLM-EIV outperforms the thresholding method in large regions of the parameter space.

We implement several statistical accelerations (that likely are of independent utility) to bring the cost of GLM-EIV down to within an order of magnitude of the thresholding method. Finally, we develop a computational infrastructure to deploy GLM-EIV at-scale across hundreds or thousands of processors on clouds (e.g., Microsoft Azure) and high-performance clusters. Leveraging this infrastructure, we apply GLM-EIV to analyze two recent, large-scale, high multiplicity-of-infection single-cell CRISPR screen datasets, yielding new biological and statistical insights. 

\section{Background and analysis challenges}

Our focus in this work is on high multiplicity-of-infection (MOI), enhancer-targeting, single-cell CRISPR screens. In this section we cover relevant biological background and motivation.
\\ \\
\textbf{P1}: The human genome consists of genes (segments of DNA that code for proteins), enhancers (segments of DNA that regulate the expression of one or more genes), and other genomic regions. Genome-wide association studies have revealed that the majority ($>95\%$) of variants associated with diseases lie outside genes and (very likely) inside enhancers. These noncoding variants contribute to disease by modulating the expression one or more genes, which in turn encode proteins that affect the phenotype. A central open challenge in genetics, therefore, is to link enhancers that harbor disease-associated variants to the genes that they target at genome-wide scale.
\\ \\
\textbf{P2}: High MOI single-cell CRISPR screens are the most promising biotechnology for solving this problem. \blue{Describe the experimental protocol here. Explain that we use the terms ``barcodes'' and ``gRNAs'' interchangeably, as polyadenylated gRNAs serve as barcodes in CROP-seq. Link to Figure \ref{analysis_challenges}.}
\\ \\
\textbf{P3}: Single-cell CRISPR screens pose several core analysis challenges. \blue{Describe the analysis challenges here: (i) unobserved perturbation; (ii) existence of background reads; (iii) highly discrete count data; (iv) nuisance variables}.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.9\linewidth]{analysis_challenges}
	\caption{\textbf{Overview of experimental design and analysis challenges}: \textbf{a,} Experimental design. For a given perturbation (e.g., the perturbation represented in yellow), we partition the cells into two groups: those that received the perturbation, and those that did not receive the perturbation. For a given gene, we conduct a differential expression analysis across the two groups of cells, yielding an estimate of the impact of the given perturbation on the given gene. \textbf{b,} DAG representing the variables in the analysis. The perturbation (unobserved) affects both gene expression and gRNA expression; technical factors (e.g., batch, sequencing depth, etc.) act as nuisance variables. The target of inference is the effect of the perturbation on gene expression (denoted with question mark). \textbf{c,} Schematic illustrating ``background reads.'' The gRNA modality has a nonzero, ``background read'' distribution even in the absence of a perturbation, complicating the assignment of perturbations to cells. \textbf{d}, Example data for a given perturbation-gene pair. Notice that (i) the perturbations are unobserved, and (ii) the gene and gRNA expression data take the form of discrete counts.}
	\label{analysis_challenges}
\end{figure}

\section{Related work}

Motivated by the challenges of single-cell data, several authors recently have extended statistical models that (implicitly or explicitly) assume Gaussianity and homoscedasticity to a broader class of exponential family distributions. For example, Lin, Lei, and Roeder \cite{Lin2021} developed eSVD, an extension of SVD to exponential family and curved Gaussian responses. Unlike SVD, eSVD models the relationship between the mean and variance of a gene's expression level, a phenomenon induced by the countedness of single-cell data \cite{Lause2021}.
Similarly, Townes et al.\ \cite{Townes2019} proposed GLM-PCA, an extension of PCA that directly models Poisson- or negative binomially-distributed gene expression counts. We see our work as a continuation of this broad effort to ``port'' common statistical methods and models to single-cell count data. Our focus, however, is on regression rather than dimension reduction: we extend the classical errors-in-variables model to response distributions and sources of measurement error that are exponential family-distributed.

The closest parallels to our work in the statistical methodology literature are Gr\"{u}n \& Leisch \cite{Grun2008} and Ibrahim \cite{Ibrahim1990}. Gr\"{u}n \& Leisch considered estimation and inference in a $k$-component mixture of GLMs. While we prefer to view the GLM-EIV model as an errors-in-variables model,  the GLM-EIV model is equivalent to a two-component mixture of \textit{products} of GLM densities. Ibrahim derived a method for fitting GLMs in the presence of missing-at-random covariates. Our method, by contrast, involves fitting two conditionally independent GLMs in the presence of a totally latent covariate. Thus, while Ibrahim is a helpful reference, our estimation and inference tasks are more complex.

The genomics literature has produced several applied methods for linking perturbations to changes in gene expression in single-cell CRISPR screens: SCEPTRE \cite{Barry2020}, MIMOSCA \cite{Dixit2016}, and scMAGeCK \cite{Yang2019}. These methods in general are focused on hypothesis testing rather than estimation; none, for instance, produces a confidence interval for the effect size of a perturbation on gene expression change. Additionally, two of these methods (MIMOSCA and scMAGeCK) use (possibly penalized) linear models to model gene expressions, thereby disregarding the countedness and sparsity of the data. 

\section{Thresholding method}

In this section we study the thresholding method from empirical and theoretical perspectives. First, we define some notation.

\blue{Let $n \approx 100,000 - 250,000$ be the number of cells in the experiment. Consider a single perturbation-gene pair. For cell $i \in \{ 1, \dots, n \},$ let $p_i \in \{ 0,1 \}$ indicate whether the cell was perturbed, $m_i \in \N$ be the number of observed gene UMIs, $g_i \in \N$ be the number of observed gRNA UMIs, $l^m_i \in \N$ be the gene library size, and $z_i \in \R^{d-1}$ be a vector of technical factors (e.g., batch, percent mitochondrial reads, etc.). The thresholding method is defined as follows:}
\blue{
\begin{itemize}
\item[1.] For given threshold $c \in \N$, calculate the imputed value $\hat{p}_i$ of $p_i$ by $$\begin{cases} \hat{p}_i = 0 \textrm{ if } g_i \geq c, \\ \hat{p}_i = 1 \textrm{ if } g_i < c. \end{cases}$$
\item[2.] Fit the regression model \cite{Sarkar2021}
$$ m_i | \left(\hat{p}_i, z_i, l^m_i \right) \sim \textrm{NB}_\theta(\mu_i),$$ where $\theta >0$ is the NB size parameter, and $$\log\left(\mu_i\right) = \beta_m^0 + \beta_m \hat{p}_i + \gamma^T_m z_i + \log\left( l_i^m\right).$$
\item[3.] Fit a GLM to obtain an estimate $\hat{\beta}_m$ of $\beta_m$. Compute a confidence interval and $p$-value for $\beta_m$.
\end{itemize}}

\subsection{Empirical analysis}

\blue{To investigate the impact of threshold selection on the thresholding method, we applied the thresholding method to the set of positive control (i.e., gene-targeting) perturbation-gene pairs in the Gasperini data using three different choices for the threshold: 1, 5, and 20.}

\blue{Description of panels a-d; threshold = 1 leads to considerable attenuation bias (a); threshold = 5 and threshold = 20 produce similar estimates, though the effect sizes are slightly greater for threshold = 20, suggesting threshold = 5 yields mild attenuation bias (b); the threshold = 20 estimates are more variable than than those of threshold = 5, as evidenced by the smaller $p$-values and wider confidence intervals.}

\blue{Description of panels e-f: these are the empirical gRNA count distributions for randomly selected gRNAs from the Gasperini and Xie datasets. There does not appear to be a clear location at which to draw the threshold; aside from the initial spike at one (zero not shown), the histograms gradually decrease.}

\blue{Take-home message: (i) the threshold is a tuning parameter that substantially affects the result; (ii) as the threshold increases, bias seems to decrease and variance seems to increase; (iii) it is not clear where to draw the threshold from gRNA counts alone.}

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{../../figures/thresholding_empirical/plot}
	\caption{\textbf{Empirical challenges of thresholded regression.}}
	\label{thresholding_empirical}
\end{figure}
\newpage

\subsection{Theoretical analysis}

We study the thresholding method from a theoretical perspective in an idealized Gaussian setting. Suppose that we observe data $(g_1, m_1), \dots, (g_n, m_n)$ from the following model:

\begin{equation}\label{theoretical_model}
\begin{cases}
m_i = \beta^m_0 + \beta^m_1 p_i + \ep_i \\
g_i = \beta^g_0 + \beta^g_1 p_i + \tau_i \\
p_i \sim \textrm{Bern}(\pi) \\
\ep_i, \tau_i \sim N(0,1) \\
p_i \indep \tau_i \indep \ep_i.
\end{cases}
\end{equation}
For a given threshold $c \in \R$, the imputed perturbation assignment $\hat{p}_i$ is $\hat{p}_i = \mathbb{I}(g_i \geq c).$ The thresholding estimator $\hat{\beta}^m_1$ for $\beta^m_1$ is given by $$\hat{\beta}^m_1 = \frac{\sum_{i=1}^n (\hat{p}_i - \overline{\hat{p}}) (m_i - \overline{m})}{\sum_{i=1}^n (\hat{p}_i - \overline{\hat{p}})}.$$

\begin{proposition}
 The almost sure limit (as $n \to \infty$) of $\hat{\beta}^m_1$ is
\begin{equation}\label{thresh_est_intercepts}
\hat{\beta}^m_1 \xrightarrow{a.s.} \beta^m_1 \left(\frac{ \pi( \omega - \E[ \hat{p}_i ])}{ \E[\hat{p}_i] (1 - \E[\hat{p}_i])}\right),
\end{equation}
where
$$\begin{cases}
\E[\hat{p}_i] = \zeta(1-\pi) + \omega\pi , \\
\omega = \Phi\left(\beta_1^g + \beta_0^g -c \right) ,\\ \zeta = \Phi\left( \beta^g_0 - c \right). \\
\end{cases}$$
\end{proposition}
Let $\gamma: \R^4 \to \R$ be defined by
$$ \gamma(\beta^g_1, \pi, c, \beta^g_0) = \frac{\pi (\omega - \E[\hat{p}_i])}{\E[\hat{p}_i] (1 - \E[\hat{p}_i]) }.$$ We call $\gamma$ the ``attenuation function.'' Observe that \begin{itemize}
\item[i.] $\gamma$ does not depend on $\beta^m_1$ or $\beta^m_0$, and
\item[ii.] $\hat{\beta}^m_1 \xrightarrow{a.s.} [\gamma(\beta_0^g, \beta_1^g, c, \pi)] \beta^m_1.$
\end{itemize}
Let $b: \R^5 \to \R$ be the asymptotic bias of $\hat{\beta}^m_1$, i.e.
$$
b(\beta^g_1, \pi, c, \beta^g_0, \beta^m_1) = \lim_{n\to\infty} \left(\beta^m_1 - \E[\hat{\beta}^m_1]\right) = \beta^m_1 - \E \left( \lim_{a.s.} \hat{\beta}^m_1 \right) = \beta^m_1 - \lim_{a.s.} \hat{\beta}^m_1,
$$
where the $\lim_{a.s.}$ denotes a.s.\ convergence. \textcolor{red}{(HOW TO MAKE THE ABOVE RIGOROUS?)} We can express $b$ in terms of $\gamma$ as $$b(\beta^g_1, \pi, c, \beta^g_0, \beta^m_1) = \beta^m_1 - \gamma(\beta^g_1, \pi, c, \beta^g_0)  \beta^m_1 = \beta^m_1 \left[1 - \gamma(\beta^g_1, \pi, c, \beta^g_0)  \right].$$ The asymptotic bias vanishes when the attenuation function equals $1$.

\begin{center}
\textbf{Bias as a function of threshold (Panel a)}
\end{center}

To investigate the basic question of ``What is a good threshold selection strategy?'', we study the relationship between the asymptotic bias $b$ of $\hat{\beta}^m_1$ and the selected threshold $c$. For simplicity, we set the perturbation probability $\pi$ to $1/2$. Let $c_\textrm{bayes} \in \R$ be the Bayes-optimal decision boundary for classifying cells as perturbed or unperturbed, i.e. $$c_\textrm{bayes} = \argmin_{c \in \R} \P(\hat{p}_i \neq p_i).$$ Simple algebra shows that $c_\textrm{bayes} = \beta_0^g + (1/2) \beta^g_1.$ Below, we state several results on the limiting bias $b$ of $\hat{\beta}^m_1$. We refer throughout to Figure \ref{thresholding_theoretical}a, which displays plots of asymptotic bias versus threshold for different values of $\beta^g_1$; the target of inference, $\beta^m_1$, is fixed to $1$ in these plots.

\begin{itemize}
\item 
\begin{proposition}\label{prop:att_bias} Assume without loss of generality that $\beta^m_1 > 0.$ Fix $\pi = 1/2$. For all $(\beta^g_1, c, \beta^g_0) \in \R^3$,
	$$b(\beta^g_1, 1/2, c, \beta^g_0, \beta^m_1) > 0.$$
\end{proposition}
The thresholding method suffers strict attenuation bias for all choices of the threshold and over all possible model parameters (Figure \ref{thresholding_theoretical}a). Attenuation bias is a common attribute of estimators that ignore measurement in errors-in-variables models \cite{Stefanski2000a}.	
	
\item \begin{proposition}
For $\pi = 1/2$ and given $(\beta^g_1, \beta^g_0, \beta^m_1) \in \R^3$, the Bayes-optimal decision boundary $c_\textrm{bayes}$ is a critical value of the limiting bias function $b$, i.e.
$$ \frac{\partial b}{\partial c}\left(\beta^g_1, 1/2, c_\textrm{bayes}, \beta^g_0, \beta^m_1\right) = 0.$$
\end{proposition}
The Bayes-optimal decision boundary is an optimum (or possibly a saddle point) of the limiting bias function (Figure \ref{thresholding_theoretical}a, vertical blue lines). Interestingly, $c_\textrm{bayes}$ is in some cases a maximizer of the bias (Figure \ref{thresholding_theoretical}a, left) and in other cases a minimizer of the bias (Figure \ref{thresholding_theoretical}a, right).
\item
\begin{proposition}\label{prop:limit}
Fix $\pi = 1/2$. As the threshold $c$ tends to infinity, the asymptotic bias $b$ tends to $\beta_1^m/2$, i.e.
$$\lim_{c \to \infty} b(\beta^g_1, 1/2, c, \beta^g_0, \beta^m_1) = \beta_1^m/2.$$
\end{proposition}
We always can set the threshold to a large number and attain a bias of $\beta_1^m/2$ (Figure \ref{thresholding_theoretical}a, all panels). This result establishes an upper bound on the bias of thresholded regression (under optimal threshold selection strategy).

 \item The following proposition compares the two threshold selection strategies introduced above (i.e., large number versus Bayes-optimal decision boundary) head-to-head.
 \begin{proposition} Let $\beta^m_1 > 0$. For $\beta^g_1 \in [0, 2\Phi^{-1}(3/4))$, we have that $$b(\beta^g_1, 1/2, c_\textrm{bayes}, \beta^g_0, \beta^m_1) > b(\beta^g_1, 1/2, \infty, \beta^g_0, \beta^m_1).$$ For $\beta^g_1 = 2\Phi^{-1}(3/4)$, we have that $$ b(\beta^g_1, 1/2, c_\textrm{bayes}, \beta^g_0, \beta^m_1) = b(\beta^g_1, 1/2, \infty, \beta^g_0, \beta^m_1).$$ Finally, for $\beta^g_1 \in (2\Phi^{-1}(3/4), \infty)$, we have that
 $$b(\beta^g_1, 1/2, c_\textrm{bayes}, \beta^g_0, \beta^m_1) < b(\beta^g_1, 1/2, \infty, \beta^g_0, \beta^m_1).$$
 \end{proposition}
In other words, setting the threshold to a large number yields a smaller bias when $\beta^g_1$ is small (i.e., $\beta^g_1 < 2\Phi^{-1}(3/4) \approx 1.35$; Figure \ref{thresholding_theoretical}a, left); setting the threshold to the Bayes-optimal decision boundary yields a smaller bias when $\beta^g_1$ is big (i.e., $\beta^g_1 > 2\Phi^{-1}(3/4)$; Figure \ref{thresholding_theoretical}a, right); and the two approaches coincide when $\beta^g_1$ is intermediate (i.e., $\beta^g_1 = 2\Phi^{-1}(3/4)$; Figure \ref{thresholding_theoretical}a, middle).

\item
 \begin{proposition}\label{prop_monotonic} Fix $\pi = 1/2$. The limiting bias $b$ decreases monotonically in $\beta_1^g$, i.e.
$$\frac{\partial b}{\partial(\beta^g_1)}\left(\beta^g_1, 1/2, c, \beta^g_0, \beta^m_1\right) \leq 0.$$
\end{proposition}
This result formalizes the intuition that the problem becomes easier as the gRNA mixture distribution becomes increasingly well-separated. To visualize Proposition (\ref{prop_monotonic}), one can fix a threshold (e.g., $c = 0$) and scan for bias across the panels.
\end{itemize}

These results are subtle, but we can summarize them as follows. First, selecting a threshold that minimizes the bias is challenging, as there is no rule of thumb that we can apply universally (e.g., ``always choose the Bayes-optimal decision boundary'' or ``always choose a large number'') due to the complexity of the bias function. Second, even if we \textit{have} selected a good threshold, we incur nonzero attenuation bias.

\begin{center}
\textbf{Generalizing to $\pi \in [0,1/2]$ (Panel b)}
\end{center}
We generalize the expression for bias to arbitrary $\pi \in [0,1/2]$:
\begin{proposition}
As the threshold $c$ tends to infinity, the limiting bias $b$ tends to $\beta^m_1 \pi,$ i.e.
$$ \lim_{ c \to \infty } b(\beta^g_1, \pi, c, \beta^g_0, \beta^m_1) = \beta^m_1 \pi.$$
\end{proposition}
That is, if the perturbation probability is $\pi$, and if we set the threshold to a large number, then the asymptotic bias is $\beta^m_1 \pi$ (Figure \ref{thresholding_theoretical}b; $\beta^m_1$ fixed at $1$). We can understand this result intuitively by considering an extreme example: when $\pi$ is very small (e.g., $\pi = 0.01$), almost all cells are unperturbed. Therefore, in selecting a large threshold, we correctly classify nearly all unperturbed cells as unperturbed; on the other hand, the \textit{perturbed} cells that we misclassify as \textit{unperturbed} are swamped in number by the truly unperturbed cells, resulting in a small bias.

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{../../figures/thresholding_theoretical/plot}
	\caption{\textbf{Theoretical challenges of thresholded regression.} \textbf{a,} (Asymptotic) bias versus threshold for different values of $\beta^g_1$. The bias function is highly nonconvex and strictly nonzero. Across all panels, $\beta^g_0 = 0, \beta^m_1 = 1$, and $\pi = 1/2$. \textbf{b,} Bias versus $\pi$ when the threshold is set to a large number. Bias is directly proportional to $\pi$ in this setting. \textbf{c,} Bias-variance decomposition of thresholding estimator in no-intercept model. Bias decreases and variance increases as the threshold tends to infinity. $\beta^m_1 = 1, \beta^g_1 = 1,$ and $\pi = 0.1$.}
	\label{thresholding_theoretical}
\end{figure}

\begin{center}
	\textbf{Bias-variance tradeoff (Panel c)}
\end{center}

Finally, to shed light on limitations of the large threshold selection strategy, we derive an exact bias-variance decomposition for the thresholding estimator. We consider a slightly simpler, no-intercept version of (\ref{theoretical_model}) for this purpose:
\begin{equation}\label{theoretical_model_no_int}
\begin{cases}
m_i = \beta_m p_i + \ep_i \\
g_i = \beta_g p_i + \tau_i \\
p_i \sim \textrm{Bern}(\pi) \\
\ep_i, \tau_i \sim N(0,1) \\
p_i \indep \tau_i \indep \ep_i.
\end{cases}
\end{equation}
The thresholding estimator $\hat{\beta}_m$ in the no-intercept case is
\begin{equation}\label{thresh_estimator_no_int}
 \hat{\beta}_m = \frac{ \sum_{i=1}^n \hat{p}_i m_i }{\sum_{i=1}^n \hat{p}_i^2}. 
\end{equation}
\begin{proposition}
The limiting distribution of $\hat{\beta}_m$ is
$$\sqrt{n}(\hat{\beta}_m - l) \xrightarrow{d} N\left(0, \frac{ \beta_m\omega\pi(\beta_m - 2l) + \E[\hat{p}_i](1 + l^2) }{\left(\E[\hat{p}_i]\right)^2} \right),$$ where
$$\begin{cases}
l = \beta_m\omega \pi/[\zeta(1-\pi) + \omega \pi], \\
\E[\hat{p}_i] = \pi \omega + (1-\pi) \zeta, \\
\omega = \Phi(\beta_g - c), \\
\zeta = \Phi(- c).\\
\end{cases}
$$
\end{proposition}
This result yields an exact bias-variance decomposition for $\hat{\beta}_m$ for large $n$ (Figure \ref{thresholding_theoretical}c). As the threshold tends to infinity, the bias decreases and the variance increases, consistent with the intuition that a large threshold reduces the misclassification rate at the cost of decreasing the ``effective sample size.'' The best strategy for maximizing estimation accuracy (as quantified by mean squared error) is to select a threshold that induces moderate bias. A downside of this approach, however, is that constructing valid confidence intervals becomes more challenging.

% These difficulties motivate our core research question: does bypassing thresholding altogether lead to better estimation and inference in single-cell CRISPR screens? To answer this question, we propose GLM-EIV, method for single-cell CRISPR screen analysis that models gRNA counts directly.

\section{GLM-EIV}

\section{Simulation studies}

\section{Real data analysis}

\section{Discussion}

\section{Appendix}

\subsection{Theoretical details for thresholding estimator}

This section contains proofs for results presented the section ``Theoretical analysis of thresholding estimator.'' The subsections are arranged as follows: FILL IN...

\subsubsection{Notation}
 All notation introduced here pertains to the Gaussian model with intercepts (\ref{theoretical_model}). Recall that the attenuation function $\gamma: \R^4 \to \R$ is defined by
$$ \gamma(\beta^g_1, c, \pi, \beta^g_0) = \frac{\pi(\omega - \E[\hat{p}_i])}{ \E[\hat{p}_i](1 -\E[\hat{p}_i])},$$ where $$\begin{cases}
\omega = \Phi\left(c - \beta_1^g - \beta_0^g \right) ,\\ \zeta = \Phi\left( c - \beta^g_0 \right), \\ \E[\hat{p}_i] = \zeta(1-\pi) + \omega\pi. 
\end{cases}$$ Additionally, recall that the bias function $b: \R^5 \to \R$ is defined by
$$ b(\beta^g_1, c, \pi, \beta^g_0, \beta^m_1) = \beta^m_1 (1 - \gamma(\beta^g_1, c, \pi, \beta^g_0)).$$ The parameters $\beta^g_0$ and $\beta^m_1$ are given, fixed constants throughout the proofs. Therefore, to minimize notation, we typically use $\gamma(\beta^g_1, c, \pi)$ (resp., $b(\beta^g_1, c, \pi)$) to refer to the function $\gamma$ (resp., $b$) evaluated at $(\beta^g_1, c, \pi, \beta^g_0)$ (resp., $(\beta^g_1, c, \pi, \beta^g_0, \beta^m_1)$). Next, we define the functions $g$ and $h: \R^4 \to \R$ by
\begin{equation}\label{def_g}
g(\beta^g_1, c, \pi, \beta^g_0) = (1-\pi)\left( \int_{ -\infty}^{\beta_0^g + \beta_1^g - c} f \right) - (1-\pi)\left( \int_{-\infty}^{ \beta_0^g - c} f \right)\end{equation}
 and
\begin{multline}\label{def_h}
h(\beta^g_1, c, \pi, \beta^g_0) = \left[(1-\pi)\left( \int_{-\infty}^{ \beta_0^g - c} f \right) + \pi\left( \int_{-\infty}^{ \beta^g_0 + \beta^g_1 - c} f \right) \right] \cdot \\ \left[(1-\pi)\left( \int_{-\infty}^{c - \beta^g_0} f \right) + \pi\left(\int_{-\infty}^{c - \beta_0^g - \beta_1^g} f \right) \right],
\end{multline} where $f: \R \to [0,\infty)$ is the $N(0,1)$ density. We will refer to $g$ and $h$ repeatedly throughout the proofs. Similarly to above, we typically use the notation $g(\beta^g_1, c, \pi)$ (resp.\ $h(\beta^g_1, c, \pi)$) to refer to $g$ (resp.\ $h$) evaluated at $(\beta^g_1, c, \pi, \beta^g_0)$.

Finally, for a given function $r: \R^{p} \to \R$, point $x \in \R^p$, and index $i \in \{1, \dots, p\}$, we use the symbol $D_i r(x)$ to refer to the derivative of the $i$th component of $r$ evaluated at $x$ (\textit{sensu} \cite{fitzpatrick2009}). For example, $D_1 \gamma(\beta^g_1, c, 1/2)$ is the derivative of the first component of $\gamma$ (the component corresponding to $\beta^g_1$) evaluated at $(\beta^g_1, c, 1/2)$. Likewise,  $D_2g(\beta^g_1, c, \pi)$ is the derivative of the second component of $g$ (the component corresponding to $c$) evaluated at $(\beta^g_1, c, \pi).$

\subsubsection{Limit of $\hat{\beta}^m_1$}

We derive the limit in probability of $\hat{\beta}^m_1$ for the Gaussian model with intercepts (\ref{theoretical_model}). Dividing by $n$ in (\ref{thresh_est_intercepts}), we can express $\hat{\beta}^m_1$ as
$$ \hat{\beta}^m_1 = \frac{ \frac{1}{n} \sum_{i=1}^n ( \hat{p}_i - \overline{\hat{p}_i})(m_i - \overline{m})}{ \frac{1}{n} \sum_{i=1}^n (\hat{p}_i - \overline{\hat{p}})}.$$ By weak LLN,
$$ \hat{\beta}^m_1 \xrightarrow{P} \frac{\textrm{Cov}(\hat{p}_i, m_i)}{\V\left(\hat{p}_i\right)}.$$ To compute this quantity, we first compute several simpler quantities:
\begin{itemize}
\item[1.] Expectation of $m_i$: $\E[m_i] = \beta^m_0 + \beta^m_1\pi.$
\item[2.] Expectation of $\hat{p}_i$: Let $f$ denote the $N(0,1)$ density. Then \begin{multline*}
\E[\hat{p}_i] = \P\left[\hat{p}_i = 1\right] = \P\left[\beta^g_0 + \beta^g_1 p_i + \tau_i \geq c \right] = \\ \textrm{(By LOTP) } \P\left[ \beta^g_0 + \tau_i \geq c \right]\P\left[p_i = 0\right] + \P\left[ \beta^g_0 + \beta^g_1 + \tau_i \geq c \right] \P[p_i = 1] \\ = \P\left[ \tau_i \geq c - \beta^g_0\right](1- \pi) + \P\left[ \tau_i \geq c - \beta^g_1 - \beta^g_0 \right](\pi) \\ = \left(\int_{c - \beta^g_0}^\infty f\right) (1 - \pi) + \left( \int_{c - \beta^g_1 - \beta^g_0}^\infty f \right)(\pi) = \zeta(1-\pi) + \omega \pi.
\end{multline*}
\item[3.] Expectation of $\hat{p}_i p_i$: 
$$\E\left[ \hat{p}_i p_i \right] = \E\left[\hat{p}_i | p_i = 1 \right] \P\left[ p_i =1 \right] = \P\left[ \beta^g_0 + \beta^g_1 + \tau_i \geq c \right] \pi = \omega \pi.$$
\item[4.] Expectation of $\hat{p}_i m_i$:
\begin{multline*}
\E\left[\hat{p}_i m_i\right] = \E[\hat{p}_i (\beta^m_0 + \beta^m_1 p_i + \ep_i)] = \beta^m_0 \E\left[\hat{p}_i\right] + \beta^m_1 \E\left[\hat{p}_i p_i\right] + \E[\hat{p}_i \ep_i] \\ = \beta^m_0 \E[\hat{p}_i] + \beta^m_1 \omega \pi + \E[\hat{p}_i] \E[\ep_i] = \beta^m_0 \E[\hat{p}_i] + \beta^m_1 \omega \pi.
\end{multline*}
\item[5.] Variance of $\hat{p}_i$: Because $\hat{p}_i$ is binary, we have that $\V[\hat{p}_i] = \E[\hat{p}_i]\left(1 - \E[\hat{p}_i]\right) .$
\item[6.] Covariance of $\hat{p}_i, m_i$:
\begin{multline*}
\textrm{Cov}\left(\hat{p}_i, m_i\right) = \E\left[\hat{p}_i m_i\right] - \E[\hat{p}_i] \E[m_i] = \beta^m_0 \E[\hat{p}_i] + \beta^m_1 \omega \pi - \E[\hat{p}_i]( \beta^m_0 + \beta^m_1 \pi)\\ = \beta^m_1 \omega \pi - \E[\hat{p}_i] \beta_1^m \pi = \beta^m_1 \pi \left( \omega - \E[\hat{p}_i]\right).
\end{multline*}
Combining these expressions, we have that
$$ \hat{\beta}^m_1 \xrightarrow{P} \frac{\beta^m_1 \pi (\omega - \E[\hat{p}_i])}{\E[\hat{p}_i](1 - \E[\hat{p}_i])} = \beta^m_1 \gamma(\beta^g_1, c, \pi).$$
\end{itemize}

\subsubsection{Re-expressing $\gamma$ in a simpler form}
We rewrite the attenuation fraction $\gamma$ in a way that makes it more amenable to theoretical analysis. We leverage the fact that $f$ integrates to unity and is even.
First,
\begin{multline}\label{thm:gamma_expression_1} \E\left[\hat{p}_i\right] = (1 - \pi) \left( \int_{c - \beta^g_0}^\infty f \right) + \pi \left(\int_{c - \beta_0^g - \beta_1^g}^\infty f \right) \\ = (1 - \pi) \left( \int_{-\infty}^{ \beta_0^g - c} f \right) + \pi \left( \int_{-\infty}^{ \beta^g_0 + \beta^g_1 - c} f \right), \end{multline}
 and so \begin{multline}\label{thm:gamma_expression_2} 1 - \E\left[\hat{p}_i\right] = (1 - \pi) + \pi - \E[\hat{p}_i]  = (1-\pi) \left(1 - \int_{c - \beta_0^g}^\infty f \right)  + \pi \left(1 - \int_{c - \beta^g_0 - \beta^g_1}^\infty f \right) \\ = (1 - \pi) \left( \int_{-\infty}^{c - \beta^g_0} f \right) + \pi \left( \int_{-\infty}^{c - \beta_0^g - \beta_1^g} f \right).
\end{multline}
Next,
\begin{equation}\label{thm:gamma_expression_3}
\omega = \int_{c - \beta^g_1 - \beta^g_0}^\infty f,\end{equation} and so
\begin{multline}\label{thm:gamma_expression_4}
\omega - \E[\hat{p}_i]  = \left( \int_{c - \beta^g_1 - \beta^g_0}^\infty f \right) - (1 - \pi) \left( \int_{c - \beta^g_0}^\infty f \right) - \pi \left( \int_{c - \beta_0^g - \beta_1^g}^\infty f \right) \\ = (1 - \pi) \left(\int_{c - \beta^g_0 - \beta^g_1 }^\infty f \right) - (1- \pi) \left(\int_{c - \beta^g_0}^\infty f \right) \\ =  (1 - \pi)\left(\int_{-\infty}^{\beta_0^g + \beta_1^g - c} f \right) - (1 - \pi) \left( \int_{-\infty}^{ \beta_0^g - c} f \right).
\end{multline}
Combining (\ref{thm:gamma_expression_1}, \ref{thm:gamma_expression_2}, \ref{thm:gamma_expression_3}, \ref{thm:gamma_expression_4}), we find that
\begin{multline}\label{gamma_alternative}
\gamma(\beta^g_1, c, \pi) = \frac{\pi(\omega - \E[\hat{p}_i])}{\E[\hat{p}_i](1 - \E[\hat{p}_i])} \\ = \frac{\pi \left[ (1 - \pi) \left( \int_{ -\infty}^{\beta_0^g + \beta_1^g - c} f \right) - (1 - \pi) \left( \int_{-\infty}^{ \beta_0^g - c} f \right) \right]}{\left[(1-\pi) \left( \int_{-\infty}^{ \beta_0^g - c} f \right) + \pi \left( \int_{-\infty}^{ \beta^g_0 + \beta^g_1 - c} f \right) \right] \left[(1 - \pi) \left( \int_{-\infty}^{c - \beta^g_0} f \right) + \pi \left( \int_{-\infty}^{c - \beta_0^g - \beta_1^g} f \right) \right]}.
\end{multline}
As a corollary, when $\pi = 1/2$,
\begin{multline}\label{gamma_alternative_pi_half}
\gamma(\beta^g_1, c, 1/2)  \\ = \frac{\left( \int_{ -\infty}^{\beta_0^g + \beta_1^g - c} f \right) - \left( \int_{-\infty}^{ \beta_0^g - c} f \right)}{\left[ \left( \int_{-\infty}^{ \beta_0^g - c} f \right) + \left( \int_{-\infty}^{ \beta^g_0 + \beta^g_1 - c} f \right) \right] \left[\left( \int_{-\infty}^{c - \beta^g_0} f \right) + \left(\int_{-\infty}^{c - \beta_0^g - \beta_1^g} f \right) \right]}.
\end{multline}
Recalling the definitions of $g$ (\ref{def_g}) and $h$ (\ref{def_h}), we can write $\gamma$ as
$$ \gamma(\beta^g_1, c, \pi) = \frac{\pi g(\beta^g_1, c, \pi)}{h(\beta^g_1, c,\pi)}.$$
The special case (\ref{gamma_alternative_pi_half}) is identical to
\begin{equation}\label{gamma_alt2_pi_half}
\gamma(\beta^g_1, c, 1/2) = \frac{(4)(1/2)g(\beta^g_1, c, 1/2)}{4 h(\beta^g_1, c, 1/2)} = \frac{2 g(\beta^g_1, c, 1/2)}{4h(\beta^g_1, c, 1/2)},
\end{equation}
i.e., the numerator and denominator of  (\ref{gamma_alt2_pi_half}) coincide with those of (\ref{gamma_alternative_pi_half}).

\subsubsection{Derivatives of  $g$ and $h$ in $c$}
We compute the derivatives of $g$ and $h$ in $c$, which we will need to prove subsequent results. First, by FTC and the evenness of $f$, we have that
\begin{multline}\label{dg_dc}
D_2 g(\beta^g_1, c, \pi) = -(1-\pi)f( \beta^g_0 + \beta^g_1 - c ) + (1-\pi) f(\beta^g_0 - c) \\ = (1-\pi) f(c - \beta^g_0) - (1-\pi)f(c - \beta^g_0 - \beta^g_1).
\end{multline}
Second, we have that
\begin{multline}\label{dh_dc}
D_2 h(\beta^g_1, c, \pi) \\ = -[(1-\pi)f(\beta^g_0 - c) + \pi f( \beta^g_0 + \beta^g_1 - c )]\left[(1-\pi)\left(\int_{-\infty}^{c - \beta^g_0} f \right) + \pi \left(\int_{-\infty}^{c - \beta_0^g - \beta_1^g} f \right) \right] \\ + [(1-\pi) f(c - \beta^g_0) +  \pi f(c - \beta^g_0 - \beta^g_1)] \left[(1-\pi)\left( \int_{-\infty}^{\beta_0^g - c} f \right) + \pi \left(\int_{-\infty}^{ \beta^g_0 + \beta^g_1 - c} f \right) \right] \\ = \left[ (1-\pi) f(c - \beta^g_0) +  \pi f(c - \beta^g_0 - \beta^g_1) \right] \cdot \\ \bigg[ (1-\pi)\left( \int_{-\infty}^{\beta_0^g - c} f \right) + \pi \left(\int_{-\infty}^{ \beta^g_0 + \beta^g_1 - c} f \right) \\ - (1-\pi)\left(\int_{-\infty}^{c - \beta^g_0} f \right) - \pi \left(\int_{-\infty}^{c - \beta_0^g - \beta_1^g} f \right) \bigg].
\end{multline}

\subsubsection{Limit of $\gamma$ in $c$}

Assume (without loss of generality) that $\beta^g_1 > 0$. We compute $\lim_{c \to \infty} \gamma(\beta^g_1, c, \pi)$. Observe that $$\lim_{c \to \infty} g(\beta^g_1, c, \pi) = \lim_{c \to \infty} h(\beta^g_1, c, \pi)  = 0.$$ Therefore, we can apply L'H\^{o}pital's rule. We have by (\ref{dg_dc}) and (\ref{dh_dc}) that \begin{multline}\label{c_limit_product}
\lim_{c \to \infty} \gamma(\beta^g_1, c, \pi) = \lim_{c \to \infty} \frac{\pi D_2 g(\beta^g_1, c, \pi)}{D_2h(\beta^g_1, c, \pi)} \\ = \lim_{c \to \infty} \bigg\{ \frac{(1-\pi) f(c - \beta^g_0) + \pi f(c - \beta^g_0 - \beta^g_1)}{\pi (1-\pi) f(c - \beta^g_0) - \pi (1-\pi)f(c - \beta^g_0 - \beta^g_1)} \\ \cdot \bigg[ (1-\pi)\left( \int_{-\infty}^{\beta_0^g - c} f \right) + \pi \left(\int_{-\infty}^{ \beta^g_0 + \beta^g_1 - c} f \right) \\ - (1-\pi)\left(\int_{-\infty}^{c - \beta^g_0} f \right) - \pi \left(\int_{-\infty}^{c - \beta_0^g - \beta_1^g} f \right) \bigg] \bigg\}^{-1}.
 \end{multline}
 We evaluate the two terms in the product (\ref{c_limit_product}) separately. First, observe that
 \begin{equation}\label{c_limit_product_2}
 \frac{(1-\pi) f(c - \beta^g_0) + \pi f(c - \beta^g_0 - \beta^g_1)}{\pi (1-\pi) f(c - \beta^g_0) - \pi (1-\pi)f(c - \beta^g_0 - \beta^g_1)} = \frac{\frac{(1-\pi) f(c - \beta^g_0)}{ f(c - \beta^g_0 - \beta^g_1)} + \pi}{\frac{ \pi(1-\pi) f(c - \beta^g_0)}{ f(c - \beta^g_0 - \beta^g_1)} - \pi(1-\pi)}.
 \end{equation}
 To evaluate the limit of (\ref{c_limit_product_2}), must evaluate the limit of
 \begin{multline}\label{c_limit_product_3}
 \frac{f(c - \beta^g_0)}{f(c - \beta^g_0 - \beta^g_1)} = \frac{\exp{[-(1/2)(c - \beta_0^g)^2]}}{\exp{[-(1/2)( c - \beta^g_0 - \beta^g_1)^2]}} \\ = \frac{\exp[ -(1/2)(c^2 - 2 c \beta^g_0 + (\beta^g_0)^2)]}{\exp\left[-(1/2)( c^2 - 2c \beta^g_0 - 2 c \beta^g_1 + (\beta^g_0)^2 + 2( \beta^g_0 \beta^g_1) + (\beta^g_1)^2)\right]} \\ = \exp\big[-c^2/2 + c \beta^g_0 - (\beta^g_0)^2/2 \\ + c^2/2 - c \beta^g_0 - c \beta^g_1 + (\beta^g_0)^2/2 + \beta^g_0 \beta^g_1 + (\beta^g_1)^2/2 \big] \\ = \exp[ -c \beta^g_1 + \beta^g_0 \beta^g_1 + (\beta^g_1)^2/2] = \exp[ \beta^g_0 \beta^g_1 + (\beta^g_1)^2/2]\exp[ -c \beta^g_1]. 
\end{multline}
Computing the limit of (\ref{c_limit_product_3}), we obtain
$$
\lim_{c \to \infty} \frac{f(c - \beta^g_0)}{f(c - \beta^g_0 - \beta^g_1)} = \exp[ \beta^g_0 \beta^g_1 + (\beta^g_1)^2/2] \lim_{c \to \infty} \exp[ -c \beta^g_1] = 0
$$ for $\beta^g_1 > 0$. We now can evaluate the limit of (\ref{c_limit_product_2}) (for $\beta^g_1 > 0$):
$$ \lim_{c \to \infty} \frac{(1-\pi) f(c - \beta^g_0) + \pi f(c - \beta^g_0 - \beta^g_1)}{\pi (1-\pi) f(c - \beta^g_0) - \pi (1-\pi)f(c - \beta^g_0 - \beta^g_1)} = \frac{-\pi}{\pi(1-\pi)} = -\frac{1}{1 -\pi}.$$ Next, we compute the limit of the other term in the product (\ref{c_limit_product}):
\begin{multline}\label{c_limit_product_4}
\lim_{c \to \infty} \bigg[ (1-\pi)\left( \int_{-\infty}^{\beta_0^g - c} f \right) + \pi \left(\int_{-\infty}^{ \beta^g_0 + \beta^g_1 - c} f \right) \\ - (1-\pi)\left(\int_{-\infty}^{c - \beta^g_0} f \right) - \pi \left(\int_{-\infty}^{c - \beta_0^g - \beta_1^g} f \right) \bigg] = -(1-\pi) - \pi = -1.
\end{multline}
Combining (\ref{c_limit_product_2}) and (\ref{c_limit_product_4}), the limit (\ref{c_limit_product}) evaluates to
$$ \lim_{c \to \infty} \gamma(\beta^g_1, c, \pi) = \left(  \frac{ 1 }{ 1 - \pi }\right)^{-1} = 1 - \pi.$$ It follows that the limit in $c$ of the bias $b$ is
$$\lim_{c \to \infty} b(\beta^g_1, c, \pi) = \beta_1^m(1 - \lim_{c \to \infty} \gamma(\beta^g_1, c, \pi)) = \beta^m_1 \pi.$$
A corollary is that
$$\lim_{c \to \infty} b(\beta^g_1, c, 1/2) = (1/2)\beta^m_1.$$

\subsubsection{Bayes-optimal decision boundary as a critical value of $\gamma$}
Let $c_\textrm{bayes} = \beta^g_0 + (1/2)\beta^g_1.$ We show that $c = c_\textrm{bayes}$ is a critical value of $\gamma$ for $\pi = 1/2$ and given $\beta^g_1$, i.e, $$D_2 \gamma (\beta^g_1, c_\textrm{bayes}, 1/2) = 0.$$ By the quotient rule,
\begin{multline}\label{quotient_rule}
D_2\gamma(\beta^g_1, c, 1/2) \\ = \frac{(1/2) D_2g(\beta^g_1, c, 1/2) h(\beta^g_1, c, 1/2) - (1/2) g(\beta^g_1, c, 1/2) D_2 h(\beta^g_1, c, 1/2)}{[h(\beta^g_1, c, \pi)]^2} \\ = \frac{\left[4(1/2) D_2g(\beta^g_1, c, 1/2)\right] h(\beta^g_1, c, 1/2) - (1/2) g(\beta^g_1, c, 1/2) \left[4 D_2 h(\beta^g_1, c, 1/2)\right]}{[2h(\beta^g_1, c, \pi)]^2},
\end{multline}
where the second equality multiplies both numerator and denominator by $4$. We have by (\ref{dg_dc}) that
\begin{multline}\label{dg_dc_bayes}
4 \cdot (1/2) \cdot D_2 g(\beta^g_1, c_\textrm{bayes}, 1/2)  = f( \beta^g_1/2) - f( -\beta^g_1/2) \\ = f(\beta^g_1/2) - f(\beta^g_1/2) = 0.
\end{multline}
Next, we have by (\ref{dh_dc}) that
\begin{multline}\label{dh_dc_bayes}
4 \cdot D_2 h(\beta^g_1, c_\textrm{bayes}, \pi) = [f( \beta^g_1/2) + f( -\beta^g_1/2)] \cdot \\ \left[  \int_{-\infty}^{-\beta^g_1/2} f + \int_{-\infty}^{\beta^g_1/2} f - \int_{-\infty}^{\beta^g_1/2} f - \int_{-\infty}^{-\beta^g_1/2} f \right] = 0.
\end{multline}
Plugging in (\ref{dh_dc_bayes}) and (\ref{dg_dc_bayes}) to (\ref{quotient_rule}), we find that 
$$D_2(\beta^g_1, c_\textrm{bayes}, 1/2) = 0.$$ It follows that $c_\textrm{bayes}$ is a critical value of the bias function $b$.

\subsubsection{Comparison of Bayes-optimal decision boundary to large threshold}

We study the bias produced by setting the threshold to a large number versus setting the threshold to the Bayes-optimal decision boundary. Let $r: \R \to \R$ be the value of attenuation function evaluated at the Bayes-optimal decision boundary $c_\textrm{bayes} = \beta^g_0 + (1/2) \beta^g_1$:
\begin{multline*}
r(\beta^g_1) = \gamma(\beta^g_1, \beta^g_0 + (1/2)\beta^g_1, 1/2) = \frac{\Phi(\beta^g_1/2) - \Phi(-\beta^g_1/2)}{\left[\Phi(-\beta^g_1/2) + \Phi( \beta^g_1/2) \right] \left[\Phi(\beta^g_1/2) + \Phi( -\beta^g_1/2)\right]} \\ = \frac{\int_{-\beta^g_1/2}^{\beta^g_1/2} f}{\left[ 1 - \Phi(\beta^g_1/2) + \Phi(\beta^g_1/2) \right]\left[ \Phi(\beta^g_1/2) + 1 - \Phi(\beta^g_1/2) \right]} = 2 \int_{0}^{\beta^g_1/2} f = 2 \Phi(\beta^g_1/2) - 1.
\end{multline*}
We set $r$ to $1/2$ and solve for $\beta^g_1$:
$$ 2\Phi(\beta^g_1/2) -1 = 1/2 \iff \Phi(\beta^g_1/2) = 3/4 \iff \beta^g_1 = 2 \Phi^{-1}(3/4) \approx 1.35.$$ The bias induced selecting a large threshold is $1/2$, i.e.
$$\gamma(\beta^g_1, \infty, 1/2) = 1/2.$$
Additionally, $r$ is a strictly increasing function. Therefore, $\gamma(\beta^g_1, c_\textrm{bayes},1/2) < \gamma(\beta^g_1, \infty, 1/2)$ on $\beta^g_1 \in [0, 2\Phi^{-1}(3/4))$; $\gamma(\beta^g_1, c_\textrm{bayes},1/2) = \gamma(\beta^g_1, \infty, 1/2)$ for $\beta^g_1 = 2\Phi^{-1}(3/4)$; and $\gamma(\beta^g_1, c_\textrm{bayes},1/2) > \gamma(\beta^g_1, \infty, 1/2)$ on $\beta^g_1 \in (2\Phi^{-1}(3/4), \infty)$. Because $b(\beta^g_1, c, 1/2) = \beta^m_1(1 - \gamma(\beta^g_1, c, 1/2))$ similar results hold for the bias (but with the inequalities flipped).

\subsubsection{Limit in $\beta^g_1$}
We compute the limit of $\gamma$ in $\beta^g_1$ given $\pi = 1/2$.  First,
\begin{multline*}
 \lim_{\beta^g_1 \to \infty} \gamma(\beta^g_1, c, 1/2) = \frac{ 1 - \int_{-\infty}^{\beta^g_0 - c} f}{\left[ 1 + \int_{-\infty}^{ \beta^g_0 - c} f \right] \left[\int_{-\infty}^{ c - \beta^g_0} f \right]} \\ = \frac{\int_{-\infty}^{c - \beta^g_0} f }{ \left[1 + \int_{-\infty}^{ \beta^g_0 - c} f \right] \left[\int_{-\infty}^{ c - \beta^g_0} f \right]} = \frac{1}{1 + \int_{-\infty}^{\beta^g_0 - c} f} < 1.
\end{multline*}
Similarly,
\begin{multline*}
\lim_{\beta^g_1 \to -\infty} \gamma(\beta^g_1, c, 1/2) = \frac{ -\int_{-\infty}^{\beta^g_0 - c} f }{\left[ \int_{-\infty}^{\beta^g_0 - c} f \right] \left[ \int_{-\infty}^{c - \beta^g_0} f + 1 \right]} = \frac{-1}{1 + \int_{-\infty}^{c - \beta^g_0} f} > -1.
\end{multline*}

\subsubsection{Monotonicity in $\beta^g_1$}
We show that $\gamma$ is monotonically increasing in $\beta^g_1$ for $\pi = 1/2$ and given $(\beta^g_1, \beta^g_0)$. We begin by stating and proving two lemmas. The first lemma establishes an inequality that will serve as the basis for the proof.

\begin{lemma}
The following inequality holds: 
\begin{multline}\label{basic_ineq_cp}
\left[\Phi(\beta^g_0 - c) + \Phi(\beta^g_0 + \beta^g_1 - c) \right] \\ \cdot \left[\Phi(\beta_0^g + \beta_1^g - c) - \Phi(\beta_0^g - c) + \Phi(c - \beta^g_0) + \Phi(c - \beta_0^g - \beta_1^g) \right] \\ \geq \left[\Phi(\beta_0^g + \beta_1^g - c) - \Phi(\beta_0^g - c)\right]\left[\Phi(c - \beta^g_0) + \Phi(c - \beta_0^g - \beta_1^g)\right].
\end{multline}
\end{lemma}
\textbf{Proof}: We take cases on the sign on $\beta^g_1$.

\noindent
\underline{Case 1}: $\beta^1_g < 0$. Then $ \beta^g_1 + (\beta^g - c) < (\beta^g_0 - c),$ implying $\Phi(\beta^g_0 + \beta^g_1 - c) < \Phi(\beta^g_0 - c),$ or $[\Phi(\beta^g_0 + \beta^g_1 - c) - \Phi(\beta^g_0 - c)] < 0.$ Moreover, $[\Phi(c - \beta^g_0) + \Phi(c - \beta_0^g - \beta_1^g)]$ is positive. Therefore, the right-hand side of (\ref{basic_ineq_cp}) is negative.

Turning our attention of the left-hand side of (\ref{basic_ineq_cp}), we see that
\begin{equation}\label{basic_ineq_cp_2}
\Phi(\beta^g_0 + \beta^g_1 - c) + \Phi( c - \beta^g_0 - \beta^g_1) = 1 -\Phi(\beta^g_0 + \beta^g_1 - c) + \Phi( c - \beta^g_0 - \beta^g_1) = 1.
\end{equation}
Additionally, $\Phi(\beta^g_0 - c) < 1$ and $ \Phi(c - \beta^g_0) > 0$. Combining these facts with (\ref{basic_ineq_cp_2}), we find that
$$ \left[\Phi(\beta_0^g + \beta_1^g - c) - \Phi(\beta_0^g - c) + \Phi(c - \beta^g_0) + \Phi(c - \beta_0^g - \beta_1^g) \right] > 0. $$ Finally, because $\left[\Phi(\beta^g_0 - c) + \Phi(\beta^g_0 + \beta^g_1 - c) \right] > 0,$ the entire left-hand side of (\ref{basic_ineq_cp}) is positive.

\noindent
\underline{Case 2}: $\beta^1_g \geq 0.$  We will show that the first term on the LHS of (\ref{basic_ineq_cp}) is greater than the first term on the RHS of (\ref{basic_ineq_cp}), and likewise that the second term on the LHS is greater than the second term on the RHS, implying the truth of the inequality. Focusing on the first term, the positivity of $\Phi(\beta^g_0 -c)$ implies that
$$ \Phi(\beta^g_0 - c) \geq - \Phi(\beta^g_0 - c),$$ and so
$$ \Phi(\beta^g_0 - c) + \Phi(\beta^g_0 + \beta^g_1 - c) \geq \Phi(\beta^g_0 - \beta^g_1 - c) - \Phi(\beta^g_0 - c).$$
Next, focusing on the second term, $\beta^g_1 \geq 0$ implies that 
\begin{equation}\label{basic_ineq_cp_3}
\beta^g_1 + \beta^g_0 - c \geq \beta^g_0 - c \implies \Phi(\beta^g_1 + \beta^g_0 - c) - \Phi(\beta^g_0 - c) \geq 0.
\end{equation}
Adding $\Phi(c - \beta^g_0) + \Phi(c - \beta^g_0 - \beta^g_1)$ to both sides of (\ref{basic_ineq_cp_3}) yields
\begin{multline*}
\Phi(\beta^g_1 + \beta^g_0 - c) - \Phi(\beta^g_0 - c) + \Phi(c - \beta^g_0) + \Phi(c - \beta^g_0 - \beta^g_1) \\ \geq \Phi(c - \beta^g_0) + \Phi(c - \beta^g_0 - \beta^g_1). \textrm{ } \square
\end{multline*}
The second lemma establishes the derivatives of the functions $2\cdot g$ and $4 \cdot h$.
\begin{lemma}
The derivatives in $\beta^g_1$ of $2\cdot g$ and $4\cdot h$ are
\begin{equation}\label{dg_dbeta}
\textcolor{violet}{D_1[2g(\beta^g_1, c, 1/2)] = f(\beta^g_0 + \beta^g_1 - c)}
\end{equation}
and
\begin{multline}\label{dh_dbeta}
\textcolor{teal}{D_1[4h(\beta^g_1, c, 1/2)] = f(\beta^g_0 + \beta^g_1 - c) \left[\Phi(c - \beta^g_0) + \Phi(c - \beta_0^g - \beta_1^g) \right]} \\ \textcolor{teal}{- f(\beta^g_0 + \beta^g_1 - c) \left[\Phi(\beta_0^g - c) + \Phi(\beta^g_0 + \beta^g_1 - c) \right]}.\end{multline}
\end{lemma}
\textbf{Proof}: Apply the FTC and product rule. $\square$

We move onto the proof of the monotonicity of $\gamma$ in $\beta^g_1$. Subtracting $$\left[\Phi(\beta_0^g - c) + \Phi(\beta^g_0 + \beta^g_1 - c) \right]\left[\Phi(\beta_0^g + \beta_1^g - c) - \Phi(\beta_0^g - c)\right]$$ from both sides of (\ref{basic_ineq_cp}) and multiplying by $f(\beta^g_0 + \beta^g_1 - c) > 0$ yields
\begin{multline}\label{basic_ineq_cp_4}
\textcolor{violet}{f(\beta^g_0 + \beta^g_1 - c)} \textcolor{red}{ \left[\Phi(\beta^g_0 - c) + \Phi\left(\beta^g_0 + \beta^g_1 - c \right) \right] \left[ \Phi(c - \beta^g_0) + \Phi(c - \beta^g_0 - \beta^g_1) \right]}  \\ \geq \textcolor{teal}{f(\beta^g_0 + \beta^g_1 - c) \left[\Phi(c - \beta^g_0) + \Phi(c - \beta_0^g - \beta_1^g)\right]}\textcolor{blue}{\left[\Phi(\beta_0^g + \beta_1^g - c) - \Phi(\beta_0^g - c)\right]} \\ -\textcolor{teal}{f(\beta^g_0 + \beta^g_1 - c)   \left[\Phi(\beta_0^g - c) + \Phi(\beta^g_0 + \beta^g_1 - c) \right]} \textcolor{blue}{\left[\Phi(\beta_0^g + \beta_1^g - c) - \Phi(\beta_0^g - c)\right]}.
\end{multline}
Next, recall that
\begin{equation}\label{def_2g}
\textcolor{blue}{2g(\beta^g_1,c,1/2) = \Phi(\beta^g_0 + \beta^g_1 - c) - \Phi(\beta^g_0 - c)}.
\end{equation}
and
\begin{equation}\label{def_4h}
\textcolor{red}{4h(\beta^g_1, c, 1/2) = \left[ \Phi(\beta^g_0 - c) + \Phi(\beta^g_0 + \beta^g_1 - c) \right] \left[\Phi(c - \beta^g_0) + \Phi( c - \beta^g_0 - \beta^g_1) \right]}.
\end{equation}
Substituting (\ref{dg_dbeta}, \ref{dh_dbeta}, \ref{def_2g}, \ref{def_4h}) into (\ref{basic_ineq_cp_4}) produces
\begin{equation*}
\textcolor{violet}{D_1[2g(\beta^g_1, c, 1/2)]}\textcolor{red}{4h(\beta^g_1, c, 1/2)} \geq \textcolor{blue}{2g(\beta^g_1, c, 1/2)}\textcolor{teal}{D_1[4h(\beta^g_1, c, 1/2)]},
\end{equation*}
or 
\begin{equation}\label{basic_ineq_cp_5}
\textcolor{violet}{D_1[2g(\beta^g_1, c, 1/2)]}\textcolor{red}{4h(\beta^g_1, c, 1/2)} - \textcolor{blue}{2g(\beta^g_1, c, 1/2)}\textcolor{teal}{D_1[4h(\beta^g_1, c, 1/2)]} \geq 0.
\end{equation}
The quotient rule implies that
\begin{multline}\label{d_gamma_d_beta}
D_1 \gamma(\beta^g_1, c, 1/2) \\ = \frac{ \textcolor{violet}{D_1[2g(\beta^g_1, c, 1/2)]}\textcolor{red}{4h(\beta^g_1, c, 1/2)} - \textcolor{blue}{2g(\beta^g_1, c, 1/2)}\textcolor{teal}{D_1[4h(\beta^g_1, c, 1/2)]} }{[4h(\beta^g_1, c, 1/2)]^2}.
\end{multline}
We conclude by (\ref{basic_ineq_cp_5}) and (\ref{d_gamma_d_beta}) that $\gamma(\cdot, c, 1/2)$ is monotonically increasing in its first argument.

\subsubsection{Strict attenuation bias}

For given $(\beta^g_0, c)$, we have that
$$ \lim_{\beta^g_1 \to \infty} \gamma(\beta^g_1, c, 1/2, \beta^g_0) = \frac{1}{1 + \Phi(\beta^g_0 - c)},$$ and $$\lim_{\beta^g_1 \to -\infty} \gamma(\beta^g_1, c, 1/2, \beta^g_0) = -\frac{1}{1 + \Phi(c - \beta^g_0)}.$$
Moreover, $\gamma(\beta^g_1, c, 1/2, \beta^g_0)$ is monotonically increasing in $\beta^g_1$. It follows that 
$$-1 < -\frac{1}{1 + \Phi(c - \beta^g_0)} \leq \gamma(\beta^g_1, c, 1/2, \beta^g_0) \leq \frac{1}{1 - \Phi(\beta^g_0 - c)} < 1$$ for all $\beta^g_1 \in \R$. But $\beta^g_0$ and $c$ were chosen arbitrarily, and so
$$-1 < \gamma(\beta^g_1, c, 1/2, \beta^g_0) < 1$$ for all $(\beta^g_1, c, \beta^g_0) \in \R^3$.

\textcolor{red}{DOES STRICT ATTENUATION BIAS HOLD UNIFORMLY OVER THE PARAMETER SPACE?}
  
\subsubsection{Bias-variance decomposition in no-intercept model} 
 
We prove the bias-variance decomposition for the no-intercept model (\ref{theoretical_model_no_int}). Define $l$ (for ``limit'') by
$$l = \beta_m \left(\frac{\omega \pi}{\zeta(1-\pi) + \omega \pi}\right),$$ where
$$
\begin{cases}
\omega = \int_{c - \beta_g}^\infty f = 1 - \Phi(\beta_g - c) \\
\zeta = \int_{c}^\infty f = 1 - \Phi(-c).
\end{cases}
$$
We have that
\begin{equation*}
\hat{\beta}_m - l = \frac{\sum_{i=1}^n \hat{p}_i m_i}{ \sum_{i=1}^n \hat{p}^2_i} - l = \frac{\sum_{i=1}^n \hat{p}_i m_i}{ \sum_{i=1}^n \hat{p}^2_i} - \frac{l \sum_{i=1}^n \hat{p}_i^2 }{ \sum_{i=1}^n \hat{p}_i^2} \\ = \frac{\sum_{i=1}^n \hat{p}_i(m_i - l \hat{p}_i)}{ \sum_{i=1}^n \hat{p}_i^2}.
\end{equation*}
Therefore,
\begin{equation}\label{bc_decomp_1}
\sqrt{n}(\hat{\beta}_m - l) = \frac{(1/\sqrt{n})\sum_{i=1}^n \hat{p}_i(m_i - l \hat{p}_i)}{(1/n)\sum_{i=1}^n \hat{p}_i^2}.
\end{equation}
Next, we compute the expectation and variance of $\hat{p}_i(m_i - l\hat{p}_i)$. To do so, we first compute several simpler quantities:
\begin{enumerate}
\item Expectation of $\hat{p}_i$: 
\begin{multline*}
\E[\hat{p}_i] = \P(p_i\beta_g + \tau_i \geq c) =  \P(\beta_g + \tau_i \geq c)\pi + \P(\tau_i \geq c)(1-\pi) \\ = \pi \omega + (1-\pi)\zeta.
\end{multline*}
\item Expectation of $\hat{p}_i p_i$: $$\E\left[\hat{p}_i p_i\right] = \E\left[\hat{p}_i | p_i = 1 \right]\P\left[p_i = 1\right] = \omega \pi.$$
\item Expectation of $\hat{p}_i m_i$:
\begin{multline*}
\E[\hat{p}_i m_i] = \E\left[\hat{p}_i(\beta_m p_i + \ep_i)\right] = \E\left[\beta_m \hat{p}_i p_i + \hat{p}_i \ep_i \right] \\ = \beta_m \E\left[ \hat{p}_i p_i \right] + \E[\hat{p}_i]\E[\ep_i] = \beta_m \omega \pi + 0 = \beta_m \omega \pi.
\end{multline*}
\item Expectation of $\hat{p}_i m_i^2$: \begin{multline*}
\E\left[\hat{p}_i m_i^2\right] = \E \left[ \hat{p}_i( \beta_m p_i + \ep_i )^2 \right] = \E\left[ \hat{p}_i \left( \beta_m^2 p_i^2 + 2 \beta_m p_i \ep_i + \ep_i^2 \right)  \right] \\ = \E\left[ \hat{p}_i p_i \beta^2_m + 2 \beta_m p_i \hat{p}_i \ep_i + \hat{p}_i \ep_i^2 \right] = \beta^2_m \E[ \hat{p}_i p_i] + 2 \beta_m \E[p_i\hat{p}_i] \E[\ep_i] + \E[\hat{p}_i] \E[ \ep^2_i ] \\ = \beta^2_m \E[ \hat{p}_i p_i] + \E[\hat{p}_i] = \beta^2_m \omega \pi + \E[ \hat{p}_i]. 
\end{multline*}
\end{enumerate}

Now, we can compute the expectation and variance of $\hat{p}_i(m_i - l\hat{p}_i)$. First,
\begin{multline}\label{bv_decomp_2}
\E\left[\hat{p}_i(m_i - l\hat{p}_i) \right] = \E[\hat{p}_i m_i] - l \E[\hat{p}_i] \\ = \beta_m \omega \pi - \left(\frac{\beta_m \omega \pi}{\zeta (1-\pi) + \omega \pi}\right)[\zeta (1-\pi) + \omega \pi] = 0.
\end{multline}
Additionally,
\begin{multline}\label{bv_decomp_3}
\V\left[\hat{p}_i(m_i - l\hat{p}_i)\right] = \E\left[\hat{p}_i^2(m_i - l\hat{p}_i)^2\right] - (\E\left[ \hat{p}_i(m_i - l\hat{p}_i)\right])^2 \\ = \E\left[ \hat{p}_i m_i^2\right] - 2l \E[m_i\hat{p}_i] +l^2 \E[\hat{p}_i]= \beta^2_m \omega \pi + \E[ \hat{p}_i] -2l \beta_m \omega \pi + l^2 \E[\hat{p}_i] \\ = \beta_m\omega\pi(\beta_m - 2l) + \E[\hat{p}_i](1 + l^2).
\end{multline}
Therefore, by CLT, (\ref{bv_decomp_2}), and (\ref{bv_decomp_3}),
\begin{equation}\label{bv_decomp_4}
(1/\sqrt{n})\sum_{i=1}^n \hat{p}_i(m_i - l \hat{p}_i) \xrightarrow{d} N\left(0, \beta_m\omega\pi(\beta_m - 2l) + \E[\hat{p}_i](1 + l^2) \right).
\end{equation}
Next, by weak LLN,
\begin{equation}\label{bv_decomp_5}
(1/n) \sum_{i=1}^n \hat{p}_i^2 = (1/n) \sum_{i=1}^n \hat{p}_i \xrightarrow{P} \E[\hat{p}_i].
\end{equation}
Finally, by (\ref{bc_decomp_1}), (\ref{bv_decomp_4}), (\ref{bv_decomp_5}), and Slutsky's Theorem,
$$ \sqrt{n}(\hat{\beta}_m - l) \xrightarrow{d} N\left(0, \frac{ \beta_m\omega\pi(\beta_m - 2l) + \E[\hat{p}_i](1 + l^2) }{\left(\E[\hat{p}_i]\right)^2} \right).$$ Thus, for large $n \in \N$, we find that 
$$ \begin{cases}
\E [\hat{\beta}_m] \approx l, \\
\V[\hat{\beta}_m] \approx \left[\beta_m\omega\pi(\beta_m - 2l) + \E[\hat{p}_i](1 + l^2)\right]/[n\E^2[\hat{p}_i]],
\end{cases}$$
completing the bias-variance decomposition.

\subsection{Derivation of EM algorithm}

\subsection{Derivation of observed information matrix}

\subsection{Implementation using R family objects}

\subsection{Statistical accelerations to GLM-EIV}

\subsection{Additional simulation results}


\bibliographystyle{unsrt}
\newpage
\bibliography{/Users/timbarry/optionFiles/glmeiv.bib}


\end{document}
