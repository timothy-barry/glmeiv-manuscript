\relax 
\citation{Rothgangl2021}
\citation{Musunuru2021}
\citation{Przybyla2021}
\citation{Dixit2016}
\citation{Datlinger2017}
\citation{Morris2021a}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}}
\citation{Lin2021}
\citation{Lause2021}
\citation{Townes2019}
\citation{Grun2008}
\citation{Ibrahim1990}
\citation{Barry2020}
\citation{Candes2018}
\citation{Liu2021}
\citation{Dixit2016}
\citation{Yang2019}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background and analysis challenges}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Related work}{4}}
\citation{Gasperini2019}
\citation{Datlinger2021}
\citation{Mimitou2019}
\citation{Gallagher2018}
\citation{Gasperini2020}
\citation{Replogle2020}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Assay overview}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Experimental design and analysis challenges}: \textbf  {a,} Experimental design. For a given perturbation (e.g., the perturbation indicated in blue), we partition the cells into two groups: perturbed and unperturbed. Next, for a given gene, we conduct a differential expression analysis across the two groups, yielding an estimate of the impact of the given perturbation on the given gene. \textbf  {b,} DAG representing all variables in the system. The perturbation (latent) impacts both gene expression and gRNA expression; technical factors act as confounders, also impacting gene and gRNA expression. The target of estimation is the effect of the perturbation on gene expression. \textbf  {c,} Schematic illustrating the ``background read'' phenomenon. Due to errors in the sequencing and alignment processes, unperturbed cells exhibit a nonzero gRNA count distribution (bottom). The target of estimation is the change in mean gene expression in response to the perturbation (top). \textbf  {d}, Example data on four cells for a given perturbation-gene pair. Note that (i) the perturbation is unobserved, and (ii) the gene and gRNA data are discrete counts.}}{6}}
\newlabel{analysis_challenges}{{1}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Analysis challenges}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Thresholding method}{7}}
\newlabel{thresh_glm}{{1}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Empirical challenges of thresholding method}{8}}
\newlabel{sec:thresholding_empirical}{{3.1}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Empirical challenges of thresholded regression.} \textbf  {a-b,} Fold change estimates produced by threshold = 1 versus threshold = 5 (a) and threshold = 20 versus threshold = 5 (b). The selected threshold substantially impacts the results. \textbf  {c-d,} $p$-values (c) and CI widths (d) produced by threshold = 20 versus threshold = 5. The latter threshold yields more confident estimates. \textbf  {e-f}, Empirical distribution of randomly-selected gRNA from Gasperini (e) and Xie (f) data (0 counts not shown). The gRNA data do not appear to imply an obvious threshold selection strategy.}}{10}}
\newlabel{thresholding_empirical}{{2}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Theoretical challenges of thresholding method}{10}}
\newlabel{sec:thresholding_theory}{{3.2}{10}}
\newlabel{theoretical_model}{{2}{11}}
\newlabel{prop:convergence}{{1}{11}}
\newlabel{thresh_est_intercepts}{{3}{11}}
\citation{Stefanski2000a}
\newlabel{prop:att_bias}{{2}{12}}
\newlabel{prop:monotonic}{{3}{12}}
\newlabel{prop:bayes_opt}{{4}{12}}
\newlabel{prop:c_limit_half}{{5}{13}}
\newlabel{prop:comparison}{{6}{13}}
\newlabel{prop:c_limit}{{7}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Theoretical challenges of thresholded regression.} \textbf  {a,} Asymptotic relative bias versus threshold for different values of $\beta ^g_1$. The bias function is highly nonconvex and strictly nonzero. Vertical blue lines, Bayes-optimal decision boundaries. Across all panels, $\beta ^g_0 = 0$ and $\pi = 1/2$. \textbf  {b,} Asymptotic relative bias versus $\pi $ when the threshold is set to a large number. The two quantities coincide exactly. \textbf  {c,} Bias-variance decomposition for thresholding method in no-intercept model. Bias decreases and variance increases as the threshold tends to infinity. $\beta ^g_1 = 1, \beta ^m_1 = 1,$ and $\pi = 0.1$.}}{14}}
\newlabel{thresholding_theoretical}{{3}{14}}
\newlabel{theoretical_model_no_int}{{4}{15}}
\newlabel{thresh_estimator_no_int}{{5}{15}}
\newlabel{prop:bv_decomp}{{8}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Thresholding method summary}{15}}
\citation{Townes2019}
\citation{Svensson2020}
\citation{Hafemeister2019}
\citation{Sarkar2021}
\citation{Datlinger2017}
\citation{Hill2018}
\@writefile{toc}{\contentsline {section}{\numberline {4}Generalized linear model with errors in variables}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Model}{16}}
\newlabel{glmeiv_model_1}{{6}{16}}
\newlabel{glmeiv_model_2}{{7}{16}}
\newlabel{glmeiv_model_3}{{8}{16}}
\newlabel{glmeiv_model_4}{{9}{16}}
\newlabel{glmeiv_model_5}{{10}{16}}
\citation{Ibrahim1990}
\newlabel{full_density}{{11}{19}}
\newlabel{full_log_lik}{{12}{19}}
\newlabel{marginal_density}{{13}{19}}
\newlabel{marginal_log_lik}{{14}{19}}
\newlabel{classical_eiv}{{15}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Estimation and inference}{19}}
\citation{Louis1982}
\citation{Barry2020}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces EM algorithm for GLM-EIV model.}}{20}}
\newlabel{algo:em_full}{{1}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Statistical accelerations}{21}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Computing pilot parameter estimates.}}{22}}
\newlabel{algo:pilot_estimates}{{2}{22}}
\citation{DITommaso2017}
\citation{Replogle2020}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Computation}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Zero-inflated model}{23}}
\bibstyle{unsrt}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Applying GLM-EIV at scale.}}{24}}
\newlabel{algo:at_scale}{{3}{24}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Simulation studies}{24}}
\newlabel{sec:simulation}{{5}{24}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Real data analysis}{24}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Discussion}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces }}{25}}
\newlabel{main_text_sim}{{4}{25}}
\bibdata{/Users/timbarry/optionFiles/glmeiv.bib}
\bibcite{Rothgangl2021}{1}
\bibcite{Musunuru2021}{2}
\bibcite{Przybyla2021}{3}
\bibcite{Dixit2016}{4}
\bibcite{Datlinger2017}{5}
\bibcite{Morris2021a}{6}
\bibcite{Lin2021}{7}
\bibcite{Lause2021}{8}
\bibcite{Townes2019}{9}
\bibcite{Grun2008}{10}
\bibcite{Ibrahim1990}{11}
\bibcite{Barry2020}{12}
\bibcite{Candes2018}{13}
\bibcite{Liu2021}{14}
\bibcite{Yang2019}{15}
\bibcite{Gasperini2019}{16}
\bibcite{Datlinger2021}{17}
\bibcite{Mimitou2019}{18}
\bibcite{Gallagher2018}{19}
\bibcite{Gasperini2020}{20}
\bibcite{Replogle2020}{21}
\bibcite{Stefanski2000a}{22}
\bibcite{Svensson2020}{23}
\bibcite{Hafemeister2019}{24}
\bibcite{Sarkar2021}{25}
\bibcite{Hill2018}{26}
\bibcite{Louis1982}{27}
\bibcite{DITommaso2017}{28}
\bibcite{fitzpatrick2009}{29}
\citation{fitzpatrick2009}
\@writefile{toc}{\contentsline {section}{Appendices}{29}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Theoretical details for thresholding estimator}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Notation}{29}}
\newlabel{sec:notation}{{A.1}{29}}
\newlabel{def_g}{{16}{29}}
\newlabel{def_h}{{17}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Almost sure limit of $\mathaccentV {hat}05E{\beta }^m_1$}{30}}
\newlabel{sec:convergence}{{A.2}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Re-expressing $\gamma $ in a simpler form}{31}}
\newlabel{sec:simplication}{{A.3}{31}}
\newlabel{thm:gamma_expression_1}{{18}{31}}
\newlabel{thm:gamma_expression_2}{{19}{31}}
\newlabel{thm:gamma_expression_3}{{20}{31}}
\newlabel{thm:gamma_expression_4}{{21}{31}}
\newlabel{gamma_alternative}{{22}{31}}
\newlabel{gamma_alternative_pi_half}{{23}{31}}
\newlabel{gamma_alt2_pi_half}{{24}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}Derivatives of $g$ and $h$ in $c$}{32}}
\newlabel{sec:derivatives}{{A.4}{32}}
\newlabel{dg_dc}{{25}{32}}
\newlabel{dh_dc}{{26}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.5}Limit of $\gamma $ in $c$}{32}}
\newlabel{sec:c_limit}{{A.5}{32}}
\newlabel{c_limit_product}{{27}{33}}
\newlabel{c_limit_product_2}{{28}{33}}
\newlabel{c_limit_product_3}{{29}{33}}
\newlabel{c_limit_product_4}{{30}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.6}Bayes-optimal decision boundary as a critical value of $\gamma $}{34}}
\newlabel{sec:bayes_opt}{{A.6}{34}}
\newlabel{quotient_rule}{{31}{34}}
\newlabel{dg_dc_bayes}{{32}{34}}
\newlabel{dh_dc_bayes}{{33}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.7}Comparing Bayes-optimal decision boundary and large threshold}{34}}
\newlabel{sec:comparison}{{A.7}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.8}Monotonicity in $\beta ^g_1$}{35}}
\newlabel{sec:monotone}{{A.8}{35}}
\newlabel{basic_ineq_cp}{{34}{35}}
\newlabel{basic_ineq_cp_2}{{35}{35}}
\newlabel{basic_ineq_cp_3}{{36}{36}}
\newlabel{dg_dbeta}{{37}{36}}
\newlabel{dh_dbeta}{{38}{36}}
\newlabel{basic_ineq_cp_4}{{39}{37}}
\newlabel{def_2g}{{40}{37}}
\newlabel{def_4h}{{41}{37}}
\newlabel{basic_ineq_cp_5}{{42}{37}}
\newlabel{d_gamma_d_beta}{{43}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.9}Strict attenuation bias}{37}}
\newlabel{sec:att_bias}{{A.9}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.10}Bias-variance decomposition in no-intercept model}{38}}
\newlabel{sec:bv_decomp}{{A.10}{38}}
\newlabel{bc_decomp_1}{{44}{38}}
\newlabel{bv_decomp_2}{{45}{39}}
\newlabel{bv_decomp_3}{{46}{39}}
\newlabel{bv_decomp_4}{{47}{39}}
\newlabel{bv_decomp_5}{{48}{39}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Estimation and inference in the GLM-EIV model}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Estimation}{39}}
\newlabel{e_step_1}{{49}{40}}
\newlabel{e_step_2}{{50}{40}}
\newlabel{Q_funct}{{51}{41}}
\citation{Louis1982}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Inference}{42}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Block structure of the observed information matrix $J(\theta ; m, g) = -\nabla ^2 \mathcal  {L}(\theta ; m, g)$. The matrix is symmetric, and so we only need to compute submatrices I-VI to compute the entire matrix.}}{43}}
\newlabel{infomatrixbackground}{{5}{43}}
\newlabel{zero_inf_info_mat}{{52}{43}}
\newlabel{sub_mat_pi}{{53}{43}}
\newlabel{d_L_d_pi}{{54}{44}}
\newlabel{submat_pi_1}{{55}{44}}
\newlabel{submat_pi_2}{{56}{45}}
\newlabel{sub_mat_1_formula}{{57}{45}}
\newlabel{sub_mat_2}{{58}{45}}
\newlabel{sub_mat_2_1}{{59}{46}}
\newlabel{sub_mat_2_2}{{60}{47}}
\newlabel{sub_mat_2_formula}{{61}{47}}
\newlabel{sub_mat_3_formula}{{62}{48}}
\newlabel{sub_mat_4}{{63}{48}}
\newlabel{sub_mat_4_1}{{64}{48}}
\newlabel{sub_mat_4_2}{{65}{49}}
\newlabel{sub_mat_4_formula}{{66}{49}}
\newlabel{sub_mat_5}{{67}{49}}
\newlabel{sub_mat_5_1}{{68}{50}}
\newlabel{sub_mat_5_2}{{69}{51}}
\newlabel{sub_mat_5_formula}{{70}{51}}
\newlabel{sub_mat_6_formula}{{71}{51}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}Implementation}{52}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Computing the matrices $\Delta ^m(j)$, $[\Delta ']^m(j)$, $V^m(j)$, $H^m(j)$, and $s^m(j)$ given given $\beta _m$.}}{53}}
\newlabel{algo:computing_info_matrices}{{4}{53}}
\newlabel{computing_info_matrix_1}{{72}{54}}
\newlabel{computing_info_matrix_2}{{73}{54}}
\newlabel{computing_info_matrix_3}{{74}{54}}
\newlabel{computing_info_matrix_4}{{75}{54}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \texttt  {linkinv}, \texttt  {variance}, \texttt  {mu.eta}, \texttt  {skewness}, \texttt  {mu.eta.prime} for common family objects (i.e., pairs of distributions and link functions).}}{55}}
\newlabel{family_object_functions}{{1}{55}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Zero-inflated model}{55}}
\newlabel{sec:zero_inf_model}{{C}{55}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Estimation}{56}}
\newlabel{q_funct_zero_inf}{{76}{57}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2}Inference}{57}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Block structure of the observed information matrix $J_z(\theta ; m, g) = -\nabla ^2 \mathcal  {L}_z(\theta ; m, g)$ for the zero-inflated model. Submatrices I, II, and VI are the same as in the background read model; therefore, we only need to compute submatrices III, VI, and V.}}{58}}
\newlabel{infomatrixzeroinf}{{6}{58}}
\newlabel{sub_mat_3_zeroinf}{{77}{58}}
\newlabel{sub_mat_3_zeroinf_1}{{78}{58}}
\newlabel{sub_mat_3_zeroinf_2}{{79}{59}}
\newlabel{sub_mat_3_zeroinf_formula}{{80}{59}}
\newlabel{sub_mat_4_zeroinf}{{81}{60}}
\newlabel{sub_mat_4_zeroinf_1}{{82}{60}}
\newlabel{sub_mat_4_zeroinf_2}{{83}{61}}
\newlabel{sub_mat_4_zeroinf_formula}{{84}{61}}
\newlabel{sub_mat_6_zeroinf}{{85}{61}}
\newlabel{sub_mat_6_zeroinf_1}{{86}{61}}
\newlabel{sub_mat_6_zeroinf_2}{{87}{62}}
\newlabel{sub_mat_6_zeroinf_formula}{{88}{62}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Statistical accelerations}{62}}
\newlabel{stat_acc_1}{{89}{62}}
\newlabel{can_param}{{90}{63}}
\newlabel{m_plus_o_mle}{{91}{63}}
\newlabel{pois_mle}{{92}{64}}
\newlabel{nb_mo_1}{{93}{65}}
\newlabel{nb_mo_2}{{94}{65}}
\newlabel{nb_mle}{{95}{65}}
\newlabel{nb_mo_3}{{96}{65}}
\newlabel{nb_mo_4}{{98}{66}}
\newlabel{nb_mo_5}{{99}{66}}
\@writefile{toc}{\contentsline {section}{\numberline {E}Additional simulation results}{66}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces }}{67}}
\newlabel{fig:gaussian_sim}{{7}{67}}
