\relax 
\citation{Rothgangl2021}
\citation{Musunuru2021}
\citation{Przybyla2021}
\citation{Dixit2016}
\citation{Datlinger2017}
\citation{Morris2021a}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}}
\citation{Lin2021}
\citation{Lause2021}
\citation{Townes2019}
\citation{Grun2008}
\citation{Ibrahim1990}
\citation{Barry2020}
\citation{Candes2018}
\citation{Liu2021}
\citation{Dixit2016}
\citation{Yang2019}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background and analysis challenges}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Related work}{4}}
\citation{Gasperini2019}
\citation{Datlinger2021}
\citation{Mimitou2019}
\citation{Gallagher2018}
\citation{Gasperini2020}
\citation{Replogle2020}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Assay overview}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Experimental design and analysis challenges}: \textbf  {a,} Experimental design. For a given perturbation (e.g., the perturbation indicated in blue), we partition the cells into two groups: perturbed and unperturbed. Next, for a given gene, we conduct a differential expression analysis across the two groups, yielding an estimate of the impact of the given perturbation on the given gene. \textbf  {b,} DAG representing all variables in the system. The perturbation (latent) impacts both gene expression and gRNA expression; technical factors act as confounders, also impacting gene and gRNA expression. The target of estimation is the effect of the perturbation on gene expression. \textbf  {c,} Schematic illustrating the ``background read'' phenomenon. Due to errors in the sequencing and alignment processes, unperturbed cells exhibit a nonzero gRNA count distribution (bottom). The target of estimation is the change in mean gene expression in response to the perturbation (top). \textbf  {d}, Example data on four cells for a given perturbation-gene pair. Note that (i) the perturbation is unobserved, and (ii) the gene and gRNA data are discrete counts.}}{6}}
\newlabel{analysis_challenges}{{1}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Analysis challenges}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Thresholding method}{7}}
\newlabel{thresh_glm}{{1}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Empirical challenges of thresholding method}{8}}
\newlabel{sec:thresholding_empirical}{{3.1}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Empirical challenges of thresholded regression.} \textbf  {a-b,} Fold change estimates produced by threshold = 1 versus threshold = 5 (a) and threshold = 20 versus threshold = 5 (b). The selected threshold substantially impacts the results. \textbf  {c-d,} $p$-values (c) and CI widths (d) produced by threshold = 20 versus threshold = 5. The latter threshold yields more confident estimates. \textbf  {e-f}, Empirical distribution of randomly-selected gRNA from Gasperini (e) and Xie (f) data (0 counts not shown). The gRNA data do not appear to imply an obvious threshold selection strategy.}}{10}}
\newlabel{thresholding_empirical}{{2}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Theoretical challenges of thresholding method}{10}}
\newlabel{sec:thresholding_theory}{{3.2}{10}}
\newlabel{theoretical_model}{{2}{11}}
\newlabel{prop:convergence}{{1}{11}}
\newlabel{thresh_est_intercepts}{{3}{11}}
\citation{Stefanski2000a}
\newlabel{prop:att_bias}{{2}{12}}
\newlabel{prop:monotonic}{{3}{12}}
\newlabel{prop:bayes_opt}{{4}{12}}
\newlabel{prop:c_limit_half}{{5}{13}}
\newlabel{prop:comparison}{{6}{13}}
\newlabel{prop:c_limit}{{7}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Theoretical challenges of thresholded regression.} \textbf  {a,} Asymptotic relative bias versus threshold for different values of $\beta ^g_1$. The bias function is highly nonconvex and strictly nonzero. Vertical blue lines, Bayes-optimal decision boundaries. Across all panels, $\beta ^g_0 = 0$ and $\pi = 1/2$. \textbf  {b,} Asymptotic relative bias versus $\pi $ when the threshold is set to a large number. The two quantities coincide exactly. \textbf  {c,} Bias-variance decomposition for thresholding method in no-intercept model. Bias decreases and variance increases as the threshold tends to infinity. $\beta ^g_1 = 1, \beta ^m_1 = 1,$ and $\pi = 0.1$.}}{14}}
\newlabel{thresholding_theoretical}{{3}{14}}
\newlabel{theoretical_model_no_int}{{4}{15}}
\newlabel{thresh_estimator_no_int}{{5}{15}}
\newlabel{prop:bv_decomp}{{8}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Thresholding method summary}{15}}
\citation{Townes2019}
\citation{Svensson2020}
\citation{Hafemeister2019}
\citation{Sarkar2021}
\citation{Datlinger2017}
\citation{Hill2018}
\@writefile{toc}{\contentsline {section}{\numberline {4}GLM-based errors-in-variables}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Model}{16}}
\newlabel{sec:model}{{4.1}{16}}
\newlabel{glmeiv_model_1}{{6}{16}}
\newlabel{glmeiv_model_2}{{7}{16}}
\newlabel{glmeiv_model_3}{{8}{16}}
\newlabel{glmeiv_model_4}{{9}{16}}
\newlabel{glmeiv_model_5}{{10}{16}}
\citation{Ibrahim1990}
\newlabel{full_density}{{11}{19}}
\newlabel{full_log_lik}{{12}{19}}
\newlabel{marginal_density}{{13}{19}}
\newlabel{marginal_log_lik}{{14}{19}}
\newlabel{classical_eiv}{{15}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Estimation and inference}{19}}
\citation{Louis1982}
\citation{Barry2020}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces EM algorithm for GLM-EIV model.}}{20}}
\newlabel{algo:em_full}{{1}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Statistical accelerations}{21}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Computing pilot parameter estimates.}}{22}}
\newlabel{algo:pilot_estimates}{{2}{22}}
\citation{DITommaso2017}
\citation{Replogle2020}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Computing}{23}}
\newlabel{sec:computing}{{4.4}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Zero-inflated model}{23}}
\citation{Ripley2013}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Applying GLM-EIV at scale.}}{24}}
\newlabel{algo:at_scale}{{3}{24}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Simulation studies}{24}}
\newlabel{sec:simulation}{{5}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Simulation study}. Columns correspond to distributions (Poisson, NB with known $\theta $, NB with estimated $\theta $), and rows correspond to metrics (bias, MSE, coverage, CI width, and time). Methods are shown in different colors; GLM-EIV (red) is masked by accelerated GLM-EIV (blue) in several panels. GLM-EIV demonstrated superior statistical performance to the thresholding method on all metrics (rows 1-4). Accelerated GLM-EIV had substantially lower computational cost than ``vanilla'' GLM-EIV (bottom row) despite demonstrating identical statistical performance (rows 1-4).}}{25}}
\newlabel{main_text_sim}{{4}{25}}
\citation{Hafemeister2019}
\@writefile{toc}{\contentsline {section}{\numberline {6}Data analysis}{26}}
\bibstyle{unsrt}
\@writefile{toc}{\contentsline {section}{\numberline {7}Discussion}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {Data analysis}. \textbf  {a-b}, Estimates for fold change produced by GLM-EIV and thresholded regression on Gasperini (\textbf  {a}) and Xie (\textbf  {b}) negative control pairs. \textbf  {c-d}, Estimates produced by GLM-EIV and thresholded regression on two positive control pairs -- \textit  {LRIF1} (\textbf  {a}) and \textit  {NDUFA2} (\textbf  {b}) -- plotted as a function of excess background contamination. Grey bands, 95\% CIs for the target of inference outputted by the methods. \textbf  {e-f}, Median relative estimate change (REC; \textbf  {e}) and confidence interval coverage rate (\textbf  {f}) across \textit  {all} 322 positive control pairs, plotted as a function of excess background contamination. Colored bands, pointwise 95\% CIs for population quantities. Panels (\textbf  {c-f}) together illustrate that GLM-EIV demonstrated greater stability than thresholded regression as background contamination increased.}}{29}}
\newlabel{fig:real_data}{{5}{29}}
\bibdata{/Users/timbarry/optionFiles/glmeiv.bib}
\bibcite{Rothgangl2021}{1}
\bibcite{Musunuru2021}{2}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \textbf  {Use of GLM-EIV in practice}. The decision tree above illustrates how we anticipate GLM-EIV could be used in practice. First, apply GLM-EIV to a set of randomly-sampled gene-perturbation pairs to assess background contamination level (positive control pairs work best for this purpose). If GLM-EIV indicates that background contamination is high (e.g., $\qopname  \relax o{exp}(\beta ^g_1) \lessapprox $ 10), apply GLM-EIV to analyze the entire dataset; otherwise, approximate the Bayes-optimal decision boundary using the fitted GLM-EIV models. Next, apply a thresholding method (e.g., SCEPTRE or thresholded negative binomial regression) to analyze the data, setting the threshold to the estimated Bayes-optimal decision boundary.}}{30}}
\bibcite{Przybyla2021}{3}
\bibcite{Dixit2016}{4}
\bibcite{Datlinger2017}{5}
\bibcite{Morris2021a}{6}
\bibcite{Lin2021}{7}
\bibcite{Lause2021}{8}
\bibcite{Townes2019}{9}
\bibcite{Grun2008}{10}
\bibcite{Ibrahim1990}{11}
\bibcite{Barry2020}{12}
\bibcite{Candes2018}{13}
\bibcite{Liu2021}{14}
\bibcite{Yang2019}{15}
\bibcite{Gasperini2019}{16}
\bibcite{Datlinger2021}{17}
\bibcite{Mimitou2019}{18}
\bibcite{Gallagher2018}{19}
\bibcite{Gasperini2020}{20}
\bibcite{Replogle2020}{21}
\bibcite{Stefanski2000a}{22}
\bibcite{Svensson2020}{23}
\bibcite{Hafemeister2019}{24}
\bibcite{Sarkar2021}{25}
\bibcite{Hill2018}{26}
\bibcite{Louis1982}{27}
\bibcite{DITommaso2017}{28}
\bibcite{Ripley2013}{29}
\bibcite{fitzpatrick2009}{30}
\@writefile{toc}{\contentsline {section}{Appendices}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Theoretical details for thresholding estimator}{33}}
\citation{fitzpatrick2009}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Notation}{34}}
\newlabel{sec:notation}{{A.1}{34}}
\newlabel{def_g}{{16}{34}}
\newlabel{def_h}{{17}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Almost sure limit of $\mathaccentV {hat}05E{\beta }^m_1$}{34}}
\newlabel{sec:convergence}{{A.2}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Re-expressing $\gamma $ in a simpler form}{35}}
\newlabel{sec:simplication}{{A.3}{35}}
\newlabel{thm:gamma_expression_1}{{18}{35}}
\newlabel{thm:gamma_expression_2}{{19}{36}}
\newlabel{thm:gamma_expression_3}{{20}{36}}
\newlabel{thm:gamma_expression_4}{{21}{36}}
\newlabel{gamma_alternative}{{22}{36}}
\newlabel{gamma_alternative_pi_half}{{23}{36}}
\newlabel{gamma_alt2_pi_half}{{24}{36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}Derivatives of $g$ and $h$ in $c$}{36}}
\newlabel{sec:derivatives}{{A.4}{36}}
\newlabel{dg_dc}{{25}{37}}
\newlabel{dh_dc}{{26}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.5}Limit of $\gamma $ in $c$}{37}}
\newlabel{sec:c_limit}{{A.5}{37}}
\newlabel{c_limit_product}{{27}{37}}
\newlabel{c_limit_product_2}{{28}{37}}
\newlabel{c_limit_product_3}{{29}{38}}
\newlabel{c_limit_product_4}{{30}{38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.6}Bayes-optimal decision boundary as a critical value of $\gamma $}{38}}
\newlabel{sec:bayes_opt}{{A.6}{38}}
\newlabel{quotient_rule}{{31}{38}}
\newlabel{dg_dc_bayes}{{32}{39}}
\newlabel{dh_dc_bayes}{{33}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.7}Comparing Bayes-optimal decision boundary and large threshold}{39}}
\newlabel{sec:comparison}{{A.7}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.8}Monotonicity in $\beta ^g_1$}{40}}
\newlabel{sec:monotone}{{A.8}{40}}
\newlabel{basic_ineq_cp}{{34}{40}}
\newlabel{basic_ineq_cp_2}{{35}{40}}
\newlabel{basic_ineq_cp_3}{{36}{40}}
\newlabel{dg_dbeta}{{37}{41}}
\newlabel{dh_dbeta}{{38}{41}}
\newlabel{basic_ineq_cp_4}{{39}{41}}
\newlabel{def_2g}{{40}{41}}
\newlabel{def_4h}{{41}{41}}
\newlabel{basic_ineq_cp_5}{{42}{41}}
\newlabel{d_gamma_d_beta}{{43}{41}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.9}Strict attenuation bias}{42}}
\newlabel{sec:att_bias}{{A.9}{42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.10}Bias-variance decomposition in no-intercept model}{42}}
\newlabel{sec:bv_decomp}{{A.10}{42}}
\newlabel{bc_decomp_1}{{44}{42}}
\newlabel{bv_decomp_2}{{45}{43}}
\newlabel{bv_decomp_3}{{46}{43}}
\newlabel{bv_decomp_4}{{47}{43}}
\newlabel{bv_decomp_5}{{48}{43}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Estimation and inference in the GLM-EIV model}{44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Estimation}{44}}
\newlabel{e_step_1}{{49}{45}}
\newlabel{e_step_2}{{50}{45}}
\newlabel{Q_funct}{{51}{45}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Inference}{46}}
\citation{Louis1982}
\newlabel{zero_inf_info_mat}{{52}{47}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Block structure of the observed information matrix $J(\theta ; m, g) = -\nabla ^2 \mathcal  {L}(\theta ; m, g)$. The matrix is symmetric, and so we only need to compute submatrices I-VI to compute the entire matrix.}}{48}}
\newlabel{infomatrixbackground}{{7}{48}}
\newlabel{sub_mat_pi}{{53}{48}}
\newlabel{d_L_d_pi}{{54}{48}}
\newlabel{submat_pi_1}{{55}{49}}
\newlabel{submat_pi_2}{{56}{49}}
\newlabel{sub_mat_1_formula}{{57}{49}}
\newlabel{sub_mat_2}{{58}{50}}
\newlabel{sub_mat_2_1}{{59}{50}}
\newlabel{sub_mat_2_2}{{60}{51}}
\newlabel{sub_mat_2_formula}{{61}{52}}
\newlabel{sub_mat_3_formula}{{62}{52}}
\newlabel{sub_mat_4}{{63}{52}}
\newlabel{sub_mat_4_1}{{64}{52}}
\newlabel{sub_mat_4_2}{{65}{53}}
\newlabel{sub_mat_4_formula}{{66}{54}}
\newlabel{sub_mat_5}{{67}{54}}
\newlabel{sub_mat_5_1}{{68}{54}}
\newlabel{sub_mat_5_2}{{69}{55}}
\newlabel{sub_mat_5_formula}{{70}{55}}
\newlabel{sub_mat_6_formula}{{71}{55}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}Implementation}{57}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Computing the matrices $\Delta ^m(j)$, $[\Delta ']^m(j)$, $V^m(j)$, $H^m(j)$, and $s^m(j)$ given given $\beta _m$.}}{57}}
\newlabel{algo:computing_info_matrices}{{4}{57}}
\newlabel{computing_info_matrix_1}{{72}{58}}
\newlabel{computing_info_matrix_2}{{73}{58}}
\newlabel{computing_info_matrix_3}{{74}{58}}
\newlabel{computing_info_matrix_4}{{75}{58}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \texttt  {linkinv}, \texttt  {variance}, \texttt  {mu.eta}, \texttt  {skewness}, \texttt  {mu.eta.prime} for common family objects (i.e., pairs of distributions and link functions).}}{59}}
\newlabel{family_object_functions}{{1}{59}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Zero-inflated model}{59}}
\newlabel{sec:zero_inf_model}{{C}{59}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Estimation}{60}}
\newlabel{q_funct_zero_inf}{{76}{61}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2}Inference}{61}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Block structure of the observed information matrix $J_z(\theta ; m, g) = -\nabla ^2 \mathcal  {L}_z(\theta ; m, g)$ for the zero-inflated model. Submatrices I, II, and VI are the same as in the background read model; therefore, we only need to compute submatrices III, VI, and V.}}{62}}
\newlabel{infomatrixzeroinf}{{8}{62}}
\newlabel{sub_mat_3_zeroinf}{{77}{62}}
\newlabel{sub_mat_3_zeroinf_1}{{78}{62}}
\newlabel{sub_mat_3_zeroinf_2}{{79}{63}}
\newlabel{sub_mat_3_zeroinf_formula}{{80}{63}}
\newlabel{sub_mat_4_zeroinf}{{81}{64}}
\newlabel{sub_mat_4_zeroinf_1}{{82}{64}}
\newlabel{sub_mat_4_zeroinf_2}{{83}{65}}
\newlabel{sub_mat_4_zeroinf_formula}{{84}{65}}
\newlabel{sub_mat_6_zeroinf}{{85}{65}}
\newlabel{sub_mat_6_zeroinf_1}{{86}{65}}
\newlabel{sub_mat_6_zeroinf_2}{{87}{66}}
\newlabel{sub_mat_6_zeroinf_formula}{{88}{66}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Statistical accelerations}{66}}
\newlabel{stat_acc_1}{{89}{66}}
\newlabel{can_param}{{90}{67}}
\newlabel{m_plus_o_mle}{{91}{67}}
\newlabel{pois_mle}{{92}{68}}
\newlabel{nb_mo_1}{{93}{69}}
\newlabel{nb_mo_2}{{94}{69}}
\newlabel{nb_mle}{{95}{69}}
\newlabel{nb_mo_3}{{96}{69}}
\newlabel{nb_mo_4}{{98}{70}}
\newlabel{nb_mo_5}{{99}{70}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Additional simulation results on Gaussian data. GLM-EIV (accelerated) outperformed the thresholding method on bias, mean squared error, confidence interval coverage rate, and confidence interval width metrics.}}{71}}
\newlabel{fig:gaussian_sim}{{9}{71}}
\@writefile{toc}{\contentsline {section}{\numberline {E}Additional simulation study}{71}}
\newlabel{sec:extra_sims}{{E}{71}}
\@writefile{toc}{\contentsline {section}{\numberline {F}Real data analysis details}{71}}
